{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PyTorch ResNet18 Flexible Input Notebook\n",
        "\n",
        "This notebook mirrors `resnet_model_arch_binary_classification_hikaru_Sep8.ipynb`, but re-implements the workflow in PyTorch using a ResNet18 backbone pretrained on ImageNet. It supports flexible combinations of imaging modalities (e.g., `B_mode`, `MBF`, `SI`) and provides patient-level evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "from torchvision.transforms import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Torchvision version: {models.__version__ if hasattr(models, '__version__') else 'N/A'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment detection and path configuration\n",
        "\n",
        "def is_google_colab() -> bool:\n",
        "    try:\n",
        "        import google.colab  # type: ignore  # noqa: F401\n",
        "\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def is_google_drive_mounted() -> bool:\n",
        "    return os.path.exists(\"/content/drive/MyDrive\")\n",
        "\n",
        "\n",
        "def configure_paths() -> Tuple[Dict[str, str], str]:\n",
        "    if is_google_colab() and is_google_drive_mounted():\n",
        "        base = \"/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney\"\n",
        "        print(\"Running on Google Colab with Google Drive mounted\")\n",
        "    elif is_google_colab():\n",
        "        base = \"/content\"\n",
        "        print(\"Running on Google Colab without Google Drive mounted\")\n",
        "    else:\n",
        "        base = \"/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney\"\n",
        "        print(\"Running locally\")\n",
        "\n",
        "    image_folders = {\n",
        "        \"B_mode\": os.path.join(base, \"data\", \"lanczos_shape_corrected_only_nc_resized_images\"),\n",
        "        \"MBF\": os.path.join(base, \"data\", \"MBF\"),\n",
        "        \"SI\": os.path.join(base, \"data\", \"SI\"),\n",
        "    }\n",
        "    csv_file = os.path.join(base, \"csv\", \"patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv\")\n",
        "\n",
        "    return image_folders, csv_file\n",
        "\n",
        "\n",
        "IMAGE_FOLDERS, CSV_FILE = configure_paths()\n",
        "print(\"Selected image folders:\")\n",
        "for key, value in IMAGE_FOLDERS.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(f\"CSV file: {CSV_FILE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Patient data structures and loaders\n",
        "\n",
        "@dataclass\n",
        "class Patient:\n",
        "    patient_id: int\n",
        "    egfr_label: int\n",
        "    egfr_value: float\n",
        "    image_paths: Dict[str, List[str]]\n",
        "\n",
        "    def get_paths(self, input_type: str) -> List[str]:\n",
        "        return self.image_paths.get(input_type, [])\n",
        "\n",
        "    def min_images_across_inputs(self, input_types: List[str]) -> int:\n",
        "        return min(len(self.get_paths(input_type)) for input_type in input_types)\n",
        "\n",
        "\n",
        "def extract_patient_id(filename: str, input_type: str) -> int:\n",
        "    if input_type == \"B_mode\":\n",
        "        # Expected format: Patient_100_Resized_Image_1.png\n",
        "        return int(filename.split(\"_\")[1])\n",
        "    # Expected format: P100_PTONR_01_Image_1_rf_MBF_resized.png\n",
        "    return int(filename.split(\"_\")[0][1:])\n",
        "\n",
        "\n",
        "def load_patients(\n",
        "    input_types: List[str],\n",
        "    image_folders: Dict[str, str],\n",
        "    csv_file: str,\n",
        ") -> List[Patient]:\n",
        "    egfr_data = pd.read_csv(csv_file)\n",
        "    egfr_data.rename(\n",
        "        columns={\"Patient ID\": \"patient_id\", \"eGFR (abs/closest)\": \"eGFR\"}, inplace=True\n",
        "    )\n",
        "    egfr_data[\"patient_id\"] = egfr_data[\"patient_id\"].astype(int)\n",
        "    egfr_data.set_index(\"patient_id\", inplace=True)\n",
        "\n",
        "    patient_image_map: Dict[int, Dict[str, List[str]]] = defaultdict(\n",
        "        lambda: {input_type: [] for input_type in input_types}\n",
        "    )\n",
        "\n",
        "    for input_type in input_types:\n",
        "        folder_path = image_folders.get(input_type)\n",
        "        if not folder_path or not os.path.exists(folder_path):\n",
        "            print(f\"Warning: folder not found for {input_type}: {folder_path}\")\n",
        "            continue\n",
        "\n",
        "        for filename in sorted(os.listdir(folder_path)):\n",
        "            try:\n",
        "                patient_id = extract_patient_id(filename, input_type)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            if patient_id in egfr_data.index:\n",
        "                patient_image_map[patient_id][input_type].append(\n",
        "                    os.path.join(folder_path, filename)\n",
        "                )\n",
        "\n",
        "    patients: List[Patient] = []\n",
        "    for patient_id, paths_dict in patient_image_map.items():\n",
        "        if all(len(paths_dict[input_type]) > 0 for input_type in input_types):\n",
        "            egfr_value = float(egfr_data.loc[patient_id, \"eGFR\"])\n",
        "            egfr_label = 1 if egfr_value >= 60 else 0\n",
        "            patients.append(\n",
        "                Patient(\n",
        "                    patient_id=patient_id,\n",
        "                    egfr_label=egfr_label,\n",
        "                    egfr_value=egfr_value,\n",
        "                    image_paths=paths_dict,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    return patients\n",
        "\n",
        "\n",
        "def summarize_patients(patients: List[Patient], input_types: List[str]) -> None:\n",
        "    print(f\"Number of patients: {len(patients)}\")\n",
        "    for input_type in input_types:\n",
        "        counts = [len(p.get_paths(input_type)) for p in patients]\n",
        "        if not counts:\n",
        "            continue\n",
        "        total_images = sum(counts)\n",
        "        print(\n",
        "            f\"{input_type}: total={total_images}, min={min(counts)}, max={max(counts)}, avg={np.mean(counts):.2f}\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flexible dataset with synchronized augmentation\n",
        "\n",
        "class FlexiblePatientDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        patients: List[Patient],\n",
        "        input_types: List[str],\n",
        "        image_size: int = 224,\n",
        "        augment: bool = False,\n",
        "    ) -> None:\n",
        "        self.patients = patients\n",
        "        self.input_types = input_types\n",
        "        self.image_size = image_size\n",
        "        self.augment = augment\n",
        "\n",
        "        self.samples: List[Tuple[List[str], int, int]] = []\n",
        "        for patient in patients:\n",
        "            n_images = patient.min_images_across_inputs(input_types)\n",
        "            for idx in range(n_images):\n",
        "                sample_paths = [patient.get_paths(input_type)[idx] for input_type in input_types]\n",
        "                self.samples.append((sample_paths, patient.egfr_label, patient.patient_id))\n",
        "\n",
        "        self.base_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        self.base_std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
        "        image_paths, label, patient_id = self.samples[idx]\n",
        "        images = [self._load_image(path) for path in image_paths]\n",
        "\n",
        "        if self.augment:\n",
        "            images = self._apply_augmentations(images)\n",
        "\n",
        "        tensors = [F.to_tensor(img) for img in images]\n",
        "        tensors = [self._normalize_tensor(t) for t in tensors]\n",
        "        concatenated = torch.cat(tensors, dim=0)\n",
        "\n",
        "        return concatenated, torch.tensor(label, dtype=torch.float32), patient_id\n",
        "\n",
        "    def _load_image(self, path: str) -> Image.Image:\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        return F.resize(image, [self.image_size, self.image_size])\n",
        "\n",
        "    def _normalize_tensor(self, tensor: torch.Tensor) -> torch.Tensor:\n",
        "        base_mean = self.base_mean.to(tensor.dtype)\n",
        "        base_std = self.base_std.to(tensor.dtype)\n",
        "        return (tensor - base_mean) / base_std\n",
        "\n",
        "    def _apply_augmentations(self, images: List[Image.Image]) -> List[Image.Image]:\n",
        "        # Match the TensorFlow pipeline: Random horizontal flip, rotation (±0.25 rad ≈ 14°), and zoom (±10%)\n",
        "        do_flip = random.random() < 0.5\n",
        "        angle_deg = random.uniform(-14.0, 14.0)\n",
        "        zoom = random.uniform(0.9, 1.1)\n",
        "\n",
        "        augmented = []\n",
        "        for image in images:\n",
        "            if do_flip:\n",
        "                image = F.hflip(image)\n",
        "            image = F.affine(image, angle=angle_deg, translate=(0, 0), scale=zoom, shear=0.0)\n",
        "            augmented.append(image)\n",
        "        return augmented\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model builder: ResNet18 with flexible input channels\n",
        "\n",
        "def build_resnet18(num_input_channels: int, device: torch.device) -> nn.Module:\n",
        "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "    if num_input_channels != 3:\n",
        "        old_conv = model.conv1\n",
        "        model.conv1 = nn.Conv2d(\n",
        "            num_input_channels,\n",
        "            old_conv.out_channels,\n",
        "            kernel_size=old_conv.kernel_size,\n",
        "            stride=old_conv.stride,\n",
        "            padding=old_conv.padding,\n",
        "            bias=old_conv.bias is not None,\n",
        "        )\n",
        "        nn.init.kaiming_normal_(model.conv1.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        if old_conv.bias is not None:\n",
        "            nn.init.zeros_(model.conv1.bias)\n",
        "    model.fc = nn.Linear(model.fc.in_features, 1)\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "def count_trainable_parameters(model: nn.Module) -> int:\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training utilities\n",
        "\n",
        "def set_seeds(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def compute_pos_weight(labels: List[int]) -> float:\n",
        "    positives = sum(labels)\n",
        "    negatives = len(labels) - positives\n",
        "    if positives == 0:\n",
        "        return 1.0\n",
        "    return max(1.0, negatives / max(positives, 1))\n",
        "\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    device: torch.device,\n",
        ") -> Tuple[float, float]:\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    all_labels: List[float] = []\n",
        "    all_probs: List[float] = []\n",
        "\n",
        "    for images, labels, _ in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images).squeeze(1)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "        all_probs.extend(probs.tolist())\n",
        "        all_labels.extend(labels.detach().cpu().numpy().tolist())\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = total_loss / len(dataloader.dataset)\n",
        "    epoch_auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else float(\"nan\")\n",
        "    return epoch_loss, epoch_auc\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: torch.device,\n",
        ") -> Tuple[float, float, Dict[int, List[float]], Dict[int, int]]:\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_labels: List[float] = []\n",
        "    all_probs: List[float] = []\n",
        "    patient_probs: Dict[int, List[float]] = defaultdict(list)\n",
        "    patient_labels: Dict[int, int] = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, patient_ids in dataloader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(images).squeeze(1)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "\n",
        "            labels_np = labels.cpu().numpy()\n",
        "            patient_ids_np = patient_ids.numpy()\n",
        "\n",
        "            all_probs.extend(probs.tolist())\n",
        "            all_labels.extend(labels_np.tolist())\n",
        "\n",
        "            for pid, prob, lab in zip(patient_ids_np, probs, labels_np):\n",
        "                patient_probs[int(pid)].append(float(prob))\n",
        "                patient_labels[int(pid)] = int(lab)\n",
        "\n",
        "    epoch_loss = total_loss / len(dataloader.dataset)\n",
        "    epoch_auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else float(\"nan\")\n",
        "    return epoch_loss, epoch_auc, patient_probs, patient_labels\n",
        "\n",
        "\n",
        "def plot_roc_curves(\n",
        "    probs: List[float],\n",
        "    labels: List[int],\n",
        "    patient_probs: Dict[int, List[float]],\n",
        "    patient_labels: Dict[int, int],\n",
        "    title_suffix: str,\n",
        ") -> Tuple[float, float]:\n",
        "    if len(set(labels)) > 1:\n",
        "        fpr_img, tpr_img, _ = roc_curve(labels, probs)\n",
        "        auc_img = roc_auc_score(labels, probs)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr_img, tpr_img, label=f\"Image ROC (AUC={auc_img:.4f})\", color=\"tab:blue\")\n",
        "        plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.title(f\"Image-level ROC {title_suffix}\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        auc_img = float(\"nan\")\n",
        "\n",
        "    patient_mean_probs = [np.mean(patient_probs[pid]) for pid in patient_probs]\n",
        "    patient_true = [patient_labels[pid] for pid in patient_probs]\n",
        "\n",
        "    if len(set(patient_true)) > 1:\n",
        "        fpr_pat, tpr_pat, _ = roc_curve(patient_true, patient_mean_probs)\n",
        "        auc_pat = roc_auc_score(patient_true, patient_mean_probs)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr_pat, tpr_pat, label=f\"Patient ROC (AUC={auc_pat:.4f})\", color=\"tab:green\")\n",
        "        plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.title(f\"Patient-level ROC {title_suffix}\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "    else:\n",
        "        auc_pat = float(\"nan\")\n",
        "\n",
        "    return auc_img, auc_pat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment configuration\n",
        "\n",
        "INPUT_TYPES = [\"B_mode\"]  # e.g., [\"B_mode\", \"MBF\"], [\"B_mode\", \"MBF\", \"SI\"]\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 100\n",
        "EARLY_STOPPING_PATIENCE = 20\n",
        "LEARNING_RATE = 1e-4\n",
        "STEP_LR_EVERY = 15\n",
        "STEP_LR_GAMMA = 0.5\n",
        "N_RUNS = 5\n",
        "BASE_SEED = 42\n",
        "\n",
        "set_seeds(BASE_SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Selected input types: {INPUT_TYPES}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and summarize patient data\n",
        "\n",
        "all_patients = load_patients(INPUT_TYPES, IMAGE_FOLDERS, CSV_FILE)\n",
        "if not all_patients:\n",
        "    raise RuntimeError(\"No patients found with the specified input types.\")\n",
        "\n",
        "summarize_patients(all_patients, INPUT_TYPES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hold-out runs with training, validation, and testing\n",
        "\n",
        "results = []\n",
        "\n",
        "for run_idx in range(N_RUNS):\n",
        "    seed = BASE_SEED + run_idx\n",
        "    set_seeds(seed)\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Run {run_idx + 1}/{N_RUNS} (seed={seed})\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    train_val_patients, test_patients = train_test_split(all_patients, test_size=0.1, random_state=seed)\n",
        "    train_patients, val_patients = train_test_split(train_val_patients, test_size=0.2, random_state=seed)\n",
        "\n",
        "    print(f\"Train patients: {len(train_patients)}\")\n",
        "    print(f\"Validation patients: {len(val_patients)}\")\n",
        "    print(f\"Test patients: {len(test_patients)}\")\n",
        "\n",
        "    train_dataset = FlexiblePatientDataset(train_patients, INPUT_TYPES, image_size=IMAGE_SIZE, augment=True)\n",
        "    val_dataset = FlexiblePatientDataset(val_patients, INPUT_TYPES, image_size=IMAGE_SIZE, augment=False)\n",
        "    test_dataset = FlexiblePatientDataset(test_patients, INPUT_TYPES, image_size=IMAGE_SIZE, augment=False)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = build_resnet18(num_input_channels=3 * len(INPUT_TYPES), device=device)\n",
        "    print(f\"Trainable parameters: {count_trainable_parameters(model):,}\")\n",
        "\n",
        "    train_labels = [label for _, label, _ in train_dataset.samples]\n",
        "    pos_weight_value = compute_pos_weight(train_labels)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight_value, device=device))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_LR_EVERY, gamma=STEP_LR_GAMMA)\n",
        "\n",
        "    best_val_auc = -1.0\n",
        "    best_state = None\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        train_loss, train_auc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_auc, val_patient_probs, val_patient_labels = evaluate(model, val_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} AUC: {train_auc:.4f} | \"\n",
        "            f\"Val Loss: {val_loss:.4f} AUC: {val_auc:.4f}\"\n",
        "        )\n",
        "\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "            best_state = {\n",
        "                \"model\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    if best_state is None:\n",
        "        raise RuntimeError(\"Training did not yield a valid model state.\")\n",
        "\n",
        "    model.load_state_dict(best_state[\"model\"])\n",
        "\n",
        "    val_loss, val_auc, val_patient_probs, val_patient_labels = evaluate(model, val_loader, criterion, device)\n",
        "    val_probs_flat = [prob for probs in val_patient_probs.values() for prob in probs]\n",
        "    val_labels_flat = [val_patient_labels[pid] for pid, probs in val_patient_probs.items() for _ in probs]\n",
        "    val_img_auc, val_patient_auc = plot_roc_curves(\n",
        "        val_probs_flat,\n",
        "        val_labels_flat,\n",
        "        val_patient_probs,\n",
        "        val_patient_labels,\n",
        "        title_suffix=f\"(Validation Run {run_idx + 1})\",\n",
        "    )\n",
        "\n",
        "    test_loss, test_auc, test_patient_probs, test_patient_labels = evaluate(model, test_loader, criterion, device)\n",
        "    test_probs_flat = [prob for probs in test_patient_probs.values() for prob in probs]\n",
        "    test_labels_flat = [test_patient_labels[pid] for pid, probs in test_patient_probs.items() for _ in probs]\n",
        "    test_img_auc, test_patient_auc = plot_roc_curves(\n",
        "        test_probs_flat,\n",
        "        test_labels_flat,\n",
        "        test_patient_probs,\n",
        "        test_patient_labels,\n",
        "        title_suffix=f\"(Test Run {run_idx + 1})\",\n",
        "    )\n",
        "\n",
        "    preds_binary = [1 if prob >= 0.5 else 0 for prob in test_probs_flat]\n",
        "    print(\"Test classification report (image-level):\")\n",
        "    print(classification_report(test_labels_flat, preds_binary, digits=4))\n",
        "\n",
        "    results.append(\n",
        "        {\n",
        "            \"val_auc\": val_auc,\n",
        "            \"test_auc\": test_auc,\n",
        "            \"val_img_auc\": val_img_auc,\n",
        "            \"val_patient_auc\": val_patient_auc,\n",
        "            \"test_img_auc\": test_img_auc,\n",
        "            \"test_patient_auc\": test_patient_auc,\n",
        "        }\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate results across runs\n",
        "\n",
        "if results:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FINAL SUMMARY OVER MULTIPLE HOLD-OUT RUNS\")\n",
        "    print(\"=\" * 60)\n",
        "    val_aucs = [entry[\"val_auc\"] for entry in results]\n",
        "    test_aucs = [entry[\"test_auc\"] for entry in results]\n",
        "    print(f\"Validation AUCs: {[f'{auc:.4f}' for auc in val_aucs]}\")\n",
        "    print(f\"Test AUCs:       {[f'{auc:.4f}' for auc in test_aucs]}\")\n",
        "    print(f\"\\nAverage Validation AUC: {np.mean(val_aucs):.4f} ± {np.std(val_aucs):.4f}\")\n",
        "    print(f\"Average Test AUC:       {np.mean(test_aucs):.4f} ± {np.std(test_aucs):.4f}\")\n",
        "else:\n",
        "    print(\"No runs executed yet.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation testing (patient-level K-Fold)\n",
        "\n",
        "N_FOLDS = 5\n",
        "cv_results = []\n",
        "\n",
        "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=BASE_SEED)\n",
        "\n",
        "for fold_idx, (train_val_idx, test_idx) in enumerate(kf.split(all_patients)):\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Cross-validation fold {fold_idx + 1}/{N_FOLDS}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    test_patients = [all_patients[i] for i in test_idx]\n",
        "    train_val_patients = [all_patients[i] for i in train_val_idx]\n",
        "\n",
        "    rng = np.random.default_rng(BASE_SEED + fold_idx)\n",
        "    rng.shuffle(train_val_patients)\n",
        "    val_split = max(1, int(len(train_val_patients) * 0.2))\n",
        "    val_patients = train_val_patients[:val_split]\n",
        "    train_patients = train_val_patients[val_split:]\n",
        "\n",
        "    print(f\"Training patients: {len(train_patients)}\")\n",
        "    print(f\"Validation patients: {len(val_patients)}\")\n",
        "    print(f\"Test patients: {len(test_patients)}\")\n",
        "\n",
        "    train_dataset = FlexiblePatientDataset(train_patients, INPUT_TYPES, image_size=IMAGE_SIZE, augment=True)\n",
        "    val_dataset = FlexiblePatientDataset(val_patients, INPUT_TYPES, image_size=IMAGE_SIZE, augment=False)\n",
        "    test_dataset = FlexiblePatientDataset(test_patients, INPUT_TYPES, image_size=IMAGE_SIZE, augment=False)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = build_resnet18(num_input_channels=3 * len(INPUT_TYPES), device=device)\n",
        "    print(f\"Trainable parameters: {count_trainable_parameters(model):,}\")\n",
        "\n",
        "    train_labels = [label for _, label, _ in train_dataset.samples]\n",
        "    pos_weight_value = compute_pos_weight(train_labels)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight_value, device=device))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_LR_EVERY, gamma=STEP_LR_GAMMA)\n",
        "\n",
        "    best_val_auc = -1.0\n",
        "    best_state = None\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        train_loss, train_auc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_auc, val_patient_probs, val_patient_labels = evaluate(model, val_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:3d} | Train Loss: {train_loss:.4f} AUC: {train_auc:.4f} | \"\n",
        "            f\"Val Loss: {val_loss:.4f} AUC: {val_auc:.4f}\"\n",
        "        )\n",
        "\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "            best_state = {\n",
        "                \"model\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    if best_state is None:\n",
        "        raise RuntimeError(\"Training did not yield a valid model state.\")\n",
        "\n",
        "    model.load_state_dict(best_state[\"model\"])\n",
        "\n",
        "    val_loss, val_auc, val_patient_probs, val_patient_labels = evaluate(model, val_loader, criterion, device)\n",
        "    val_probs_flat = [prob for probs in val_patient_probs.values() for prob in probs]\n",
        "    val_labels_flat = [val_patient_labels[pid] for pid, probs in val_patient_probs.items() for _ in probs]\n",
        "    val_img_auc, val_patient_auc = plot_roc_curves(\n",
        "        val_probs_flat,\n",
        "        val_labels_flat,\n",
        "        val_patient_probs,\n",
        "        val_patient_labels,\n",
        "        title_suffix=f\"(Validation Fold {fold_idx + 1})\",\n",
        "    )\n",
        "\n",
        "    test_loss, test_auc, test_patient_probs, test_patient_labels = evaluate(model, test_loader, criterion, device)\n",
        "    test_probs_flat = [prob for probs in test_patient_probs.values() for prob in probs]\n",
        "    test_labels_flat = [test_patient_labels[pid] for pid, probs in test_patient_probs.items() for _ in probs]\n",
        "    test_img_auc, test_patient_auc = plot_roc_curves(\n",
        "        test_probs_flat,\n",
        "        test_labels_flat,\n",
        "        test_patient_probs,\n",
        "        test_patient_labels,\n",
        "        title_suffix=f\"(Test Fold {fold_idx + 1})\",\n",
        "    )\n",
        "\n",
        "    preds_binary = [1 if prob >= 0.5 else 0 for prob in test_probs_flat]\n",
        "    print(\"Test classification report (image-level):\")\n",
        "    print(classification_report(test_labels_flat, preds_binary, digits=4))\n",
        "\n",
        "    cv_results.append(\n",
        "        {\n",
        "            \"val_auc\": val_auc,\n",
        "            \"test_auc\": test_auc,\n",
        "            \"val_img_auc\": val_img_auc,\n",
        "            \"val_patient_auc\": val_patient_auc,\n",
        "            \"test_img_auc\": test_img_auc,\n",
        "            \"test_patient_auc\": test_patient_auc,\n",
        "        }\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation summary\n",
        "\n",
        "if cv_results:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FINAL CROSS-VALIDATION SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    cv_val_aucs = [entry[\"val_auc\"] for entry in cv_results]\n",
        "    cv_test_aucs = [entry[\"test_auc\"] for entry in cv_results]\n",
        "    print(f\"Validation AUCs: {[f'{auc:.4f}' for auc in cv_val_aucs]}\")\n",
        "    print(f\"Test AUCs:       {[f'{auc:.4f}' for auc in cv_test_aucs]}\")\n",
        "    print(f\"\\nAverage Validation AUC: {np.mean(cv_val_aucs):.4f} ± {np.std(cv_val_aucs):.4f}\")\n",
        "    print(f\"Average Test AUC:       {np.mean(cv_test_aucs):.4f} ± {np.std(cv_test_aucs):.4f}\")\n",
        "else:\n",
        "    print(\"Cross-validation has not been run yet.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
