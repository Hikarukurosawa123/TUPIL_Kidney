{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!git clone https://github.com/Hikarukurosawa123/TUPIL_Kidney.git\n",
        "#!git pull origin main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import KBinsDiscretizer, MinMaxScaler\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
        "from collections import defaultdict\n",
        "from scipy.io import loadmat\n",
        "import h5py\n",
        "import re\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Input configuration - specify which inputs to use\n",
        "# QUS Options: 'ESD', 'EAC', 'SI', 'SS', 'MBF'\n",
        "# Image Options: 'B_mode' (greyscale B-mode images)\n",
        "# Can use any combination\n",
        "INPUT_TYPES = ['ESD', 'EAC', 'SI', 'SS', 'MBF', 'B_mode']  # All QUS parameters + B-mode\n",
        "# Examples:\n",
        "# INPUT_TYPES = ['ESD']  # Only ESD\n",
        "# INPUT_TYPES = ['ESD', 'EAC', 'B_mode']  # ESD + EAC + B-mode\n",
        "# INPUT_TYPES = ['ESD', 'EAC', 'SI', 'SS', 'MBF']  # All QUS only\n",
        "# INPUT_TYPES = ['B_mode']  # Only B-mode\n",
        "\n",
        "# Environment detection\n",
        "def is_google_colab():\n",
        "    \"\"\"Check if running in Google Colab environment\"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "def is_google_drive_mounted():\n",
        "    \"\"\"Check if Google Drive is mounted in Colab\"\"\"\n",
        "    return os.path.exists('/content/drive/MyDrive')\n",
        "\n",
        "# Set paths based on environment\n",
        "if is_google_colab() and is_google_drive_mounted():\n",
        "    # Google Colab with Google Drive mounted\n",
        "    QUS_DATA_DIR = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/QUS_resized'\n",
        "    SAMPLE_ID_FILE = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/QUS_combined/sample_id_combined.mat'\n",
        "    CSV_FILE = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/csv/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
        "    MODEL_WEIGHTS_PATH = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/model_weights/RadImageNet-ResNet50_notop.h5'\n",
        "    B_MODE_IMAGE_FOLDER = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/Bmode_resize'\n",
        "    print(\"Running on Google Colab with Google Drive mounted\")\n",
        "elif is_google_colab():\n",
        "    # Google Colab without Google Drive mounted\n",
        "    QUS_DATA_DIR = '/content/QUS_resized'\n",
        "    SAMPLE_ID_FILE = '/content/QUS_combined/sample_id_combined.mat'\n",
        "    CSV_FILE = '/content/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
        "    MODEL_WEIGHTS_PATH = '/content/model_weights/RadImageNet-ResNet50_notop.h5'\n",
        "    B_MODE_IMAGE_FOLDER = '/content/lanczos_shape_corrected_only_nc_resized_images'\n",
        "    print(\"Running on Google Colab without Google Drive mounted\")\n",
        "else:\n",
        "    # Local environment\n",
        "    QUS_DATA_DIR = 'data/QUS_resized'\n",
        "    SAMPLE_ID_FILE = 'data/QUS_combined/sample_id_combined.mat'\n",
        "    CSV_FILE = 'csv/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
        "    MODEL_WEIGHTS_PATH = 'data/model_weights/RadImageNet-ResNet50_notop.h5'\n",
        "    B_MODE_IMAGE_FOLDER = 'data/lanczos_shape_corrected_only_nc_resized_images'\n",
        "    print(\"Running locally\")\n",
        "\n",
        "print(f\"Selected QUS input types: {INPUT_TYPES}\")\n",
        "print(f\"QUS_DATA_DIR: {QUS_DATA_DIR}\")\n",
        "print(f\"CSV_FILE: {CSV_FILE}\")\n",
        "print(f\"MODEL_WEIGHTS_PATH: {MODEL_WEIGHTS_PATH}\")\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 160\n",
        "SEED = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# =============================\n",
        "# Separate QUS types and image types\n",
        "# =============================\n",
        "QUS_TYPES = [t for t in INPUT_TYPES if t in ['ESD', 'EAC', 'SI', 'SS', 'MBF']]\n",
        "IMAGE_TYPES = [t for t in INPUT_TYPES if t == 'B_mode']\n",
        "\n",
        "print(f\"INPUT_TYPES: {INPUT_TYPES}\")\n",
        "print(f\"QUS_TYPES: {QUS_TYPES}\")\n",
        "print(f\"IMAGE_TYPES: {IMAGE_TYPES}\")\n",
        "\n",
        "# =============================\n",
        "# Helper Functions\n",
        "# =============================\n",
        "\n",
        "def extract_patient_id(sample_id):\n",
        "    \"\"\"Extract patient ID (integer) from sample_id string\"\"\"\n",
        "    match = re.search(r'P(\\d+)', str(sample_id))\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "def extract_matlab_string(cell_item):\n",
        "    \"\"\"Extract string from MATLAB cell array element\"\"\"\n",
        "    if isinstance(cell_item, np.ndarray):\n",
        "        if cell_item.size == 0:\n",
        "            return \"\"\n",
        "        if cell_item.dtype.kind in ['U', 'S']:\n",
        "            return str(cell_item.flat[0])\n",
        "        elif cell_item.dtype == object:\n",
        "            return extract_matlab_string(cell_item.flat[0])\n",
        "        else:\n",
        "            return str(cell_item.flat[0])\n",
        "    else:\n",
        "        return str(cell_item)\n",
        "\n",
        "# =============================\n",
        "# Load QUS matrices\n",
        "# =============================\n",
        "\n",
        "print(\"Loading QUS matrices...\")\n",
        "qus_matrices = {}\n",
        "for qus_name in INPUT_TYPES:\n",
        "    npy_file = os.path.join(QUS_DATA_DIR, f'{qus_name}.npy')\n",
        "    if os.path.exists(npy_file):\n",
        "        qus_matrices[qus_name] = np.load(npy_file)\n",
        "        print(f\"  Loaded {qus_name}: shape {qus_matrices[qus_name].shape}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"QUS file not found: {npy_file}\")\n",
        "\n",
        "# =============================\n",
        "# Check for NaN/Inf in raw data\n",
        "# =============================\n",
        "print(\"\\n=== Checking QUS matrices for NaN/Inf/zero values ===\")\n",
        "for qus_name, qus_array in qus_matrices.items():\n",
        "    total_elements = np.prod(qus_array.shape)\n",
        "    n_nan = np.isnan(qus_array).sum()\n",
        "    n_inf = np.isinf(qus_array).sum()\n",
        "    n_zero = np.sum(qus_array == 0)\n",
        "    print(f\"{qus_name}: shape={qus_array.shape}, NaN={n_nan}, Inf={n_inf}, zeros={n_zero}/{total_elements}\")\n",
        "\n",
        "# =============================\n",
        "# Plot one example case\n",
        "# =============================\n",
        "def plot_qus_case(qus_array, case_idx=0, qus_name=\"QUS\"):\n",
        "    \"\"\"Plot a single case with NaN and zero masks\"\"\"\n",
        "    img = qus_array[:, :, case_idx]\n",
        "\n",
        "    # Create masks\n",
        "    nan_mask = np.isnan(img)\n",
        "    zero_mask = img == 0\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Original image\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(img, cmap='viridis')\n",
        "    plt.title(f\"{qus_name} - Case {case_idx} Original\")\n",
        "    plt.colorbar()\n",
        "\n",
        "    # NaN mask\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(nan_mask, cmap='Reds')\n",
        "    plt.title(f\"{qus_name} - Case {case_idx} NaNs\")\n",
        "\n",
        "    # Zero mask\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(zero_mask, cmap='Blues')\n",
        "    plt.title(f\"{qus_name} - Case {case_idx} Zeros\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example: plot first case of the first QUS type (only if QUS types exist)\n",
        "if len(QUS_TYPES) > 0:\n",
        "    first_qus_name = QUS_TYPES[0]\n",
        "    plot_qus_case(qus_matrices[first_qus_name], case_idx=0, qus_name=first_qus_name)\n",
        "\n",
        "# =============================\n",
        "# Load sample IDs\n",
        "# =============================\n",
        "print(\"\\nLoading sample IDs...\")\n",
        "sample_id_data = loadmat(SAMPLE_ID_FILE, struct_as_record=False, squeeze_me=True)\n",
        "sample_id_keys = [k for k in sample_id_data.keys() if not k.startswith('__')]\n",
        "if len(sample_id_keys) == 0:\n",
        "    raise ValueError(\"No data found in sample_id_combined.mat\")\n",
        "\n",
        "sample_ids_var = sample_id_data[sample_id_keys[0]]\n",
        "\n",
        "# Extract sample IDs\n",
        "if isinstance(sample_ids_var, np.ndarray) and sample_ids_var.dtype == object:\n",
        "    sample_ids = []\n",
        "    for i in range(sample_ids_var.shape[0] if sample_ids_var.ndim > 0 else 1):\n",
        "        item = sample_ids_var[i] if sample_ids_var.ndim == 1 else sample_ids_var[i, 0]\n",
        "        sample_ids.append(extract_matlab_string(item))\n",
        "elif isinstance(sample_ids_var, np.ndarray) and sample_ids_var.dtype.kind in ['U', 'S']:\n",
        "    sample_ids = [str(x) for x in sample_ids_var]\n",
        "elif isinstance(sample_ids_var, (list, tuple)):\n",
        "    sample_ids = [str(s) for s in sample_ids_var]\n",
        "else:\n",
        "    sample_ids = [str(sample_ids_var)]\n",
        "\n",
        "print(f\"Loaded {len(sample_ids)} sample IDs\")\n",
        "print(f\"Sample IDs (first 5): {sample_ids[:5]}\")\n",
        "\n",
        "# Verify all QUS matrices have same number of cases (if QUS types exist)\n",
        "if len(QUS_TYPES) > 0:\n",
        "    n_cases_list = [qus_matrices[qus_name].shape[2] for qus_name in QUS_TYPES]\n",
        "    if len(set(n_cases_list)) > 1:\n",
        "        raise ValueError(f\"QUS matrices have different number of cases: {n_cases_list}\")\n",
        "    n_cases = n_cases_list[0]\n",
        "    print(f\"\\nAll QUS matrices have {n_cases} cases\")\n",
        "\n",
        "# =============================\n",
        "# Load eGFR data\n",
        "# =============================\n",
        "print(\"\\nLoading eGFR data...\")\n",
        "egfr_df = pd.read_csv(CSV_FILE)\n",
        "egfr_dict = {}\n",
        "for _, row in egfr_df.iterrows():\n",
        "    patient_id = int(row['Patient ID'])\n",
        "    egfr_value = row['eGFR (abs/closest)']\n",
        "    if not pd.isna(egfr_value):\n",
        "        egfr_dict[patient_id] = float(egfr_value)\n",
        "print(f\"Loaded eGFR for {len(egfr_dict)} patients\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flexible Patient class for QUS data\n",
        "class Patient:\n",
        "    def __init__(self, patient_id, egfr, egfr_val, case_indices_dict):\n",
        "        self.patient_id = patient_id\n",
        "        self.egfr = egfr  # binary label (0 or 1)\n",
        "        self.egfr_val = egfr_val  # actual eGFR value\n",
        "        self.case_indices_dict = case_indices_dict  # Dictionary with qus_type -> list of case indices\n",
        "\n",
        "    def get_case_indices(self, qus_type):\n",
        "        \"\"\"Get case indices for a specific QUS type\"\"\"\n",
        "        return self.case_indices_dict.get(qus_type, [])\n",
        "\n",
        "    def has_all_inputs(self, required_qus_types):\n",
        "        \"\"\"Check if patient has all required QUS types\"\"\"\n",
        "        return all(len(self.get_case_indices(qus_type)) > 0 for qus_type in required_qus_types)\n",
        "\n",
        "\n",
        "def load_b_mode_images(b_mode_folder):\n",
        "    \"\"\"Load B-mode image paths and organize by patient ID\"\"\"\n",
        "    if not os.path.exists(b_mode_folder):\n",
        "        print(f\"Warning: B-mode folder not found: {b_mode_folder}\")\n",
        "        return {}\n",
        "    \n",
        "    patient_image_map = defaultdict(list)\n",
        "    files = sorted(os.listdir(b_mode_folder))\n",
        "    \n",
        "    for filename in files:\n",
        "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            continue\n",
        "        # Format: Patient_100_Resized_Image_1.png\n",
        "        try:\n",
        "            parts = filename.split('_')\n",
        "            if len(parts) >= 2 and parts[0] == 'Patient':\n",
        "                patient_id = int(parts[1])\n",
        "                full_path = os.path.join(b_mode_folder, filename)\n",
        "                patient_image_map[patient_id].append(full_path)\n",
        "        except (ValueError, IndexError):\n",
        "            continue\n",
        "    \n",
        "    print(f\"Loaded B-mode images for {len(patient_image_map)} patients\")\n",
        "    return patient_image_map\n",
        "\n",
        "def load_patients_from_qus(qus_types, qus_matrices, sample_ids, egfr_dict, b_mode_image_map=None):\n",
        "    \"\"\"\n",
        "    Load patients from QUS matrices and/or B-mode images and match with eGFR data.\n",
        "    \n",
        "    Args:\n",
        "        qus_types: List of QUS types to use (e.g., ['ESD', 'EAC', 'SI', 'SS', 'MBF'])\n",
        "        qus_matrices: Dictionary of QUS matrices (224, 224, n_cases)\n",
        "        sample_ids: List of sample IDs corresponding to cases\n",
        "        egfr_dict: Dictionary mapping patient_id -> eGFR value\n",
        "        b_mode_image_map: Dictionary mapping patient_id -> list of B-mode image paths\n",
        "    \n",
        "    Returns:\n",
        "        List of Patient objects\n",
        "    \"\"\"\n",
        "    # Map patient ID to case indices for each QUS type\n",
        "    patient_case_map = defaultdict(lambda: {qus_type: [] for qus_type in qus_types})\n",
        "    \n",
        "    # For each case, extract patient ID and assign to patient (if QUS types exist)\n",
        "    if len(qus_types) > 0 and len(sample_ids) > 0:\n",
        "        for case_idx in range(len(sample_ids)):\n",
        "            sample_id = sample_ids[case_idx]\n",
        "            patient_id = extract_patient_id(sample_id)\n",
        "            \n",
        "            if patient_id is None:\n",
        "                continue\n",
        "            \n",
        "            if patient_id not in egfr_dict:\n",
        "                continue\n",
        "            \n",
        "            # Add case index to all QUS types (same case index for all QUS parameters)\n",
        "            for qus_type in qus_types:\n",
        "                patient_case_map[patient_id][qus_type].append(case_idx)\n",
        "    \n",
        "    # Add B-mode images to patient_case_map\n",
        "    if b_mode_image_map is not None:\n",
        "        for patient_id, image_paths in b_mode_image_map.items():\n",
        "            if patient_id not in egfr_dict:\n",
        "                continue\n",
        "            patient_case_map[patient_id]['B_mode'] = image_paths\n",
        "    \n",
        "    # Build Patient objects - only include patients with all required input types\n",
        "    patient_objects = []\n",
        "    all_required_types = qus_types + (['B_mode'] if 'B_mode' in INPUT_TYPES else [])\n",
        "    \n",
        "    for patient_id, case_indices_dict in patient_case_map.items():\n",
        "        # Check if patient has all required input types\n",
        "        has_all_types = True\n",
        "        for req_type in all_required_types:\n",
        "            if req_type in qus_types:\n",
        "                if len(case_indices_dict.get(req_type, [])) == 0:\n",
        "                    has_all_types = False\n",
        "                    break\n",
        "            elif req_type == 'B_mode':\n",
        "                if len(case_indices_dict.get('B_mode', [])) == 0:\n",
        "                    has_all_types = False\n",
        "                    break\n",
        "        \n",
        "        if has_all_types:\n",
        "            egfr = egfr_dict[patient_id]\n",
        "            egfrLabel = 1 if egfr >= 60 else 0\n",
        "            patient_objects.append(Patient(\n",
        "                patient_id,\n",
        "                egfrLabel,\n",
        "                egfr,\n",
        "                case_indices_dict\n",
        "            ))\n",
        "\n",
        "    return patient_objects\n",
        "\n",
        "# Load B-mode images if needed\n",
        "b_mode_image_map = None\n",
        "if 'B_mode' in INPUT_TYPES:\n",
        "    print(\"\\nLoading B-mode images...\")\n",
        "    b_mode_image_map = load_b_mode_images(B_MODE_IMAGE_FOLDER)\n",
        "\n",
        "# Load patients\n",
        "print(\"\\nLoading patients from QUS data and/or B-mode images...\")\n",
        "all_patients = load_patients_from_qus(QUS_TYPES, qus_matrices, sample_ids, egfr_dict, b_mode_image_map)\n",
        "print(f\"Total patients with all required input types: {len(all_patients)}\")\n",
        "\n",
        "def summarize_patients_qus(patients, input_types, qus_types):\n",
        "    \"\"\"Summarize the patient data\"\"\"\n",
        "    num_patients = len(patients)\n",
        "    \n",
        "    print(f\"Number of patients: {num_patients}\")\n",
        "    print(f\"Input types: {input_types}\")\n",
        "    \n",
        "    # Count cases for each QUS type\n",
        "    for qus_type in qus_types:\n",
        "        total_cases = sum(len(patient.get_case_indices(qus_type)) for patient in patients)\n",
        "        print(f\"Total {qus_type} cases: {total_cases}\")\n",
        "        \n",
        "        # Distribution of cases per patient\n",
        "        counts = [len(patient.get_case_indices(qus_type)) for patient in patients]\n",
        "        print(f\"{qus_type} - Min: {min(counts)}, Max: {max(counts)}, Avg: {np.mean(counts):.2f}\")\n",
        "    \n",
        "    # Count B-mode images\n",
        "    if 'B_mode' in input_types:\n",
        "        total_images = sum(len(patient.get_case_indices('B_mode')) for patient in patients)\n",
        "        print(f\"Total B_mode images: {total_images}\")\n",
        "        counts = [len(patient.get_case_indices('B_mode')) for patient in patients]\n",
        "        print(f\"B_mode - Min: {min(counts)}, Max: {max(counts)}, Avg: {np.mean(counts):.2f}\")\n",
        "    \n",
        "    return num_patients\n",
        "\n",
        "summarize_patients_qus(all_patients, INPUT_TYPES, QUS_TYPES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import cv2\n",
        "\n",
        "def load_b_mode_image_greyscale(image_path):\n",
        "    \"\"\"Load B-mode image as greyscale (single channel)\n",
        "    \n",
        "    Note: Images are normalized to [0, 1] range. No MinMaxScaler is applied.\n",
        "    \"\"\"\n",
        "    import cv2\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Could not load image: {image_path}\")\n",
        "    # Resize to 224x224 if needed\n",
        "    if img.shape != (224, 224):\n",
        "        img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
        "    # Normalize to [0, 1] - this is standard image normalization, NOT MinMaxScaler\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    return img[..., np.newaxis]  # (224, 224, 1)\n",
        "\n",
        "def create_dataset_from_patients_qus(patients, input_types, qus_types, qus_matrices, scalers=None, augment=False, batch_size=4):\n",
        "    \"\"\"\n",
        "    Creates a tf.data.Dataset for QUS data and/or B-mode images with safe min-max scaling.\n",
        "    NaN regions are replaced with zero.\n",
        "    \n",
        "    Args:\n",
        "        patients: list of Patient objects\n",
        "        input_types: list of all input types (QUS + B_mode)\n",
        "        qus_types: list of QUS types only\n",
        "        qus_matrices: dict of QUS matrices (H, W, n_cases)\n",
        "        scalers: optional dict of fitted MinMaxScalers (for QUS types only)\n",
        "        augment: whether to apply augmentation\n",
        "        batch_size: dataset batch size\n",
        "    \n",
        "    Returns:\n",
        "        dataset, scalers\n",
        "    \"\"\"\n",
        "    qus_data_lists = {qus_type: [] for qus_type in qus_types}\n",
        "    b_mode_data_list = []\n",
        "    labels = []\n",
        "\n",
        "    # Collect data per patient\n",
        "    for patient in patients:\n",
        "        # Get minimum number of cases/images across all input types\n",
        "        all_counts = []\n",
        "        for input_type in input_types:\n",
        "            if input_type in qus_types:\n",
        "                all_counts.append(len(patient.get_case_indices(input_type)))\n",
        "            elif input_type == 'B_mode':\n",
        "                all_counts.append(len(patient.get_case_indices('B_mode')))\n",
        "        \n",
        "        min_cases = min(all_counts) if all_counts else 0\n",
        "        \n",
        "        for i in range(min_cases):\n",
        "            # Collect QUS data (if QUS types exist)\n",
        "            if len(qus_types) > 0:\n",
        "                for qus_type in qus_types:\n",
        "                    case_idx = patient.get_case_indices(qus_type)[i]\n",
        "                    qus_map = qus_matrices[qus_type][:, :, case_idx]\n",
        "                    # Replace NaNs with zero\n",
        "                    qus_map = np.nan_to_num(qus_map, nan=0.0)\n",
        "                    qus_data_lists[qus_type].append(qus_map)\n",
        "            \n",
        "            # Collect B-mode data\n",
        "            if 'B_mode' in input_types:\n",
        "                b_mode_path = patient.get_case_indices('B_mode')[i]\n",
        "                b_mode_img = load_b_mode_image_greyscale(b_mode_path)\n",
        "                b_mode_data_list.append(b_mode_img)\n",
        "            \n",
        "            labels.append(patient.egfr)\n",
        "\n",
        "    # Convert to numpy arrays and add channel dimension for QUS\n",
        "    for qus_type in qus_types:\n",
        "        if len(qus_data_lists[qus_type]) > 0:\n",
        "            qus_data_lists[qus_type] = np.array(qus_data_lists[qus_type])[..., np.newaxis]  # (n_samples, H, W, 1)\n",
        "\n",
        "    # Convert B-mode to numpy array (already has channel dimension)\n",
        "    # Note: B-mode images are normalized to [0, 1] when loaded - no MinMaxScaler needed\n",
        "    if 'B_mode' in input_types:\n",
        "        b_mode_data = np.array(b_mode_data_list)  # (n_samples, H, W, 1)\n",
        "        print(f\"B_mode images: shape={b_mode_data.shape}, range=[{np.min(b_mode_data):.4f}, {np.max(b_mode_data):.4f}] (no scaling applied)\")\n",
        "\n",
        "    # Fit scalers if not provided (only for QUS types)\n",
        "    if scalers is None:\n",
        "        scalers = {}\n",
        "        for qus_type in qus_types:\n",
        "            if len(qus_data_lists[qus_type]) > 0:\n",
        "                scaler = MinMaxScaler()\n",
        "                data_flat = qus_data_lists[qus_type].reshape(-1, 1)\n",
        "                scaler.fit(data_flat)\n",
        "                scalers[qus_type] = scaler\n",
        "                print(f\"Fitted scaler for {qus_type}: min={scaler.data_min_[0]:.4f}, max={scaler.data_max_[0]:.4f}\")\n",
        "\n",
        "    # Apply scaling safely (only for QUS types)\n",
        "    scaled_qus_data = []\n",
        "    for qus_type in qus_types:\n",
        "        if len(qus_data_lists[qus_type]) > 0:\n",
        "            data = qus_data_lists[qus_type]\n",
        "            scaler = scalers[qus_type]\n",
        "            data_flat = data.reshape(-1, 1)\n",
        "            scaled_flat = scaler.transform(data_flat)\n",
        "            scaled = scaled_flat.reshape(data.shape)\n",
        "            scaled_qus_data.append(scaled)\n",
        "            print(f\"Scaled {qus_type}: shape={scaled.shape}, range=[{np.min(scaled):.4f}, {np.max(scaled):.4f}]\")\n",
        "\n",
        "    # Concatenate all inputs along channels\n",
        "    # QUS data is scaled, B-mode is already normalized (no scaling needed)\n",
        "    all_inputs = scaled_qus_data.copy()\n",
        "    if 'B_mode' in input_types:\n",
        "        all_inputs.append(b_mode_data)  # B-mode already in [0, 1] range, no scaling\n",
        "    \n",
        "    # Handle case where there might be only one input type\n",
        "    if len(all_inputs) == 1:\n",
        "        combined_input = all_inputs[0]\n",
        "    else:\n",
        "        combined_input = np.concatenate(all_inputs, axis=-1)  # (n_samples, H, W, num_channels)\n",
        "    labels = np.array(labels)\n",
        "    print(f\"Combined input shape: {combined_input.shape}\")\n",
        "\n",
        "    # Create tf.data.Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((combined_input, labels)).batch(batch_size)\n",
        "\n",
        "    # Data augmentation\n",
        "    if augment:\n",
        "        data_augmentation = tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "            tf.keras.layers.RandomRotation(0.25),\n",
        "            tf.keras.layers.RandomZoom(0.1),\n",
        "        ])\n",
        "        def augment_fn(inputs, label):\n",
        "            augmented = data_augmentation(inputs, training=True)\n",
        "            return augmented, label\n",
        "        dataset = dataset.map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset.prefetch(tf.data.AUTOTUNE), scalers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_resnet_model_flexible_qus(input_types, with_transfer_learning=True):\n",
        "    \"\"\"\n",
        "    Build ResNet model for flexible QUS input types.\n",
        "\n",
        "        Args:\n",
        "        input_types: List of input types (e.g., ['ESD', 'EAC', 'SI', 'SS', 'MBF', 'B_mode'])\n",
        "        with_transfer_learning: Whether to use transfer learning\n",
        "    \n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    \n",
        "    if len(input_types) == 1:\n",
        "                # Single input model\n",
        "        input_type = input_types[0]\n",
        "        input_layer = layers.Input(shape=(224, 224, 1), name=f'{input_type}_input')\n",
        "        inputs = input_layer\n",
        "        \n",
        "        # Convert single channel to 3 channels for ResNet (repeat channel)\n",
        "        x = layers.Concatenate()([input_layer, input_layer, input_layer])\n",
        "\n",
        "        # Load base model\n",
        "        if with_transfer_learning:\n",
        "            base_model = models.load_model(MODEL_WEIGHTS_PATH, compile=False)\n",
        "        else:\n",
        "            base_model = tf.keras.applications.ResNet50(\n",
        "                weights=None,\n",
        "                include_top=False,\n",
        "                input_shape=(224, 224, 3)\n",
        "            )\n",
        "\n",
        "        base_model.trainable = True\n",
        "        x = base_model(x)\n",
        "\n",
        "    else:\n",
        "        # Multiple input model\n",
        "        input_layers = []\n",
        "        for input_type in input_types:\n",
        "            input_layer = layers.Input(shape=(224, 224, 1), name=f'{input_type}_input')\n",
        "            input_layers.append(input_layer)\n",
        "\n",
        "        # Convert each single channel to 3 channels for ResNet\n",
        "        expanded_inputs = []\n",
        "        for input_layer in input_layers:\n",
        "            expanded = layers.Concatenate()([input_layer, input_layer, input_layer])\n",
        "            expanded_inputs.append(expanded)\n",
        "\n",
        "        # Load base model\n",
        "        if with_transfer_learning:\n",
        "            base_model = models.load_model(MODEL_WEIGHTS_PATH, compile=False)\n",
        "        else:\n",
        "            base_model = tf.keras.applications.ResNet50(\n",
        "                weights=None,\n",
        "                include_top=False,\n",
        "                input_shape=(224, 224, 3)\n",
        "            )\n",
        "\n",
        "        base_model.trainable = True\n",
        "\n",
        "        # Process each input through the base model\n",
        "        feature_maps = []\n",
        "        for expanded_input in expanded_inputs:\n",
        "            features = base_model(expanded_input)\n",
        "            pooled = layers.GlobalAveragePooling2D()(features)\n",
        "            feature_maps.append(pooled)\n",
        "\n",
        "        # Concatenate features from all inputs\n",
        "        x = layers.Concatenate()(feature_maps)\n",
        "\n",
        "    # Global average pooling for single input\n",
        "    if len(input_types) == 1:\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Dense layers for classification\n",
        "    x = layers.Dense(4096, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(2048, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # Final binary classification\n",
        "    output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Create model\n",
        "    if len(input_types) == 1:\n",
        "        model = models.Model(inputs=inputs, outputs=output)\n",
        "    else:\n",
        "        model = models.Model(inputs=input_layers, outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_resnet_model_flexible_concatenated_qus(input_types, qus_types, with_transfer_learning=False):\n",
        "    \"\"\"\n",
        "    Alternative architecture: concatenate QUS inputs first, then process through single model.\n",
        "\n",
        "    Args:\n",
        "        qus_types: List of QUS types (e.g., ['ESD', 'EAC', 'SI', 'SS', 'MBF'])\n",
        "        with_transfer_learning: Whether to use transfer learning\n",
        "\n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "\n",
        "    if len(input_types) == 1:\n",
        "        # Single input - same as regular model\n",
        "        return build_resnet_model_flexible_qus(input_types, with_transfer_learning)\n",
        "    \n",
        "    # Multiple inputs - concatenate along channel axis\n",
        "    # Note: We already concatenate in the dataset creation, so we have a single input\n",
        "    num_channels = len(qus_types) + (1 if 'B_mode' in input_types else 0)\n",
        "    input_layer = layers.Input(shape=(224, 224, num_channels), name='combined_input')\n",
        "    concatenated_input = input_layer\n",
        "\n",
        "    # # Convert to 3 channels for ResNet (repeat channels if needed)\n",
        "    # num_channels = len(qus_types)\n",
        "    # if num_channels == 1:\n",
        "    #     x = layers.Concatenate()([concatenated_input, concatenated_input, concatenated_input])\n",
        "    # elif num_channels == 2:\n",
        "    #     x = layers.Concatenate()([concatenated_input, concatenated_input[..., 0:1]])\n",
        "    # elif num_channels >= 3:\n",
        "    #     x = concatenated_input[..., :3]  # Take first 3 channels\n",
        "\n",
        "    # Load base model\n",
        "    if with_transfer_learning:\n",
        "        base_model = models.load_model(MODEL_WEIGHTS_PATH, compile=False)\n",
        "        # Need to adapt the input shape if it doesn't match\n",
        "        if num_channels != 3:\n",
        "            # Create a wrapper to handle different channel counts\n",
        "            # For now, we'll use a custom ResNet50-like architecture\n",
        "            base_model = None\n",
        "    \n",
        "    if base_model is None or num_channels != 3:\n",
        "        # Use ResNet50 without transfer learning for non-3-channel inputs\n",
        "        base_model = tf.keras.applications.ResNet50(\n",
        "            weights=None,\n",
        "            include_top=False,\n",
        "            input_shape=(224, 224, num_channels)\n",
        "        )\n",
        "\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # Process concatenated input\n",
        "    x = base_model(concatenated_input)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(4096, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(2048, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # Final binary classification\n",
        "    output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inputs=input_layer, outputs=output)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plotAndReturnValidationTesting_flexible_qus(val_patients, model, input_types, qus_types, qus_matrices, scalers):\n",
        "    \"\"\"Updated validation testing function for flexible QUS input models\"\"\"\n",
        "    patient_to_true_labels = {}\n",
        "    patient_to_predicted_probs_list = {}\n",
        "\n",
        "    all_true_labels_individual = []\n",
        "    all_predicted_probs_individual = []\n",
        "\n",
        "    # ================= Collect predictions =================\n",
        "    for patient in val_patients:\n",
        "        patient_dataset, _ = create_dataset_from_patients_qus([patient], INPUT_TYPES, QUS_TYPES, qus_matrices, scalers=scalers, augment=False, batch_size=BATCH_SIZE)\n",
        "        for inputs, labels in patient_dataset:\n",
        "            true_label = labels[0].numpy()  # Single label per patient\n",
        "            predictions = model.predict(inputs, verbose=0).flatten()\n",
        "\n",
        "            patient_to_true_labels[patient.patient_id] = true_label\n",
        "            patient_to_predicted_probs_list[patient.patient_id] = predictions.tolist()\n",
        "\n",
        "            # Collect image-level predictions for metrics\n",
        "            all_true_labels_individual.extend(labels.numpy().flatten())\n",
        "            all_predicted_probs_individual.extend(predictions)\n",
        "\n",
        "    # ================= Image-level ROC =================\n",
        "    all_predicted_labels_individual = [1 if prob >= 0.5 else 0 for prob in all_predicted_probs_individual]\n",
        "    fpr_img, tpr_img, _ = roc_curve(all_true_labels_individual, all_predicted_probs_individual)\n",
        "    auc_img = roc_auc_score(all_true_labels_individual, all_predicted_probs_individual)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr_img, tpr_img, label=f\"Image-level ROC (AUC = {auc_img:.4f})\", color='blue')\n",
        "    plt.plot([0, 1], [0, 1], 'r--', label=\"Random Guess\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(f\"Image-level ROC Curve - {', '.join(input_types)}\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # ================= Patient-level ROC =================\n",
        "    patient_true = []\n",
        "    patient_probs = []\n",
        "\n",
        "    for pid in patient_to_predicted_probs_list:\n",
        "        patient_true.append(patient_to_true_labels[pid])\n",
        "        patient_probs.append(np.mean(patient_to_predicted_probs_list[pid]))  # average across cases\n",
        "\n",
        "    fpr_pat, tpr_pat, _ = roc_curve(patient_true, patient_probs)\n",
        "    auc_pat = roc_auc_score(patient_true, patient_probs)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr_pat, tpr_pat, label=f\"Patient-level ROC (AUC = {auc_pat:.4f})\", color='green')\n",
        "    plt.plot([0, 1], [0, 1], 'r--', label=\"Random Guess\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(f\"Patient-level ROC Curve - {', '.join(input_types)}\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return patient_to_true_labels, patient_to_predicted_probs_list\n",
        "\n",
        "\n",
        "# Updated hyperparameter tuning configurations for flexible QUS input\n",
        "def resnet_flexible_branched_qus(input_types):\n",
        "    \"\"\"Flexible QUS input with separate branches\"\"\"\n",
        "    model = build_resnet_model_flexible_qus(input_types, with_transfer_learning=True)\n",
        "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "    return model\n",
        "\n",
        "def resnet_flexible_concatenated_qus(input_types, qus_types):\n",
        "    \"\"\"Flexible QUS input with concatenated channels\"\"\"\n",
        "    model = build_resnet_model_flexible_concatenated_qus(input_types, qus_types, with_transfer_learning=False)\n",
        "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "    return model\n",
        "\n",
        "def resnet_flexible_no_transfer_qus(input_types):\n",
        "    \"\"\"Flexible QUS input without transfer learning\"\"\"\n",
        "    model = build_resnet_model_flexible_qus(input_types, with_transfer_learning=False)\n",
        "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "    return model\n",
        "\n",
        "# Create hyperparameter configurations based on selected input types\n",
        "def create_hyperparameter_configs_qus(input_types, qus_types):\n",
        "    \"\"\"Create hyperparameter configurations for the selected input types\"\"\"\n",
        "    return {\n",
        "        f'resnet_flexible_branched_{'_'.join(input_types)}': lambda: resnet_flexible_branched_qus(input_types),\n",
        "        f'resnet_flexible_concatenated_{'_'.join(input_types)}': lambda: resnet_flexible_concatenated_qus(input_types, qus_types),\n",
        "        f'resnet_flexible_no_transfer_{'_'.join(input_types)}': lambda: resnet_flexible_no_transfer_qus(input_types)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "N_RUNS = 5\n",
        "val_aucs, test_aucs = [], []\n",
        "\n",
        "for run_idx in range(N_RUNS):\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"▶️ HOLD-OUT RUN {run_idx+1}/{N_RUNS}\")\n",
        "    print(f\"{'='*40}\\n\")\n",
        "\n",
        "    # Split into train/val/test with new random state each run\n",
        "    SEED_RUN = SEED + run_idx  # different seed each run\n",
        "    train_and_val_patients, test_patients = train_test_split(\n",
        "        all_patients, test_size=0.1, random_state=SEED_RUN\n",
        "    )\n",
        "    train_patients, val_patients = train_test_split(\n",
        "        train_and_val_patients, test_size=0.2, random_state=SEED_RUN\n",
        "    )\n",
        "\n",
        "            print(f\"Training patients: {len(train_patients)}\")\n",
        "        print(f\"Validation patients: {len(val_patients)}\")\n",
        "        print(f\"Test patients: {len(test_patients)}\")\n",
        "\n",
        "        # Create datasets with min-max scaling\n",
        "        # Fit scalers on training data only\n",
        "        print(\"\\nCreating training dataset and fitting scalers...\")\n",
        "        train_dataset, train_scalers = create_dataset_from_patients_qus(\n",
        "            train_patients, INPUT_TYPES, QUS_TYPES, qus_matrices, scalers=None, augment=True, batch_size=BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        # Use the same scalers for validation and test\n",
        "        print(\"\\nCreating validation dataset (using training scalers)...\")\n",
        "        val_dataset, _ = create_dataset_from_patients_qus(\n",
        "            val_patients, INPUT_TYPES, QUS_TYPES, qus_matrices, scalers=train_scalers, augment=False, batch_size=BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        print(\"\\nCreating test dataset (using training scalers)...\")\n",
        "        test_dataset, _ = create_dataset_from_patients_qus(\n",
        "            test_patients, INPUT_TYPES, QUS_TYPES, qus_matrices, scalers=train_scalers, augment=False, batch_size=BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        # Compute class weights\n",
        "        weights = class_weight.compute_class_weight(\n",
        "            class_weight='balanced',\n",
        "            classes=np.unique(np.array([p.egfr for p in train_patients]).astype(int)),\n",
        "            y=np.array([p.egfr for p in train_patients]).astype(int)\n",
        "        )\n",
        "        class_weights_dict = dict(enumerate(weights))\n",
        "        print(f\"Class weights: {class_weights_dict}\")\n",
        "\n",
        "        # Get model configuration\n",
        "        #hyperparameter_configs = create_hyperparameter_configs_qus(INPUT_TYPES, QUS_TYPES)\n",
        "        #MODEL_CONFIG_NAME = f'resnet_flexible_branched_{'_'.join(INPUT_TYPES)}'\n",
        "        #print(f\"Using model configuration: {MODEL_CONFIG_NAME}\")\n",
        "\n",
        "        # Build and compile model\n",
        "        model = resnet_flexible_concatenated_qus(INPUT_TYPES, QUS_TYPES)\n",
        "\n",
        "        # Early stopping and LR scheduler\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_auc', mode='max', patience=20, restore_best_weights=True\n",
        "        )\n",
        "        # def step_decay(epoch, lr):\n",
        "        #     drop_rate = 0.5\n",
        "        #     drop_every = 15\n",
        "        #     if epoch > 0 and epoch % drop_every == 0:\n",
        "        #         return lr * drop_rate\n",
        "        #     return lr\n",
        "        # lr_scheduler = tf.keras.callbacks.LearningRateScheduler(step_decay, verbose=0)\n",
        "\n",
        "        print(\"Starting training...\")\n",
        "        history = model.fit(\n",
        "            train_dataset,\n",
        "            validation_data=val_dataset,\n",
        "            epochs=EPOCHS,\n",
        "            callbacks=[early_stopping],\n",
        "            class_weight=class_weights_dict,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # === Evaluate Validation ===\n",
        "        print(\"\\nEvaluating on validation set...\")\n",
        "        val_true, val_pred = [], []\n",
        "        patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting_flexible_qus(val_patients, model, INPUT_TYPES, QUS_TYPES, qus_matrices, train_scalers)\n",
        "        for patient_id in patient_to_predicted_probs:\n",
        "            for prob in patient_to_predicted_probs[patient_id]:\n",
        "                val_true.append(patient_to_true_labels[patient_id])\n",
        "                val_pred.append(prob)\n",
        "        val_auc = roc_auc_score(val_true, val_pred)\n",
        "        val_aucs.append(val_auc)\n",
        "        print(f\"Validation AUC (Run {run_idx+1}): {val_auc:.4f}\")\n",
        "\n",
        "        # === Evaluate Test ===\n",
        "        print(\"\\nEvaluating on test set...\")\n",
        "        test_true, test_pred = [], []\n",
        "        patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting_flexible_qus(test_patients, model, INPUT_TYPES, QUS_TYPES, qus_matrices, train_scalers)\n",
        "        for patient_id in patient_to_predicted_probs:\n",
        "            for prob in patient_to_predicted_probs[patient_id]:\n",
        "                test_true.append(patient_to_true_labels[patient_id])\n",
        "                test_pred.append(prob)\n",
        "        test_auc = roc_auc_score(test_true, test_pred)\n",
        "        test_aucs.append(test_auc)\n",
        "        print(f\"Test AUC (Run {run_idx+1}): {test_auc:.4f}\")\n",
        "\n",
        "        print(classification_report(test_true, [1 if p >= 0.5 else 0 for p in test_pred], digits=4))\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# ✅ Final Summary of 5 Runs\n",
        "# ===========================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL SUMMARY OVER MULTIPLE HOLD-OUT RUNS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Validation AUCs: {['%.4f' % a for a in val_aucs]}\")\n",
        "print(f\"Test AUCs:       {['%.4f' % a for a in test_aucs]}\")\n",
        "print(f\"\\nAverage Validation AUC: {np.mean(val_aucs):.4f} ± {np.std(val_aucs):.4f}\")\n",
        "print(f\"Average Test AUC:       {np.mean(test_aucs):.4f} ± {np.std(test_aucs):.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
