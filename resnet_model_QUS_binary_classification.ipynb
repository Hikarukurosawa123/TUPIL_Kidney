{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!git clone https://github.com/Hikarukurosawa123/TUPIL_Kidney.git\n",
        "#!git pull origin main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import KBinsDiscretizer, MinMaxScaler\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
        "from collections import defaultdict\n",
        "from scipy.io import loadmat\n",
        "import h5py\n",
        "import re\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# QUS input configuration - specify which QUS parameters to use\n",
        "# Options: 'ESD', 'EAC', 'SI', 'SS', 'MBF' - can use any combination\n",
        "INPUT_TYPES = ['ESD', 'EAC', 'SI', 'SS', 'MBF']  # All QUS parameters\n",
        "# Examples:\n",
        "# INPUT_TYPES = ['ESD']  # Only ESD\n",
        "# INPUT_TYPES = ['ESD', 'EAC']  # ESD + EAC\n",
        "# INPUT_TYPES = ['ESD', 'EAC', 'SI', 'SS', 'MBF']  # All five\n",
        "\n",
        "# Environment detection\n",
        "def is_google_colab():\n",
        "    \"\"\"Check if running in Google Colab environment\"\"\"\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "def is_google_drive_mounted():\n",
        "    \"\"\"Check if Google Drive is mounted in Colab\"\"\"\n",
        "    return os.path.exists('/content/drive/MyDrive')\n",
        "\n",
        "# Set paths based on environment\n",
        "if is_google_colab() and is_google_drive_mounted():\n",
        "    # Google Colab with Google Drive mounted\n",
        "    QUS_DATA_DIR = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/QUS_resized'\n",
        "    SAMPLE_ID_FILE = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/QUS_combined/sample_id_combined.mat'\n",
        "    CSV_FILE = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/csv/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
        "    MODEL_WEIGHTS_PATH = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/model_weights/RadImageNet-ResNet50_notop.h5'\n",
        "    print(\"Running on Google Colab with Google Drive mounted\")\n",
        "elif is_google_colab():\n",
        "    # Google Colab without Google Drive mounted\n",
        "    QUS_DATA_DIR = '/content/QUS_resized'\n",
        "    SAMPLE_ID_FILE = '/content/QUS_combined/sample_id_combined.mat'\n",
        "    CSV_FILE = '/content/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
        "    MODEL_WEIGHTS_PATH = '/content/model_weights/RadImageNet-ResNet50_notop.h5'\n",
        "    print(\"Running on Google Colab without Google Drive mounted\")\n",
        "else:\n",
        "    # Local environment\n",
        "    QUS_DATA_DIR = 'data/QUS_resized'\n",
        "    SAMPLE_ID_FILE = 'data/QUS_combined/sample_id_combined.mat'\n",
        "    CSV_FILE = 'csv/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
        "    MODEL_WEIGHTS_PATH = 'data/model_weights/RadImageNet-ResNet50_notop.h5'\n",
        "    print(\"Running locally\")\n",
        "\n",
        "print(f\"Selected QUS input types: {INPUT_TYPES}\")\n",
        "print(f\"QUS_DATA_DIR: {QUS_DATA_DIR}\")\n",
        "print(f\"CSV_FILE: {CSV_FILE}\")\n",
        "print(f\"MODEL_WEIGHTS_PATH: {MODEL_WEIGHTS_PATH}\")\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 160\n",
        "SEED = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to extract patient ID from sample_id\n",
        "def extract_patient_id(sample_id):\n",
        "    \"\"\"Extract patient ID (integer) from sample_id string\"\"\"\n",
        "    match = re.search(r'P(\\d+)', str(sample_id))\n",
        "    if match:\n",
        "        return int(match.group(1))\n",
        "    return None\n",
        "\n",
        "# Helper function to extract MATLAB string from cell array element\n",
        "def extract_matlab_string(cell_item):\n",
        "    \"\"\"Extract string from MATLAB cell array element\"\"\"\n",
        "    if isinstance(cell_item, np.ndarray):\n",
        "        if cell_item.size == 0:\n",
        "            return \"\"\n",
        "        if cell_item.dtype.kind in ['U', 'S']:\n",
        "            return str(cell_item.flat[0])\n",
        "        elif cell_item.dtype == object:\n",
        "            return extract_matlab_string(cell_item.flat[0])\n",
        "        else:\n",
        "            return str(cell_item.flat[0])\n",
        "    else:\n",
        "        return str(cell_item)\n",
        "\n",
        "# Load QUS matrices and sample IDs\n",
        "print(\"Loading QUS matrices...\")\n",
        "qus_matrices = {}\n",
        "for qus_name in INPUT_TYPES:\n",
        "    npy_file = os.path.join(QUS_DATA_DIR, f'{qus_name}.npy')\n",
        "    if os.path.exists(npy_file):\n",
        "        qus_matrices[qus_name] = np.load(npy_file)\n",
        "        print(f\"  Loaded {qus_name}: shape {qus_matrices[qus_name].shape}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"QUS file not found: {npy_file}\")\n",
        "\n",
        "# Load sample IDs\n",
        "print(\"\\nLoading sample IDs...\")\n",
        "sample_id_data = loadmat(SAMPLE_ID_FILE, struct_as_record=False, squeeze_me=True)\n",
        "sample_id_keys = [k for k in sample_id_data.keys() if not k.startswith('__')]\n",
        "if len(sample_id_keys) == 0:\n",
        "    raise ValueError(\"No data found in sample_id_combined.mat\")\n",
        "\n",
        "sample_ids_var = sample_id_data[sample_id_keys[0]]\n",
        "\n",
        "# Extract sample IDs\n",
        "if isinstance(sample_ids_var, np.ndarray) and sample_ids_var.dtype == object:\n",
        "    sample_ids = []\n",
        "    for i in range(sample_ids_var.shape[0] if sample_ids_var.ndim > 0 else 1):\n",
        "        item = sample_ids_var[i] if sample_ids_var.ndim == 1 else sample_ids_var[i, 0]\n",
        "        sample_ids.append(extract_matlab_string(item))\n",
        "elif isinstance(sample_ids_var, np.ndarray) and sample_ids_var.dtype.kind in ['U', 'S']:\n",
        "    sample_ids = [str(x) for x in sample_ids_var]\n",
        "elif isinstance(sample_ids_var, (list, tuple)):\n",
        "    sample_ids = [str(s) for s in sample_ids_var]\n",
        "else:\n",
        "    sample_ids = [str(sample_ids_var)]\n",
        "\n",
        "print(f\"Loaded {len(sample_ids)} sample IDs\")\n",
        "print(f\"Sample IDs (first 5): {sample_ids[:5]}\")\n",
        "\n",
        "# Verify all QUS matrices have the same number of cases\n",
        "n_cases_list = [qus_matrices[qus_name].shape[2] for qus_name in INPUT_TYPES]\n",
        "if len(set(n_cases_list)) > 1:\n",
        "    raise ValueError(f\"QUS matrices have different number of cases: {n_cases_list}\")\n",
        "n_cases = n_cases_list[0]\n",
        "print(f\"\\nAll QUS matrices have {n_cases} cases\")\n",
        "\n",
        "# Load eGFR data\n",
        "print(\"\\nLoading eGFR data...\")\n",
        "egfr_df = pd.read_csv(CSV_FILE)\n",
        "egfr_dict = {}\n",
        "for _, row in egfr_df.iterrows():\n",
        "    patient_id = int(row['Patient ID'])\n",
        "    egfr_value = row['eGFR (abs/closest)']\n",
        "    if not pd.isna(egfr_value):\n",
        "        egfr_dict[patient_id] = float(egfr_value)\n",
        "print(f\"Loaded eGFR for {len(egfr_dict)} patients\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flexible Patient class for QUS data\n",
        "class Patient:\n",
        "    def __init__(self, patient_id, egfr, egfr_val, case_indices_dict):\n",
        "        self.patient_id = patient_id\n",
        "        self.egfr = egfr  # binary label (0 or 1)\n",
        "        self.egfr_val = egfr_val  # actual eGFR value\n",
        "        self.case_indices_dict = case_indices_dict  # Dictionary with qus_type -> list of case indices\n",
        "    \n",
        "    def get_case_indices(self, qus_type):\n",
        "        \"\"\"Get case indices for a specific QUS type\"\"\"\n",
        "        return self.case_indices_dict.get(qus_type, [])\n",
        "    \n",
        "    def has_all_inputs(self, required_qus_types):\n",
        "        \"\"\"Check if patient has all required QUS types\"\"\"\n",
        "        return all(len(self.get_case_indices(qus_type)) > 0 for qus_type in required_qus_types)\n",
        "\n",
        "\n",
        "def load_patients_from_qus(qus_types, qus_matrices, sample_ids, egfr_dict):\n",
        "    \"\"\"\n",
        "    Load patients from QUS matrices and match with eGFR data.\n",
        "    \n",
        "    Args:\n",
        "        qus_types: List of QUS types to use (e.g., ['ESD', 'EAC', 'SI', 'SS', 'MBF'])\n",
        "        qus_matrices: Dictionary of QUS matrices (224, 224, n_cases)\n",
        "        sample_ids: List of sample IDs corresponding to cases\n",
        "        egfr_dict: Dictionary mapping patient_id -> eGFR value\n",
        "    \n",
        "    Returns:\n",
        "        List of Patient objects\n",
        "    \"\"\"\n",
        "    # Map patient ID to case indices for each QUS type\n",
        "    patient_case_map = defaultdict(lambda: {qus_type: [] for qus_type in qus_types})\n",
        "    \n",
        "    # For each case, extract patient ID and assign to patient\n",
        "    for case_idx in range(len(sample_ids)):\n",
        "        sample_id = sample_ids[case_idx]\n",
        "        patient_id = extract_patient_id(sample_id)\n",
        "        \n",
        "        if patient_id is None:\n",
        "            continue\n",
        "        \n",
        "        if patient_id not in egfr_dict:\n",
        "            continue\n",
        "        \n",
        "        # Add case index to all QUS types (same case index for all QUS parameters)\n",
        "        for qus_type in qus_types:\n",
        "            patient_case_map[patient_id][qus_type].append(case_idx)\n",
        "    \n",
        "    # Build Patient objects - only include patients with all required QUS types\n",
        "    patient_objects = []\n",
        "    for patient_id, case_indices_dict in patient_case_map.items():\n",
        "        # Check if patient has all required QUS types\n",
        "        has_all_types = all(len(case_indices_dict[qus_type]) > 0 for qus_type in qus_types)\n",
        "        \n",
        "        if has_all_types:\n",
        "            egfr = egfr_dict[patient_id]\n",
        "            egfrLabel = 1 if egfr >= 60 else 0\n",
        "            patient_objects.append(Patient(\n",
        "                patient_id,\n",
        "                egfrLabel,\n",
        "                egfr,\n",
        "                case_indices_dict\n",
        "            ))\n",
        "\n",
        "    return patient_objects\n",
        "\n",
        "# Load patients\n",
        "print(\"Loading patients from QUS data...\")\n",
        "all_patients = load_patients_from_qus(INPUT_TYPES, qus_matrices, sample_ids, egfr_dict)\n",
        "print(f\"Total patients with all QUS types: {len(all_patients)}\")\n",
        "\n",
        "def summarize_patients_qus(patients, qus_types):\n",
        "    \"\"\"Summarize the QUS patient data\"\"\"\n",
        "    num_patients = len(patients)\n",
        "    \n",
        "    print(f\"Number of patients: {num_patients}\")\n",
        "    print(f\"QUS types: {qus_types}\")\n",
        "    \n",
        "    # Count cases for each QUS type\n",
        "    for qus_type in qus_types:\n",
        "        total_cases = sum(len(patient.get_case_indices(qus_type)) for patient in patients)\n",
        "        print(f\"Total {qus_type} cases: {total_cases}\")\n",
        "        \n",
        "        # Distribution of cases per patient\n",
        "        counts = [len(patient.get_case_indices(qus_type)) for patient in patients]\n",
        "        print(f\"{qus_type} - Min: {min(counts)}, Max: {max(counts)}, Avg: {np.mean(counts):.2f}\")\n",
        "    \n",
        "    return num_patients\n",
        "\n",
        "summarize_patients_qus(all_patients, INPUT_TYPES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def create_dataset_from_patients_qus(patients, qus_types, qus_matrices, scalers=None, augment=False, batch_size=4):\n",
        "    \"\"\"\n",
        "    Creates a tf.data.Dataset for QUS data with min-max scaling.\n",
        "    \n",
        "    Args:\n",
        "        patients: List of Patient objects\n",
        "        qus_types: List of QUS types to use\n",
        "        qus_matrices: Dictionary of QUS matrices (224, 224, n_cases)\n",
        "        scalers: Dictionary of fitted MinMaxScalers (one per QUS type). If None, will fit on data.\n",
        "        augment: Whether to apply data augmentation\n",
        "        batch_size: Batch size for the dataset\n",
        "    \n",
        "    Returns:\n",
        "        tf.data.Dataset with flexible QUS inputs and fitted scalers\n",
        "    \"\"\"\n",
        "    \n",
        "    # Collect QUS data and labels\n",
        "    qus_data_lists = {qus_type: [] for qus_type in qus_types}\n",
        "    labels = []\n",
        "    \n",
        "    for patient in patients:\n",
        "        # Get the minimum number of cases across all QUS types for this patient\n",
        "        min_cases = min(len(patient.get_case_indices(qus_type)) for qus_type in qus_types)\n",
        "        \n",
        "        for i in range(min_cases):\n",
        "            # Extract QUS maps for each type\n",
        "            for qus_type in qus_types:\n",
        "                case_idx = patient.get_case_indices(qus_type)[i]\n",
        "                qus_map = qus_matrices[qus_type][:, :, case_idx]  # Extract (224, 224) slice\n",
        "                qus_data_lists[qus_type].append(qus_map)\n",
        "            labels.append(patient.egfr)\n",
        "    \n",
        "    # Convert to numpy arrays\n",
        "    for qus_type in qus_types:\n",
        "        qus_data_lists[qus_type] = np.array(qus_data_lists[qus_type])\n",
        "        # Add channel dimension: (n_samples, 224, 224) -> (n_samples, 224, 224, 1)\n",
        "        qus_data_lists[qus_type] = np.expand_dims(qus_data_lists[qus_type], axis=-1)\n",
        "    \n",
        "    # Apply min-max scaling\n",
        "    if scalers is None:\n",
        "        # Fit scalers on training data\n",
        "        scalers = {}\n",
        "        for qus_type in qus_types:\n",
        "            scaler = MinMaxScaler()\n",
        "            # Reshape to (n_samples * 224 * 224, 1) for fitting\n",
        "            data_flat = qus_data_lists[qus_type].reshape(-1, 1)\n",
        "            scaler.fit(data_flat)\n",
        "            scalers[qus_type] = scaler\n",
        "            print(f\"Fitted scaler for {qus_type}: min={scaler.data_min_[0]:.4f}, max={scaler.data_max_[0]:.4f}\")\n",
        "    \n",
        "    # Transform data using scalers\n",
        "    scaled_qus_data = {}\n",
        "    for qus_type in qus_types:\n",
        "        data_flat = qus_data_lists[qus_type].reshape(-1, 1)\n",
        "        scaled_flat = scalers[qus_type].transform(data_flat)\n",
        "        scaled_qus_data[qus_type] = scaled_flat.reshape(qus_data_lists[qus_type].shape)\n",
        "        print(f\"Scaled {qus_type}: shape {scaled_qus_data[qus_type].shape}, range [{np.min(scaled_qus_data[qus_type]):.4f}, {np.max(scaled_qus_data[qus_type]):.4f}]\")\n",
        "    \n",
        "    # Create dataset based on number of QUS types\n",
        "    if len(qus_types) == 1:\n",
        "        # Single input\n",
        "        qus_type = qus_types[0]\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((scaled_qus_data[qus_type], labels))\n",
        "    else:\n",
        "        # Multiple inputs - create tuple of inputs\n",
        "        input_tuples = list(zip(*[scaled_qus_data[qus_type] for qus_type in qus_types]))\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((input_tuples, labels))\n",
        "    \n",
        "    dataset = dataset.batch(batch_size)\n",
        "    \n",
        "    if augment:\n",
        "        # Define augmentation pipeline\n",
        "        data_augmentation = tf.keras.Sequential([\n",
        "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "            tf.keras.layers.RandomRotation(0.25),\n",
        "            tf.keras.layers.RandomZoom(0.1),\n",
        "        ])\n",
        "        \n",
        "        if len(qus_types) == 1:\n",
        "            # Single input augmentation\n",
        "            def augment_single(inputs, label):\n",
        "                augmented = data_augmentation(inputs, training=True)\n",
        "                return augmented, label\n",
        "            dataset = dataset.map(augment_single, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        else:\n",
        "            # Multiple input augmentation - apply same transformation to all inputs\n",
        "            def augment_multiple(inputs, label):\n",
        "                # Concatenate all inputs along channel axis for synchronized augmentation\n",
        "                combined = tf.concat(inputs, axis=-1)  # H x W x (num_qus_types)\n",
        "                combined = data_augmentation(combined, training=True)\n",
        "                \n",
        "                # Split back into separate inputs\n",
        "                split_inputs = []\n",
        "                for i in range(len(qus_types)):\n",
        "                    split_inputs.append(combined[..., i:i+1])  # Extract single channel\n",
        "                \n",
        "                return tuple(split_inputs), label\n",
        "            \n",
        "            dataset = dataset.map(augment_multiple, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    return dataset.prefetch(tf.data.AUTOTUNE), scalers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_resnet_model_flexible_qus(qus_types, with_transfer_learning=True):\n",
        "    \"\"\"\n",
        "    Build ResNet model for flexible QUS input types.\n",
        "    \n",
        "    Args:\n",
        "        qus_types: List of QUS types (e.g., ['ESD', 'EAC', 'SI', 'SS', 'MBF'])\n",
        "        with_transfer_learning: Whether to use transfer learning\n",
        "    \n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    \n",
        "    if len(qus_types) == 1:\n",
        "        # Single input model\n",
        "        qus_type = qus_types[0]\n",
        "        input_layer = layers.Input(shape=(224, 224, 1), name=f'{qus_type}_input')\n",
        "        inputs = input_layer\n",
        "        \n",
        "        # Convert single channel to 3 channels for ResNet (repeat channel)\n",
        "        x = layers.Concatenate()([input_layer, input_layer, input_layer])\n",
        "        \n",
        "        # Load base model\n",
        "        if with_transfer_learning:\n",
        "            base_model = models.load_model(MODEL_WEIGHTS_PATH, compile=False)\n",
        "        else:\n",
        "            base_model = tf.keras.applications.ResNet50(\n",
        "                weights=None, \n",
        "                include_top=False, \n",
        "                input_shape=(224, 224, 3)\n",
        "            )\n",
        "        \n",
        "        base_model.trainable = True\n",
        "        x = base_model(x)\n",
        "        \n",
        "    else:\n",
        "        # Multiple input model\n",
        "        input_layers = []\n",
        "        for qus_type in qus_types:\n",
        "            input_layer = layers.Input(shape=(224, 224, 1), name=f'{qus_type}_input')\n",
        "            input_layers.append(input_layer)\n",
        "        \n",
        "        # Convert each single channel to 3 channels for ResNet\n",
        "        expanded_inputs = []\n",
        "        for input_layer in input_layers:\n",
        "            expanded = layers.Concatenate()([input_layer, input_layer, input_layer])\n",
        "            expanded_inputs.append(expanded)\n",
        "        \n",
        "        # Load base model\n",
        "        if with_transfer_learning:\n",
        "            base_model = models.load_model(MODEL_WEIGHTS_PATH, compile=False)\n",
        "        else:\n",
        "            base_model = tf.keras.applications.ResNet50(\n",
        "                weights=None, \n",
        "                include_top=False, \n",
        "                input_shape=(224, 224, 3)\n",
        "            )\n",
        "        \n",
        "        base_model.trainable = True\n",
        "        \n",
        "        # Process each input through the base model\n",
        "        feature_maps = []\n",
        "        for expanded_input in expanded_inputs:\n",
        "            features = base_model(expanded_input)\n",
        "            pooled = layers.GlobalAveragePooling2D()(features)\n",
        "            feature_maps.append(pooled)\n",
        "        \n",
        "        # Concatenate features from all inputs\n",
        "        x = layers.Concatenate()(feature_maps)\n",
        "    \n",
        "    # Global average pooling for single input\n",
        "    if len(qus_types) == 1:\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "    \n",
        "    # Dense layers for classification\n",
        "    x = layers.Dense(4096, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(2048, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    \n",
        "    # Final binary classification\n",
        "    output = layers.Dense(1, activation='sigmoid')(x)\n",
        "    \n",
        "    # Create model\n",
        "    if len(qus_types) == 1:\n",
        "        model = models.Model(inputs=inputs, outputs=output)\n",
        "    else:\n",
        "        model = models.Model(inputs=input_layers, outputs=output)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def build_resnet_model_flexible_concatenated_qus(qus_types, with_transfer_learning=True):\n",
        "    \"\"\"\n",
        "    Alternative architecture: concatenate QUS inputs first, then process through single model.\n",
        "    \n",
        "    Args:\n",
        "        qus_types: List of QUS types (e.g., ['ESD', 'EAC', 'SI', 'SS', 'MBF'])\n",
        "        with_transfer_learning: Whether to use transfer learning\n",
        "    \n",
        "    Returns:\n",
        "        Compiled Keras model\n",
        "    \"\"\"\n",
        "    \n",
        "    if len(qus_types) == 1:\n",
        "        # Single input - same as regular model\n",
        "        return build_resnet_model_flexible_qus(qus_types, with_transfer_learning)\n",
        "    \n",
        "    # Multiple inputs - concatenate along channel axis\n",
        "    input_layers = []\n",
        "    for qus_type in qus_types:\n",
        "        input_layer = layers.Input(shape=(224, 224, 1), name=f'{qus_type}_input')\n",
        "        input_layers.append(input_layer)\n",
        "    \n",
        "    # Concatenate inputs along channel axis (224, 224, num_qus_types)\n",
        "    concatenated_input = layers.Concatenate(axis=-1)(input_layers)\n",
        "    \n",
        "    # Convert to 3 channels for ResNet (repeat channels if needed)\n",
        "    num_channels = len(qus_types)\n",
        "    if num_channels == 1:\n",
        "        x = layers.Concatenate()([concatenated_input, concatenated_input, concatenated_input])\n",
        "    elif num_channels == 2:\n",
        "        x = layers.Concatenate()([concatenated_input, concatenated_input[..., 0:1]])\n",
        "    elif num_channels >= 3:\n",
        "        x = concatenated_input[..., :3]  # Take first 3 channels\n",
        "    \n",
        "    # Load base model\n",
        "    if with_transfer_learning:\n",
        "        base_model = models.load_model(MODEL_WEIGHTS_PATH, compile=False)\n",
        "    else:\n",
        "        base_model = tf.keras.applications.ResNet50(\n",
        "            weights=None, \n",
        "            include_top=False, \n",
        "            input_shape=(224, 224, 3)\n",
        "        )\n",
        "    \n",
        "    base_model.trainable = True\n",
        "    \n",
        "    # Process concatenated input\n",
        "    x = base_model(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(4096, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(2048, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    \n",
        "    # Final binary classification\n",
        "    output = layers.Dense(1, activation='sigmoid')(x)\n",
        "    \n",
        "    model = models.Model(inputs=input_layers, outputs=output)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plotAndReturnValidationTesting_flexible_qus(val_patients, model, qus_types, qus_matrices, scalers):\n",
        "    \"\"\"Updated validation testing function for flexible QUS input models\"\"\"\n",
        "    patient_to_true_labels = {}\n",
        "    patient_to_predicted_probs_list = {}\n",
        "\n",
        "    all_true_labels_individual = []\n",
        "    all_predicted_probs_individual = []\n",
        "\n",
        "    # ================= Collect predictions =================\n",
        "    for patient in val_patients:\n",
        "        patient_dataset, _ = create_dataset_from_patients_qus([patient], qus_types, qus_matrices, scalers=scalers, augment=False, batch_size=BATCH_SIZE)\n",
        "        for inputs, labels in patient_dataset:\n",
        "            true_label = labels[0].numpy()  # Single label per patient\n",
        "            predictions = model.predict(inputs, verbose=0).flatten()\n",
        "\n",
        "            patient_to_true_labels[patient.patient_id] = true_label\n",
        "            patient_to_predicted_probs_list[patient.patient_id] = predictions.tolist()\n",
        "\n",
        "            # Collect image-level predictions for metrics\n",
        "            all_true_labels_individual.extend(labels.numpy().flatten())\n",
        "            all_predicted_probs_individual.extend(predictions)\n",
        "\n",
        "    # ================= Image-level ROC =================\n",
        "    all_predicted_labels_individual = [1 if prob >= 0.5 else 0 for prob in all_predicted_probs_individual]\n",
        "    fpr_img, tpr_img, _ = roc_curve(all_true_labels_individual, all_predicted_probs_individual)\n",
        "    auc_img = roc_auc_score(all_true_labels_individual, all_predicted_probs_individual)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr_img, tpr_img, label=f\"Image-level ROC (AUC = {auc_img:.4f})\", color='blue')\n",
        "    plt.plot([0, 1], [0, 1], 'r--', label=\"Random Guess\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(f\"Image-level ROC Curve - {', '.join(qus_types)} QUS\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # ================= Patient-level ROC =================\n",
        "    patient_true = []\n",
        "    patient_probs = []\n",
        "\n",
        "    for pid in patient_to_predicted_probs_list:\n",
        "        patient_true.append(patient_to_true_labels[pid])\n",
        "        patient_probs.append(np.mean(patient_to_predicted_probs_list[pid]))  # average across cases\n",
        "\n",
        "    fpr_pat, tpr_pat, _ = roc_curve(patient_true, patient_probs)\n",
        "    auc_pat = roc_auc_score(patient_true, patient_probs)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr_pat, tpr_pat, label=f\"Patient-level ROC (AUC = {auc_pat:.4f})\", color='green')\n",
        "    plt.plot([0, 1], [0, 1], 'r--', label=\"Random Guess\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(f\"Patient-level ROC Curve - {', '.join(qus_types)} QUS\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return patient_to_true_labels, patient_to_predicted_probs_list\n",
        "\n",
        "\n",
        "# Updated hyperparameter tuning configurations for flexible QUS input\n",
        "def resnet_flexible_branched_qus(qus_types):\n",
        "    \"\"\"Flexible QUS input with separate branches\"\"\"\n",
        "    model = build_resnet_model_flexible_qus(qus_types, with_transfer_learning=True)\n",
        "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "    return model\n",
        "\n",
        "def resnet_flexible_concatenated_qus(qus_types):\n",
        "    \"\"\"Flexible QUS input with concatenated channels\"\"\"\n",
        "    model = build_resnet_model_flexible_concatenated_qus(qus_types, with_transfer_learning=True)\n",
        "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "    return model\n",
        "\n",
        "def resnet_flexible_no_transfer_qus(qus_types):\n",
        "    \"\"\"Flexible QUS input without transfer learning\"\"\"\n",
        "    model = build_resnet_model_flexible_qus(qus_types, with_transfer_learning=False)\n",
        "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "    return model\n",
        "\n",
        "# Create hyperparameter configurations based on selected QUS types\n",
        "def create_hyperparameter_configs_qus(qus_types):\n",
        "    \"\"\"Create hyperparameter configurations for the selected QUS types\"\"\"\n",
        "    return {\n",
        "        f'resnet_flexible_branched_{'_'.join(qus_types)}': lambda: resnet_flexible_branched_qus(qus_types),\n",
        "        f'resnet_flexible_concatenated_{'_'.join(qus_types)}': lambda: resnet_flexible_concatenated_qus(qus_types),\n",
        "        f'resnet_flexible_no_transfer_{'_'.join(qus_types)}': lambda: resnet_flexible_no_transfer_qus(qus_types)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "N_RUNS = 5\n",
        "val_aucs, test_aucs = [], []\n",
        "\n",
        "for run_idx in range(N_RUNS):\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"▶️ HOLD-OUT RUN {run_idx+1}/{N_RUNS}\")\n",
        "    print(f\"{'='*40}\\n\")\n",
        "\n",
        "    # Split into train/val/test with new random state each run\n",
        "    SEED_RUN = SEED + run_idx  # different seed each run\n",
        "    train_and_val_patients, test_patients = train_test_split(\n",
        "        all_patients, test_size=0.1, random_state=SEED_RUN\n",
        "    )\n",
        "    train_patients, val_patients = train_test_split(\n",
        "        train_and_val_patients, test_size=0.2, random_state=SEED_RUN\n",
        "    )\n",
        "\n",
        "    print(f\"Training patients: {len(train_patients)}\")\n",
        "    print(f\"Validation patients: {len(val_patients)}\")\n",
        "    print(f\"Test patients: {len(test_patients)}\")\n",
        "\n",
        "    # Create datasets with min-max scaling\n",
        "    # Fit scalers on training data only\n",
        "    print(\"\\nCreating training dataset and fitting scalers...\")\n",
        "    train_dataset, train_scalers = create_dataset_from_patients_qus(\n",
        "        train_patients, INPUT_TYPES, qus_matrices, scalers=None, augment=True, batch_size=BATCH_SIZE\n",
        "    )\n",
        "    \n",
        "    # Use the same scalers for validation and test\n",
        "    print(\"\\nCreating validation dataset (using training scalers)...\")\n",
        "    val_dataset, _ = create_dataset_from_patients_qus(\n",
        "        val_patients, INPUT_TYPES, qus_matrices, scalers=train_scalers, augment=False, batch_size=BATCH_SIZE\n",
        "    )\n",
        "    \n",
        "    print(\"\\nCreating test dataset (using training scalers)...\")\n",
        "    test_dataset, _ = create_dataset_from_patients_qus(\n",
        "        test_patients, INPUT_TYPES, qus_matrices, scalers=train_scalers, augment=False, batch_size=BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    # Compute class weights\n",
        "    weights = class_weight.compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(np.array([p.egfr for p in train_patients]).astype(int)),\n",
        "        y=np.array([p.egfr for p in train_patients]).astype(int)\n",
        "    )\n",
        "    class_weights_dict = dict(enumerate(weights))\n",
        "    print(f\"Class weights: {class_weights_dict}\")\n",
        "\n",
        "    # Get model configuration\n",
        "    hyperparameter_configs = create_hyperparameter_configs_qus(INPUT_TYPES)\n",
        "    MODEL_CONFIG_NAME = f'resnet_flexible_branched_{'_'.join(INPUT_TYPES)}'\n",
        "    print(f\"Using model configuration: {MODEL_CONFIG_NAME}\")\n",
        "\n",
        "    # Build and compile model\n",
        "    model = hyperparameter_configs[MODEL_CONFIG_NAME]()\n",
        "\n",
        "    # Early stopping and LR scheduler\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_auc', mode='max', patience=20, restore_best_weights=True\n",
        "    )\n",
        "    def step_decay(epoch, lr):\n",
        "        drop_rate = 0.5\n",
        "        drop_every = 15\n",
        "        if epoch > 0 and epoch % drop_every == 0:\n",
        "            return lr * drop_rate\n",
        "        return lr\n",
        "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(step_decay, verbose=0)\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[early_stopping, lr_scheduler],\n",
        "        class_weight=class_weights_dict,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # === Evaluate Validation ===\n",
        "    print(\"\\nEvaluating on validation set...\")\n",
        "    val_true, val_pred = [], []\n",
        "    patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting_flexible_qus(val_patients, model, INPUT_TYPES, qus_matrices, train_scalers)\n",
        "    for patient_id in patient_to_predicted_probs:\n",
        "        for prob in patient_to_predicted_probs[patient_id]:\n",
        "            val_true.append(patient_to_true_labels[patient_id])\n",
        "            val_pred.append(prob)\n",
        "    val_auc = roc_auc_score(val_true, val_pred)\n",
        "    val_aucs.append(val_auc)\n",
        "    print(f\"Validation AUC (Run {run_idx+1}): {val_auc:.4f}\")\n",
        "\n",
        "    # === Evaluate Test ===\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    test_true, test_pred = [], []\n",
        "    patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting_flexible_qus(test_patients, model, INPUT_TYPES, qus_matrices, train_scalers)\n",
        "    for patient_id in patient_to_predicted_probs:\n",
        "        for prob in patient_to_predicted_probs[patient_id]:\n",
        "            test_true.append(patient_to_true_labels[patient_id])\n",
        "            test_pred.append(prob)\n",
        "    test_auc = roc_auc_score(test_true, test_pred)\n",
        "    test_aucs.append(test_auc)\n",
        "    print(f\"Test AUC (Run {run_idx+1}): {test_auc:.4f}\")\n",
        "\n",
        "    print(classification_report(test_true, [1 if p >= 0.5 else 0 for p in test_pred], digits=4))\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# ✅ Final Summary of 5 Runs\n",
        "# ===========================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL SUMMARY OVER MULTIPLE HOLD-OUT RUNS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Validation AUCs: {['%.4f' % a for a in val_aucs]}\")\n",
        "print(f\"Test AUCs:       {['%.4f' % a for a in test_aucs]}\")\n",
        "print(f\"\\nAverage Validation AUC: {np.mean(val_aucs):.4f} ± {np.std(val_aucs):.4f}\")\n",
        "print(f\"Average Test AUC:       {np.mean(test_aucs):.4f} ± {np.std(test_aucs):.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
