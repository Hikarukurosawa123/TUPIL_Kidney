{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "executionInfo": {
     "elapsed": 25638,
     "status": "error",
     "timestamp": 1756486779113,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "E-mCkvSCGVH9",
    "outputId": "28de828b-0a42-4102-d563-461917826d2c"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/Hikarukurosawa123/TUPIL_Kidney.git\n",
    "#!git clone https://github.com/Hikarukurosawa123/TUPIL_Kidney.git drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney\n",
    "#%cd drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney\n",
    "#!git stash\n",
    "\n",
    "#!git add .\n",
    "#!git commit -m \"Added latest Colab notebook\"\n",
    "#!git push origin main\n",
    "\n",
    "#%cd /content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney\n",
    "#!git stash\n",
    "\n",
    "!git pull origin main     # or \"master\", depending on the branch name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    "from skimage.transform import resize\n",
    "from collections import defaultdict\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from numpy import expand_dims\n",
    "import scipy.ndimage\n",
    "from sklearn.utils import class_weight\n",
    "# Flexible input configuration - specify which input types to use\n",
    "# Options: 'B_mode', 'MBF', 'SI' - can use any combination\n",
    "INPUT_TYPES = ['B_mode', 'MBF', 'SI']  # Change this to specify which inputs to use\n",
    "# Examples:\n",
    "# INPUT_TYPES = ['B_mode']  # Only B-mode\n",
    "# INPUT_TYPES = ['B_mode', 'MBF']  # B-mode + MBF\n",
    "# INPUT_TYPES = ['B_mode', 'SI']  # B-mode + SI\n",
    "# INPUT_TYPES = ['MBF', 'SI']  # MBF + SI\n",
    "# INPUT_TYPES = ['B_mode', 'MBF', 'SI']  # All three\n",
    "\n",
    "\n",
    "# Environment detection\n",
    "def is_google_colab():\n",
    "    \"\"\"Check if running in Google Colab environment\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_google_drive_mounted():\n",
    "    \"\"\"Check if Google Drive is mounted in Colab\"\"\"\n",
    "    return os.path.exists('/content/drive/MyDrive')\n",
    "# Set paths based on environment\n",
    "if is_google_colab() and is_google_drive_mounted():\n",
    "    # Google Colab with Google Drive mounted\n",
    "    IMAGE_FOLDERS = {\n",
    "        'B_mode': '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/Bmode',\n",
    "        'MBF': '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/MBF',\n",
    "        'SI': '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/SI'\n",
    "    }\n",
    "    CSV_FILE = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/csv/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
    "    MODEL_WEIGHTS_PATH = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/model_weights/RadImageNet-ResNet50_notop.h5'\n",
    "    print(\"Running on Google Colab with Google Drive mounted\")\n",
    "elif is_google_colab():\n",
    "    # Google Colab without Google Drive mounted\n",
    "    IMAGE_FOLDERS = {\n",
    "        'B_mode': '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/Bmode',\n",
    "        'MBF': '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/MBF',\n",
    "        'SI': '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/SI'\n",
    "    }\n",
    "    CSV_FILE = '/content/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
    "    MODEL_WEIGHTS_PATH = '/content/model_weights/RadImageNet-ResNet50_notop.h5'\n",
    "    print(\"Running on Google Colab without Google Drive mounted\")\n",
    "else:\n",
    "    # Local environment\n",
    "    IMAGE_FOLDERS = {\n",
    "        'B_mode': '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/Bmode_resize',\n",
    "        'MBF': '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/MBF',\n",
    "        'SI': '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/SI'\n",
    "    }\n",
    "    CSV_FILE = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/csv/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
    "    MODEL_WEIGHTS_PATH = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/model_weights/RadImageNet-ResNet50_notop.h5'\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# Print selected input types and their paths\n",
    "print(f\"Selected input types: {INPUT_TYPES}\")\n",
    "for input_type in INPUT_TYPES:\n",
    "    print(f\"{input_type}_FOLDER: {IMAGE_FOLDERS[input_type]}\")\n",
    "print(f\"CSV_FILE: {CSV_FILE}\")\n",
    "print(f\"MODEL_WEIGHTS_PATH: {MODEL_WEIGHTS_PATH}\")\n",
    "\n",
    "BATCH_SIZE = 16 # batch = 8 -> 10% batch = 16 --> doesnt work\n",
    "EPOCHS = 160 # epoch = 40 --> 10% epoch = 50 --> 9%\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4331,
     "status": "ok",
     "timestamp": 1757878289738,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "KbXHUIK5oT2y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "IMAGE_FOLDER: /Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/lanczos_shape_corrected_only_nc_resized_images\n",
      "CSV_FILE: /Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/csv/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv\n",
      "MODEL_WEIGHTS_PATH: /Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/model_weights/RadImageNet-ResNet50_notop.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    "from skimage.transform import resize\n",
    "from collections import defaultdict\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from numpy import expand_dims\n",
    "import scipy.ndimage\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Environment detection\n",
    "def is_google_colab():\n",
    "    \"\"\"Check if running in Google Colab environment\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def is_google_drive_mounted():\n",
    "    \"\"\"Check if Google Drive is mounted in Colab\"\"\"\n",
    "    return os.path.exists('/content/drive/MyDrive')\n",
    "\n",
    "# Set paths based on environment\n",
    "if is_google_colab() and is_google_drive_mounted():\n",
    "    # Google Colab with Google Drive mounted\n",
    "    B_MODE_IMAGE_FOLDER = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/Bmode'\n",
    "    H_SCAN_IMAGE_FOLDER = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/Hscan' # currently resized image input (224 x 224)\n",
    "\n",
    "    CSV_FILE = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/csv/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
    "    MODEL_WEIGHTS_PATH = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/model_weights/RadImageNet-ResNet50_notop.h5'\n",
    "    print(\"Running on Google Colab with Google Drive mounted\")\n",
    "elif is_google_colab():\n",
    "    # Google Colab without Google Drive mounted\n",
    "    B_MODE_IMAGE_FOLDER = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/Bmode'\n",
    "    H_SCAN_IMAGE_FOLDER = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/Hscan' # currently resized image input (224 x 224)\n",
    "\n",
    "    CSV_FILE = '/content/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
    "    MODEL_WEIGHTS_PATH = '/content/model_weights/RadImageNet-ResNet50_notop.h5'\n",
    "    print(\"Running on Google Colab without Google Drive mounted\")\n",
    "else:\n",
    "    # Local environment\n",
    "    B_MODE_IMAGE_FOLDER = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/Bmode'\n",
    "    H_SCAN_IMAGE_FOLDER = 'Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/Hscan' # currently resized image input (224 x 224)\n",
    "\n",
    "    CSV_FILE = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/csv/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
    "    MODEL_WEIGHTS_PATH = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/model_weights/RadImageNet-ResNet50_notop.h5'\n",
    "    print(\"Running locally\")\n",
    "\n",
    "print(f\"IMAGE_FOLDER: {B_MODE_IMAGE_FOLDER}\")\n",
    "print(f\"CSV_FILE: {CSV_FILE}\")\n",
    "print(f\"MODEL_WEIGHTS_PATH: {MODEL_WEIGHTS_PATH}\")\n",
    "\n",
    "BATCH_SIZE = 16 # batch = 8 -> 10% batch = 16 --> doesnt work\n",
    "EPOCHS = 160 # epoch = 40 --> 10% epoch = 50 --> 9%\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Detection Test ===\n",
      "is_google_colab(): False\n",
      "is_google_drive_mounted(): False\n",
      "Current working directory: /Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney\n",
      "IMAGE_FOLDER exists: True\n",
      "CSV_FILE exists: True\n",
      "MODEL_WEIGHTS_PATH exists: True\n",
      "=== End Test ===\n"
     ]
    }
   ],
   "source": [
    "# Test environment detection and path setup\n",
    "print(\"=== Environment Detection Test ===\") \n",
    "print(f\"is_google_colab(): {is_google_colab()}\")\n",
    "print(f\"is_google_drive_mounted(): {is_google_drive_mounted()}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"IMAGE_FOLDER exists: {os.path.exists(B_MODE_IMAGE_FOLDER)}\")\n",
    "print(f\"IMAGE_FOLDER exists: {os.path.exists(H_SCAN_IMAGE_FOLDER)}\")\n",
    "print(f\"CSV_FILE exists: {os.path.exists(CSV_FILE)}\")\n",
    "print(f\"MODEL_WEIGHTS_PATH exists: {os.path.exists(MODEL_WEIGHTS_PATH)}\")\n",
    "print(\"=== End Test ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1757878290965,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "vHGZMxH_U5p0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flexible Patient class for variable number of input types\n",
    "class Patient:\n",
    "    def __init__(self, patient_id, egfr, egfr_val, image_paths_dict):\n",
    "        self.patient_id = patient_id\n",
    "        self.egfr = egfr #label\n",
    "        self.egfr_val = egfr_val #actual value\n",
    "        self.image_paths_dict = image_paths_dict  # Dictionary with input_type -> list of paths\n",
    "    \n",
    "    def get_paths(self, input_type):\n",
    "        \"\"\"Get paths for a specific input type\"\"\"\n",
    "        return self.image_paths_dict.get(input_type, [])\n",
    "    \n",
    "    def has_all_inputs(self, required_input_types):\n",
    "        \"\"\"Check if patient has all required input types\"\"\"\n",
    "        return all(len(self.get_paths(input_type)) > 0 for input_type in required_input_types)\n",
    "\n",
    "\n",
    "def label_img_classification_by_patient_flexible(input_types, image_folders, csv_file):\n",
    "    \"\"\"\n",
    "    Returns a list of Patient objects with flexible input types.\n",
    "    \n",
    "    Args:\n",
    "        input_types: List of input types to use (e.g., ['B_mode', 'MBF', 'SI'])\n",
    "        image_folders: Dictionary mapping input_type -> folder_path\n",
    "        csv_file: Path to CSV file with patient data\n",
    "    \"\"\"\n",
    "    eGFR_data = pd.read_csv(csv_file)\n",
    "    eGFR_data.rename(columns={'Patient ID': 'patient_id', 'eGFR (abs/closest)': 'eGFR'}, inplace=True)\n",
    "    eGFR_data['patient_id'] = eGFR_data['patient_id'].astype(int)\n",
    "    eGFR_data.set_index('patient_id', inplace=True)\n",
    "\n",
    "    # Initialize patient image map with all input types\n",
    "    patient_image_map = defaultdict(lambda: {input_type: [] for input_type in input_types})\n",
    "\n",
    "    # Helper function to extract patient ID from filename\n",
    "    def extract_patient_id(filename, input_type):\n",
    "        try:\n",
    "            if input_type == \"B_mode\":\n",
    "                # Format: Patient_100_Resized_Image_1.png\n",
    "                return int(filename.split('_')[1])\n",
    "            else:\n",
    "                # Format: P100_PTONR_01_Image_1_rf_MBF_resized.png or P100_PTONR_01_Image_1_rf_SI_resized.png\n",
    "                return int(filename.split('_')[0][1:])  # Remove 'P' prefix\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    # Process files for each input type\n",
    "    for input_type in input_types:\n",
    "        if input_type in image_folders:\n",
    "            folder_path = image_folders[input_type]\n",
    "            if os.path.exists(folder_path):\n",
    "                files = sorted(os.listdir(folder_path))\n",
    "                for filename in files:\n",
    "                    patient_id = extract_patient_id(filename, input_type)\n",
    "                    if patient_id and patient_id in eGFR_data.index:\n",
    "                        patient_image_map[patient_id][input_type].append(os.path.join(folder_path, filename))\n",
    "            else:\n",
    "                print(f\"Warning: Folder {folder_path} does not exist for input type {input_type}\")\n",
    "\n",
    "    # Build Patient objects - only include patients with all required input types\n",
    "    patient_objects = []\n",
    "    for patient_id, paths_dict in patient_image_map.items():\n",
    "        # Check if patient has all required input types\n",
    "        has_all_types = all(len(paths_dict[input_type]) > 0 for input_type in input_types)\n",
    "        \n",
    "        if has_all_types:\n",
    "            egfr = eGFR_data.loc[patient_id, 'eGFR']\n",
    "            egfrLabel = 1 if egfr >= 60 else 0\n",
    "            patient_objects.append(Patient(\n",
    "                patient_id,\n",
    "                egfrLabel,\n",
    "                egfr,\n",
    "                paths_dict\n",
    "            ))\n",
    "\n",
    "    return patient_objects\n",
    "\n",
    "\n",
    "# Prepare image preprocessing function\n",
    "def preprocess_img(img_path):\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_png(image, channels=3)  # RGB image\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # Convert TensorFlow tensor to NumPy array for compatibility with ImageDataGenerator\n",
    "    return tf.keras.utils.img_to_array(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_dataset_from_patients_flexible(patients, input_types, augment=False, batch_size=4):\n",
    "    \"\"\"\n",
    "    Creates a tf.data.Dataset for flexible input images.\n",
    "    \n",
    "    Args:\n",
    "        patients: List of Patient objects\n",
    "        input_types: List of input types to use (e.g., ['B_mode', 'MBF', 'SI'])\n",
    "        augment: Whether to apply data augmentation\n",
    "        batch_size: Batch size for the dataset\n",
    "    \n",
    "    Returns:\n",
    "        tf.data.Dataset with flexible inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load images for all input types\n",
    "    image_lists = {input_type: [] for input_type in input_types}\n",
    "    labels = []\n",
    "    \n",
    "    for patient in patients:\n",
    "        # Get the minimum number of images across all input types for this patient\n",
    "        min_images = min(len(patient.get_paths(input_type)) for input_type in input_types)\n",
    "        \n",
    "        for i in range(min_images):\n",
    "            # Load images for each input type\n",
    "            for input_type in input_types:\n",
    "                image_lists[input_type].append(preprocess_img(patient.get_paths(input_type)[i]))\n",
    "            labels.append(patient.egfr)\n",
    "    \n",
    "    # Create dataset based on number of input types\n",
    "    if len(input_types) == 1:\n",
    "        # Single input\n",
    "        input_type = input_types[0]\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_lists[input_type], labels))\n",
    "    else:\n",
    "        # Multiple inputs - create tuple of inputs\n",
    "        input_tuples = list(zip(*[image_lists[input_type] for input_type in input_types]))\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((input_tuples, labels))\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    if augment:\n",
    "        # Define augmentation pipeline\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "            tf.keras.layers.RandomRotation(0.25),\n",
    "            tf.keras.layers.RandomZoom(0.1),\n",
    "        ])\n",
    "        \n",
    "        if len(input_types) == 1:\n",
    "            # Single input augmentation\n",
    "            def augment_single(inputs, label):\n",
    "                augmented = data_augmentation(inputs, training=True)\n",
    "                return augmented, label\n",
    "            dataset = dataset.map(augment_single, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        else:\n",
    "            # Multiple input augmentation - apply same transformation to all inputs\n",
    "            def augment_multiple(inputs, label):\n",
    "                # Concatenate all inputs along channel axis for synchronized augmentation\n",
    "                combined = tf.concat(inputs, axis=-1)  # H x W x (3 * num_inputs)\n",
    "                combined = data_augmentation(combined, training=True)\n",
    "                \n",
    "                # Split back into separate inputs\n",
    "                split_inputs = []\n",
    "                for i in range(len(input_types)):\n",
    "                    start_channel = i * 3\n",
    "                    end_channel = (i + 1) * 3\n",
    "                    split_inputs.append(combined[..., start_channel:end_channel])\n",
    "                \n",
    "                return tuple(split_inputs), label\n",
    "            \n",
    "            dataset = dataset.map(augment_multiple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "def summarize_patients_flexible(patients, input_types):\n",
    "    \"\"\"Summarize the flexible input patient data\"\"\"\n",
    "    num_patients = len(patients)\n",
    "    \n",
    "    print(f\"Number of patients: {num_patients}\")\n",
    "    print(f\"Input types: {input_types}\")\n",
    "    \n",
    "    # Count images for each input type\n",
    "    for input_type in input_types:\n",
    "        total_images = sum(len(patient.get_paths(input_type)) for patient in patients)\n",
    "        print(f\"Total {input_type} images: {total_images}\")\n",
    "        \n",
    "        # Distribution of images per patient\n",
    "        counts = [len(patient.get_paths(input_type)) for patient in patients]\n",
    "        print(f\"{input_type} - Min: {min(counts)}, Max: {max(counts)}, Avg: {np.mean(counts):.2f}\")\n",
    "    \n",
    "    return num_patients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_resnet_model_flexible(input_types, with_transfer_learning=True):\n",
    "    \"\"\"\n",
    "    Build ResNet model for flexible input types.\n",
    "    \n",
    "    Args:\n",
    "        input_types: List of input types (e.g., ['B_mode', 'MBF', 'SI'])\n",
    "        with_transfer_learning: Whether to use transfer learning\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(input_types) == 1:\n",
    "        # Single input model\n",
    "        input_type = input_types[0]\n",
    "        input_layer = layers.Input(shape=(224, 224, 3), name=f'{input_type}_input')\n",
    "        inputs = input_layer\n",
    "        \n",
    "        # Load base model\n",
    "        if with_transfer_learning:\n",
    "            base_model = models.load_model(MODEL_WEIGHTS_PATH, compile=False)\n",
    "        else:\n",
    "            base_model = tf.keras.applications.ResNet50(\n",
    "                weights=None, \n",
    "                include_top=False, \n",
    "                input_shape=(224, 224, 3)\n",
    "            )\n",
    "        \n",
    "        base_model.trainable = True\n",
    "        x = base_model(inputs)\n",
    "        \n",
    "    else:\n",
    "        # Multiple input model\n",
    "        input_layers = []\n",
    "        for input_type in input_types:\n",
    "            input_layer = layers.Input(shape=(224, 224, 3), name=f'{input_type}_input')\n",
    "            input_layers.append(input_layer)\n",
    "        \n",
    "        # Load base model\n",
    "        if with_transfer_learning:\n",
    "            base_model = models.load_model(MODEL_WEIGHTS_PATH, compile=False)\n",
    "        else:\n",
    "            base_model = tf.keras.applications.ResNet50(\n",
    "                weights=None, \n",
    "                include_top=False, \n",
    "                input_shape=(224, 224, 3)\n",
    "            )\n",
    "        \n",
    "        base_model.trainable = True\n",
    "        \n",
    "        # Process each input through the base model\n",
    "        feature_maps = []\n",
    "        for input_layer in input_layers:\n",
    "            features = base_model(input_layer)\n",
    "            pooled = layers.GlobalAveragePooling2D()(features)\n",
    "            feature_maps.append(pooled)\n",
    "        \n",
    "        # Concatenate features from all inputs\n",
    "        x = layers.Concatenate()(feature_maps)\n",
    "    \n",
    "    # Global average pooling for single input\n",
    "    if len(input_types) == 1:\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Dense layers for classification\n",
    "    x = layers.Dense(4096, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(2048, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Final binary classification\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # Create model\n",
    "    if len(input_types) == 1:\n",
    "        model = models.Model(inputs=inputs, outputs=output)\n",
    "    else:\n",
    "        model = models.Model(inputs=input_layers, outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_resnet_model_flexible_concatenated(input_types, with_transfer_learning=True):\n",
    "    \"\"\"\n",
    "    Alternative architecture: concatenate inputs first, then process through single model.\n",
    "    \n",
    "    Args:\n",
    "        input_types: List of input types (e.g., ['B_mode', 'MBF', 'SI'])\n",
    "        with_transfer_learning: Whether to use transfer learning\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(input_types) == 1:\n",
    "        # Single input - same as regular model\n",
    "        return build_resnet_model_flexible(input_types, with_transfer_learning)\n",
    "    \n",
    "    # Multiple inputs - concatenate along channel axis\n",
    "    input_layers = []\n",
    "    for input_type in input_types:\n",
    "        input_layer = layers.Input(shape=(224, 224, 3), name=f'{input_type}_input')\n",
    "        input_layers.append(input_layer)\n",
    "    \n",
    "    # Concatenate inputs along channel axis (224, 224, 3*num_inputs)\n",
    "    concatenated_input = layers.Concatenate(axis=-1)(input_layers)\n",
    "    \n",
    "    # Load base model\n",
    "    if with_transfer_learning:\n",
    "        base_model = models.load_model(MODEL_WEIGHTS_PATH, compile=False)\n",
    "    else:\n",
    "        base_model = tf.keras.applications.ResNet50(\n",
    "            weights=None, \n",
    "            include_top=False, \n",
    "            input_shape=(224, 224, 3 * len(input_types))\n",
    "        )\n",
    "    \n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Process concatenated input\n",
    "    x = base_model(concatenated_input)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(4096, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(2048, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Final binary classification\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = models.Model(inputs=input_layers, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RAN56 Model ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "# ----------------------\n",
    "# Residual Unit\n",
    "# ----------------------\n",
    "def residual_unit(x, filters, name=None):\n",
    "    shortcut = x\n",
    "    x = layers.Conv2D(filters, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Match shortcut channels if needed\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, (1, 1), padding='same')(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "# ----------------------\n",
    "# Attention Module\n",
    "# ----------------------\n",
    "def attention_module(inputs, filters, name=None):\n",
    "    # ===== Trunk Branch =====\n",
    "    trunk = residual_unit(inputs, filters)\n",
    "    trunk = residual_unit(trunk, filters)\n",
    "\n",
    "    # ===== Mask Branch =====\n",
    "    # Downsampling path\n",
    "    mp1 = layers.MaxPooling2D(pool_size=(2, 2), strides=2)(inputs)\n",
    "    mp1 = residual_unit(mp1, filters)\n",
    "\n",
    "    mp2 = layers.MaxPooling2D(pool_size=(2, 2), strides=2)(mp1)\n",
    "    mp2 = residual_unit(mp2, filters)\n",
    "\n",
    "    # Upsampling path\n",
    "    up1 = layers.UpSampling2D(size=(2, 2))(mp2)\n",
    "    up1 = residual_unit(up1, filters)\n",
    "    up1 = layers.Add()([up1, mp1])  # skip connection\n",
    "\n",
    "    up2 = layers.UpSampling2D(size=(2, 2))(up1)\n",
    "    up2 = residual_unit(up2, filters)\n",
    "\n",
    "    # Mask output\n",
    "    mask = layers.Conv2D(filters, (1, 1), padding='same', activation='sigmoid')(up2)\n",
    "\n",
    "    # ===== Combine Mask + Trunk =====\n",
    "    out = layers.Multiply()([mask, trunk])\n",
    "    out = layers.Add()([out, trunk])  # (1 + mask) * trunk\n",
    "    out = residual_unit(out, filters)\n",
    "\n",
    "    return out\n",
    "def RAN56(input_shape=(224, 224, 3)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Stage 1\n",
    "    x = layers.Conv2D(64, (7, 7), strides=2, padding='same', activation='relu')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\n",
    "\n",
    "    # Stage 2\n",
    "    x = layers.Conv2D(64, (1, 1), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    att1 = attention_module(x, 256)\n",
    "    skip1 = att1\n",
    "\n",
    "    # Stage 3\n",
    "    x = layers.Conv2D(128, (1, 1), strides=2, padding='same', activation='relu')(att1)\n",
    "    x = layers.Conv2D(512, (1, 1), padding='same', activation='relu')(x)\n",
    "    att2 = attention_module(x, 512)\n",
    "    skip2 = att2\n",
    "\n",
    "    # Stage 4\n",
    "    x = layers.Conv2D(256, (1, 1), strides=2, padding='same', activation='relu')(att2)\n",
    "    x = layers.Conv2D(1024, (3, 3), padding='same', activation='relu')(x)\n",
    "    att3 = attention_module(x, 1024)\n",
    "    skip3 = att3\n",
    "\n",
    "    # Stage 5\n",
    "    x = layers.Conv2D(512, (1, 1), strides=2, padding='same', activation='relu')(att3)\n",
    "    x = layers.Conv2D(2048, (3, 3), padding='same', activation='relu')(x)\n",
    "\n",
    "    # Combine skip connections\n",
    "    x = layers.Add()([x, skip1, skip2, skip3])\n",
    "\n",
    "    # Global average pooling and classifier\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "     # Flatten feature and classifier head\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(100, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    #outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = models.Model(inputs, output, name='RAN56')\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = RAN56(input_shape=(224, 224, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAndReturnValidationTesting_flexible(val_patients, model, input_types):\n",
    "    \"\"\"Updated validation testing function for flexible input models\"\"\"\n",
    "    patient_to_true_labels = {}\n",
    "    patient_to_predicted_probs_list = {}\n",
    "\n",
    "    all_true_labels_individual = []\n",
    "    all_predicted_probs_individual = []\n",
    "\n",
    "    # ================= Collect predictions =================\n",
    "    for patient in val_patients:\n",
    "        patient_dataset = create_dataset_from_patients_flexible([patient], input_types, augment=False, batch_size=BATCH_SIZE)\n",
    "        for inputs, labels in patient_dataset:\n",
    "            true_label = labels[0].numpy()  # Single label per patient\n",
    "            predictions = model.predict(inputs).flatten()\n",
    "\n",
    "            patient_to_true_labels[patient.patient_id] = true_label\n",
    "            patient_to_predicted_probs_list[patient.patient_id] = predictions.tolist()\n",
    "\n",
    "            # Collect image-level predictions for metrics\n",
    "            all_true_labels_individual.extend(labels.numpy().flatten())\n",
    "            all_predicted_probs_individual.extend(predictions)\n",
    "\n",
    "    # ================= Image-level ROC =================\n",
    "    all_predicted_labels_individual = [1 if prob >= 0.5 else 0 for prob in all_predicted_probs_individual]\n",
    "    fpr_img, tpr_img, _ = roc_curve(all_true_labels_individual, all_predicted_probs_individual)\n",
    "    auc_img = roc_auc_score(all_true_labels_individual, all_predicted_probs_individual)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_img, tpr_img, label=f\"Image-level ROC (AUC = {auc_img:.4f})\", color='blue')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label=\"Random Guess\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"Image-level ROC Curve - {', '.join(input_types)} Input\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # ================= Patient-level ROC =================\n",
    "    patient_true = []\n",
    "    patient_probs = []\n",
    "\n",
    "    for pid in patient_to_predicted_probs_list:\n",
    "        patient_true.append(patient_to_true_labels[pid])\n",
    "        patient_probs.append(np.mean(patient_to_predicted_probs_list[pid]))  # average across images\n",
    "\n",
    "    fpr_pat, tpr_pat, _ = roc_curve(patient_true, patient_probs)\n",
    "    auc_pat = roc_auc_score(patient_true, patient_probs)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_pat, tpr_pat, label=f\"Patient-level ROC (AUC = {auc_pat:.4f})\", color='green')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label=\"Random Guess\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"Patient-level ROC Curve - {', '.join(input_types)} Input\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return patient_to_true_labels, patient_to_predicted_probs_list\n",
    "\n",
    "\n",
    "# Updated hyperparameter tuning configurations for flexible input\n",
    "def resnet_flexible_branched(input_types):\n",
    "    \"\"\"Flexible input with separate branches\"\"\"\n",
    "    model = build_resnet_model_flexible(input_types, with_transfer_learning=True)\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "def resnet_flexible_concatenated(input_types):\n",
    "    \"\"\"Flexible input with concatenated channels\"\"\"\n",
    "    model = build_resnet_model_flexible_concatenated(input_types, with_transfer_learning=True)\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "def resnet_flexible_no_transfer(input_types):\n",
    "    \"\"\"Flexible input without transfer learning\"\"\"\n",
    "    model = build_resnet_model_flexible(input_types, with_transfer_learning=False)\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "    return model\n",
    "\n",
    "# Create hyperparameter configurations based on selected input types\n",
    "def create_hyperparameter_configs(input_types):\n",
    "    \"\"\"Create hyperparameter configurations for the selected input types\"\"\"\n",
    "    return {\n",
    "        f'resnet_flexible_branched_{\"_\".join(input_types)}': lambda: resnet_flexible_branched(input_types),\n",
    "        f'resnet_flexible_concatenated_{\"_\".join(input_types)}': lambda: resnet_flexible_concatenated(input_types),\n",
    "        f'resnet_flexible_no_transfer_{\"_\".join(input_types)}': lambda: resnet_flexible_no_transfer(input_types)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistogramOfDataset(training_labels, validation_labels, n_bins):\n",
    "  plt.hist(training_labels, bins=n_bins, alpha=0.5, label='Training Data', color='blue')\n",
    "  plt.hist(validation_labels, bins=n_bins, alpha=0.5, label='Validation Data', color='red')\n",
    "  plt.xlabel('eGFR')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.title(f'Histogram with {n_bins} bins')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def plotTrainingHistory(history):\n",
    "  # Plot training and validation loss\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.plot(history.history['loss'], label='Training Loss')\n",
    "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Binary Crossentropy Loss')\n",
    "  plt.title('Training and Validation Loss')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  # Plot Accuracy\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.title('Training and Validation Accuracy')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "def plotAndReturnValidationTesting(val_patients, model):\n",
    "    patient_to_true_labels = {}\n",
    "    patient_to_predicted_probs_list = {}\n",
    "\n",
    "    all_true_labels_individual = []\n",
    "    all_predicted_probs_individual = []\n",
    "\n",
    "    # ================= Collect predictions =================\n",
    "    for patient in val_patients:\n",
    "        patient_dataset = create_dataset_from_patients_flexible([patient], augment=False, batch_size=BATCH_SIZE)\n",
    "        for images, labels in patient_dataset:\n",
    "            true_label = labels[0].numpy()  # Single label per patient\n",
    "            predictions = model.predict(images).flatten()\n",
    "\n",
    "            patient_to_true_labels[patient.patient_id] = true_label\n",
    "            patient_to_predicted_probs_list[patient.patient_id] = predictions.tolist()\n",
    "\n",
    "            # Collect image-level predictions for metrics\n",
    "            all_true_labels_individual.extend(labels.numpy().flatten())\n",
    "            all_predicted_probs_individual.extend(predictions)\n",
    "\n",
    "    # ================= Image-level ROC =================\n",
    "    all_predicted_labels_individual = [1 if prob >= 0.5 else 0 for prob in all_predicted_probs_individual]\n",
    "    fpr_img, tpr_img, _ = roc_curve(all_true_labels_individual, all_predicted_probs_individual)\n",
    "    auc_img = roc_auc_score(all_true_labels_individual, all_predicted_probs_individual)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_img, tpr_img, label=f\"Image-level ROC (AUC = {auc_img:.4f})\", color='blue')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label=\"Random Guess\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Image-level ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # ================= Patient-level ROC =================\n",
    "    patient_true = []\n",
    "    patient_probs = []\n",
    "\n",
    "    for pid in patient_to_predicted_probs_list:\n",
    "        patient_true.append(patient_to_true_labels[pid])\n",
    "        patient_probs.append(np.mean(patient_to_predicted_probs_list[pid]))  # average across images\n",
    "\n",
    "    fpr_pat, tpr_pat, _ = roc_curve(patient_true, patient_probs)\n",
    "    auc_pat = roc_auc_score(patient_true, patient_probs)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_pat, tpr_pat, label=f\"Patient-level ROC (AUC = {auc_pat:.4f})\", color='green')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label=\"Random Guess\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Patient-level ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return patient_to_true_labels, patient_to_predicted_probs_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading patients with flexible input...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'label_img_classification_by_patient_flexible' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load patients with flexible input\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading patients with flexible input...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m all_patients = \u001b[43mlabel_img_classification_by_patient_flexible\u001b[49m(\n\u001b[32m     11\u001b[39m     INPUT_TYPES, \n\u001b[32m     12\u001b[39m     IMAGE_FOLDERS, \n\u001b[32m     13\u001b[39m     CSV_FILE\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal patients with all selected input types: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_patients)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Summarize the data\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'label_img_classification_by_patient_flexible' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "N_RUNS = 5\n",
    "val_aucs, test_aucs = [], []\n",
    "\n",
    "for run_idx in range(N_RUNS):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"▶️ HOLD-OUT RUN {run_idx+1}/{N_RUNS}\")\n",
    "    print(f\"{'='*40}\\n\")\n",
    "\n",
    "    # Load patients (if not already loaded)\n",
    "    print(\"Loading patients with flexible input...\")\n",
    "    all_patients = label_img_classification_by_patient_flexible(\n",
    "        INPUT_TYPES, \n",
    "        IMAGE_FOLDERS, \n",
    "        CSV_FILE\n",
    "    )\n",
    "    print(f\"Total patients with all selected input types: {len(all_patients)}\")\n",
    "\n",
    "    # Summarize once (optional to skip inside loop if identical)\n",
    "    summarize_patients_flexible(all_patients, INPUT_TYPES)\n",
    "\n",
    "    # Split into train/val/test with new random state each run\n",
    "    SEED_RUN = SEED + run_idx  # different seed each run\n",
    "    train_and_val_patients, test_patients = train_test_split(\n",
    "        all_patients, test_size=0.1, random_state=SEED_RUN\n",
    "    )\n",
    "    train_patients, val_patients = train_test_split(\n",
    "        train_and_val_patients, test_size=0.2, random_state=SEED_RUN\n",
    "    )\n",
    "\n",
    "    print(f\"Training patients: {len(train_patients)}\")\n",
    "    print(f\"Validation patients: {len(val_patients)}\")\n",
    "    print(f\"Test patients: {len(test_patients)}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = create_dataset_from_patients_flexible(\n",
    "        train_patients, INPUT_TYPES, augment=True, batch_size=BATCH_SIZE\n",
    "    )\n",
    "    val_dataset = create_dataset_from_patients_flexible(\n",
    "        val_patients, INPUT_TYPES, augment=False, batch_size=BATCH_SIZE\n",
    "    )\n",
    "    test_dataset = create_dataset_from_patients_flexible(\n",
    "        test_patients, INPUT_TYPES, augment=False, batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # Compute class weights\n",
    "    weights = class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(np.array([p.egfr for p in train_patients]).astype(int)),\n",
    "        y=np.array([p.egfr for p in train_patients]).astype(int)\n",
    "    )\n",
    "    class_weights_dict = dict(enumerate(weights))\n",
    "    print(f\"Class weights: {class_weights_dict}\")\n",
    "\n",
    "    # Get model configuration\n",
    "    hyperparameter_configs = create_hyperparameter_configs(INPUT_TYPES)\n",
    "    MODEL_CONFIG_NAME = f'resnet_flexible_branched_{\"_\".join(INPUT_TYPES)}'\n",
    "    print(f\"Using model configuration: {MODEL_CONFIG_NAME}\")\n",
    "\n",
    "    # Build and compile model\n",
    "    model = hyperparameter_configs[MODEL_CONFIG_NAME]()\n",
    "\n",
    "    # Early stopping and LR scheduler\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc', mode='max', patience=20, restore_best_weights=True\n",
    "    )\n",
    "    def step_decay(epoch, lr):\n",
    "        drop_rate = 0.5\n",
    "        drop_every = 15\n",
    "        if epoch > 0 and epoch % drop_every == 0:\n",
    "            return lr * drop_rate\n",
    "        return lr\n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(step_decay, verbose=0)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[early_stopping, lr_scheduler],\n",
    "        class_weight=class_weights_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # === Evaluate Validation ===\n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    val_true, val_pred = [], []\n",
    "    patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting_flexible(val_patients, model, INPUT_TYPES)\n",
    "    for patient_id in patient_to_predicted_probs:\n",
    "        for prob in patient_to_predicted_probs[patient_id]:\n",
    "            val_true.append(patient_to_true_labels[patient_id])\n",
    "            val_pred.append(prob)\n",
    "    val_auc = roc_auc_score(val_true, val_pred)\n",
    "    val_aucs.append(val_auc)\n",
    "    print(f\"Validation AUC (Run {run_idx+1}): {val_auc:.4f}\")\n",
    "\n",
    "    # === Evaluate Test ===\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_true, test_pred = [], []\n",
    "    patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting_flexible(test_patients, model, INPUT_TYPES)\n",
    "    for patient_id in patient_to_predicted_probs:\n",
    "        for prob in patient_to_predicted_probs[patient_id]:\n",
    "            test_true.append(patient_to_true_labels[patient_id])\n",
    "            test_pred.append(prob)\n",
    "    test_auc = roc_auc_score(test_true, test_pred)\n",
    "    test_aucs.append(test_auc)\n",
    "    print(f\"Test AUC (Run {run_idx+1}): {test_auc:.4f}\")\n",
    "\n",
    "    print(classification_report(test_true, [1 if p >= 0.5 else 0 for p in test_pred], digits=4))\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# ✅ Final Summary of 5 Runs\n",
    "# ===========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY OVER MULTIPLE HOLD-OUT RUNS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Validation AUCs: {['%.4f' % a for a in val_aucs]}\")\n",
    "print(f\"Test AUCs:       {['%.4f' % a for a in test_aucs]}\")\n",
    "print(f\"\\nAverage Validation AUC: {np.mean(val_aucs):.4f} ± {np.std(val_aucs):.4f}\")\n",
    "print(f\"Average Test AUC:       {np.mean(test_aucs):.4f} ± {np.std(test_aucs):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to easily switch between different model configurations\n",
    "# This cell demonstrates how to use the hyperparameter configurations\n",
    "\n",
    "print(\"=== Model Configuration Examples ===\")\n",
    "print()\n",
    "\n",
    "# Get all available configurations for current INPUT_TYPES\n",
    "hyperparameter_configs = create_hyperparameter_configs(INPUT_TYPES)\n",
    "print(f\"Available configurations for {INPUT_TYPES}:\")\n",
    "for config_name in hyperparameter_configs.keys():\n",
    "    print(f\"  - {config_name}\")\n",
    "\n",
    "print()\n",
    "print(\"To use a different model architecture, simply change the MODEL_CONFIG_NAME variable:\")\n",
    "print()\n",
    "\n",
    "# Example 1: Branched architecture (default)\n",
    "print(\"# Example 1: Branched architecture\")\n",
    "print(f\"MODEL_CONFIG_NAME = 'resnet_flexible_branched_{'_'.join(INPUT_TYPES)}'\")\n",
    "print(\"model = hyperparameter_configs[MODEL_CONFIG_NAME]()\")\n",
    "print()\n",
    "\n",
    "# Example 2: Concatenated architecture\n",
    "print(\"# Example 2: Concatenated architecture\")\n",
    "print(f\"MODEL_CONFIG_NAME = 'resnet_flexible_concatenated_{'_'.join(INPUT_TYPES)}'\")\n",
    "print(\"model = hyperparameter_configs[MODEL_CONFIG_NAME]()\")\n",
    "print()\n",
    "\n",
    "# Example 3: No transfer learning\n",
    "print(\"# Example 3: No transfer learning\")\n",
    "print(f\"MODEL_CONFIG_NAME = 'resnet_flexible_no_transfer_{'_'.join(INPUT_TYPES)}'\")\n",
    "print(\"model = hyperparameter_configs[MODEL_CONFIG_NAME]()\")\n",
    "print()\n",
    "\n",
    "print(\"Benefits of using hyperparameter configurations:\")\n",
    "print(\"1. Easy switching between architectures\")\n",
    "print(\"2. Consistent model creation\")\n",
    "print(\"3. Automatic naming based on input types\")\n",
    "print(\"4. Easy to add new configurations\")\n",
    "print()\n",
    "\n",
    "# You can also create custom configurations\n",
    "print(\"Custom configuration example:\")\n",
    "print(\"def custom_model_config(input_types):\")\n",
    "print(\"    # Your custom model building logic here\")\n",
    "print(\"    model = build_resnet_model_flexible(input_types, with_transfer_learning=True)\")\n",
    "print(\"    # Custom compilation\")\n",
    "print(\"    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), ...)\")\n",
    "print(\"    return model\")\n",
    "print()\n",
    "print(\"# Add to configurations:\")\n",
    "print(\"hyperparameter_configs['custom_model'] = lambda: custom_model_config(INPUT_TYPES)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Hyperparameter tuning across different model configurations\n",
    "# This cell shows how to test multiple model architectures\n",
    "\n",
    "def train_and_evaluate_model(config_name, hyperparameter_configs, train_dataset, val_dataset, val_patients, INPUT_TYPES):\n",
    "    \"\"\"Train and evaluate a single model configuration\"\"\"\n",
    "    print(f\"\\n=== Training {config_name} ===\")\n",
    "    \n",
    "    # Build model\n",
    "    model = hyperparameter_configs[config_name]()\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc', mode='max', patience=10, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=50,  # Reduced for faster testing\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0  # Silent training\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting_flexible(val_patients, model, INPUT_TYPES)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    true_labels, predicted_probs = [], []\n",
    "    for patient_id in patient_to_predicted_probs:\n",
    "        for prob in patient_to_predicted_probs[patient_id]:\n",
    "            true_labels.append(patient_to_true_labels[patient_id])\n",
    "            predicted_probs.append(prob)\n",
    "    \n",
    "    val_auc = roc_auc_score(true_labels, predicted_probs)\n",
    "    print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    return val_auc, model\n",
    "\n",
    "# Example usage (commented out to avoid long execution)\n",
    "print(\"=== Hyperparameter Tuning Example ===\")\n",
    "print()\n",
    "print(\"To test multiple model configurations:\")\n",
    "print()\n",
    "print(\"# Get all configurations\")\n",
    "print(\"hyperparameter_configs = create_hyperparameter_configs(INPUT_TYPES)\")\n",
    "print()\n",
    "print(\"# Test each configuration\")\n",
    "print(\"results = {}\")\n",
    "print(\"for config_name in hyperparameter_configs.keys():\")\n",
    "print(\"    auc, model = train_and_evaluate_model(config_name, hyperparameter_configs, train_dataset, val_dataset, val_patients, INPUT_TYPES)\")\n",
    "print(\"    results[config_name] = auc\")\n",
    "print()\n",
    "print(\"# Find best configuration\")\n",
    "print(\"best_config = max(results, key=results.get)\")\n",
    "print(f\"print(f'Best configuration: {{best_config}} with AUC: {{results[best_config]:.4f}}')\")\n",
    "print()\n",
    "print(\"This approach allows you to:\")\n",
    "print(\"1. Test multiple architectures automatically\")\n",
    "print(\"2. Compare performance across configurations\")\n",
    "print(\"3. Select the best performing model\")\n",
    "print(\"4. Easily add new configurations to test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the flexible input system with hyperparameter configurations\n",
    "# To change input types, simply modify the INPUT_TYPES list at the top of the notebook\n",
    "\n",
    "print(\"=== Flexible Input System Usage Examples ===\")\n",
    "print()\n",
    "print(\"To use different combinations of inputs, change the INPUT_TYPES variable:\")\n",
    "print()\n",
    "print(\"# Only B-mode images:\")\n",
    "print(\"INPUT_TYPES = ['B_mode']\")\n",
    "print()\n",
    "print(\"# B-mode + MBF images:\")\n",
    "print(\"INPUT_TYPES = ['B_mode', 'MBF']\")\n",
    "print()\n",
    "print(\"# B-mode + SI images:\")\n",
    "print(\"INPUT_TYPES = ['B_mode', 'SI']\")\n",
    "print()\n",
    "print(\"# MBF + SI images:\")\n",
    "print(\"INPUT_TYPES = ['MBF', 'SI']\")\n",
    "print()\n",
    "print(\"# All three input types:\")\n",
    "print(\"INPUT_TYPES = ['B_mode', 'MBF', 'SI']\")\n",
    "print()\n",
    "print(\"The system will automatically:\")\n",
    "print(\"- Load only the specified input types\")\n",
    "print(\"- Create appropriate model architectures\")\n",
    "print(\"- Handle data augmentation for multiple inputs\")\n",
    "print(\"- Generate appropriate evaluation plots\")\n",
    "print()\n",
    "print(\"Current configuration:\")\n",
    "print(f\"Selected input types: {INPUT_TYPES}\")\n",
    "print(f\"Number of input types: {len(INPUT_TYPES)}\")\n",
    "print()\n",
    "print(\"Available model architectures (via hyperparameter configurations):\")\n",
    "print(\"1. Branched architecture: Each input processed separately, then features concatenated\")\n",
    "print(\"2. Concatenated architecture: Inputs concatenated first, then processed together\")\n",
    "print(\"3. No transfer learning: Train from scratch without pre-trained weights\")\n",
    "print()\n",
    "print(\"How to use hyperparameter configurations:\")\n",
    "print(\"1. Get configurations: hyperparameter_configs = create_hyperparameter_configs(INPUT_TYPES)\")\n",
    "print(\"2. Choose model: MODEL_CONFIG_NAME = 'resnet_flexible_branched_B_mode_MBF_SI'\")\n",
    "print(\"3. Build model: model = hyperparameter_configs[MODEL_CONFIG_NAME]()\")\n",
    "print(\"4. Train and evaluate as usual\")\n",
    "print()\n",
    "print(\"Benefits of hyperparameter configurations:\")\n",
    "print(\"- Easy switching between architectures\")\n",
    "print(\"- Consistent model creation\")\n",
    "print(\"- Automatic naming based on input types\")\n",
    "print(\"- Easy to add new configurations\")\n",
    "print(\"- Perfect for hyperparameter tuning\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM9pDb4gqcW+6GHF8ydM7uE",
   "collapsed_sections": [
    "7vLhJCCFcjb1",
    "c2MVI9TGcnTc",
    "zd1NaH0Tm5Kb"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "1f2Z66JDnwNNAjyugDY8yz4izwuL-tiqc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
