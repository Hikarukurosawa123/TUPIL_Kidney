{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "executionInfo": {
     "elapsed": 25638,
     "status": "error",
     "timestamp": 1756486779113,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "E-mCkvSCGVH9",
    "outputId": "28de828b-0a42-4102-d563-461917826d2c"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/Hikarukurosawa123/TUPIL_Kidney.git\n",
    "#!git clone https://github.com/Hikarukurosawa123/TUPIL_Kidney.git drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney\n",
    "#%cd drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney\n",
    "#!git stash\n",
    "\n",
    "#!git add .\n",
    "#!git commit -m \"Added latest Colab notebook\"\n",
    "#!git push origin main\n",
    "\n",
    "#%cd /content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney\n",
    "#!git stash\n",
    "\n",
    "!git pull origin main     # or \"master\", depending on the branch name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4331,
     "status": "ok",
     "timestamp": 1757878289738,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "KbXHUIK5oT2y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "IMAGE_FOLDER: /Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/lanczos_shape_corrected_only_nc_resized_images\n",
      "CSV_FILE: /Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/csv/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv\n",
      "MODEL_WEIGHTS_PATH: /Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/model_weights/RadImageNet-ResNet50_notop.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    "from skimage.transform import resize\n",
    "from collections import defaultdict\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from numpy import expand_dims\n",
    "import scipy.ndimage\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Environment detection\n",
    "def is_google_colab():\n",
    "    \"\"\"Check if running in Google Colab environment\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def is_google_drive_mounted():\n",
    "    \"\"\"Check if Google Drive is mounted in Colab\"\"\"\n",
    "    return os.path.exists('/content/drive/MyDrive')\n",
    "\n",
    "# Set paths based on environment\n",
    "if is_google_colab() and is_google_drive_mounted():\n",
    "    # Google Colab with Google Drive mounted\n",
    "    B_MODE_IMAGE_FOLDER = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/Bmode'\n",
    "    H_SCAN_IMAGE_FOLDER = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/Hscan' # currently resized image input (224 x 224)\n",
    "\n",
    "    CSV_FILE = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/csv/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
    "    MODEL_WEIGHTS_PATH = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/model_weights/RadImageNet-ResNet50_notop.h5'\n",
    "    print(\"Running on Google Colab with Google Drive mounted\")\n",
    "elif is_google_colab():\n",
    "    # Google Colab without Google Drive mounted\n",
    "    B_MODE_IMAGE_FOLDER = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/Bmode'\n",
    "    H_SCAN_IMAGE_FOLDER = '/content/drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney/data/Hscan' # currently resized image input (224 x 224)\n",
    "\n",
    "    CSV_FILE = '/content/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
    "    MODEL_WEIGHTS_PATH = '/content/model_weights/RadImageNet-ResNet50_notop.h5'\n",
    "    print(\"Running on Google Colab without Google Drive mounted\")\n",
    "else:\n",
    "    # Local environment\n",
    "    B_MODE_IMAGE_FOLDER = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/Bmode'\n",
    "    H_SCAN_IMAGE_FOLDER = 'Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/Hscan' # currently resized image input (224 x 224)\n",
    "\n",
    "    CSV_FILE = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/csv/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
    "    MODEL_WEIGHTS_PATH = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/model_weights/RadImageNet-ResNet50_notop.h5'\n",
    "    print(\"Running locally\")\n",
    "\n",
    "print(f\"IMAGE_FOLDER: {B_MODE_IMAGE_FOLDER}\")\n",
    "print(f\"CSV_FILE: {CSV_FILE}\")\n",
    "print(f\"MODEL_WEIGHTS_PATH: {MODEL_WEIGHTS_PATH}\")\n",
    "\n",
    "BATCH_SIZE = 16 # batch = 8 -> 10% batch = 16 --> doesnt work\n",
    "EPOCHS = 160 # epoch = 40 --> 10% epoch = 50 --> 9%\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Detection Test ===\n",
      "is_google_colab(): False\n",
      "is_google_drive_mounted(): False\n",
      "Current working directory: /Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney\n",
      "IMAGE_FOLDER exists: True\n",
      "CSV_FILE exists: True\n",
      "MODEL_WEIGHTS_PATH exists: True\n",
      "=== End Test ===\n"
     ]
    }
   ],
   "source": [
    "# Test environment detection and path setup\n",
    "print(\"=== Environment Detection Test ===\") \n",
    "print(f\"is_google_colab(): {is_google_colab()}\")\n",
    "print(f\"is_google_drive_mounted(): {is_google_drive_mounted()}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"IMAGE_FOLDER exists: {os.path.exists(IMAGE_FOLDER)}\")\n",
    "print(f\"CSV_FILE exists: {os.path.exists(CSV_FILE)}\")\n",
    "print(f\"MODEL_WEIGHTS_PATH exists: {os.path.exists(MODEL_WEIGHTS_PATH)}\")\n",
    "print(\"=== End Test ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1757878290965,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "vHGZMxH_U5p0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROFXH-uNi8OT"
   },
   "source": [
    "## 1. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1757878292553,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "SNQ-3gY75oY2"
   },
   "outputs": [],
   "source": [
    "# Patient class used to hold all information related for training\n",
    "class Patient:\n",
    "    def __init__(self, patient_id, egfr, egfr_val, image_paths):\n",
    "        self.patient_id = patient_id\n",
    "        self.egfr = egfr #label \n",
    "        self.egfr_val = egfr_val #actual value \n",
    "        self.image_paths = image_paths\n",
    "\n",
    "# Creates patients objects based on images and eGFR csv raw dataset\n",
    "def label_img_classification_by_patient(b_mode_folder, h_scan_folder, csv_file):\n",
    "    \"\"\"\n",
    "    Returns a list of Patient objects with both B-mode and H-scan image paths\n",
    "    and a binary label based on eGFR.\n",
    "    \"\"\"\n",
    "    eGFR_data = pd.read_csv(csv_file)\n",
    "    eGFR_data.rename(columns={'Patient ID': 'patient_id', 'eGFR (abs/closest)': 'eGFR'}, inplace=True)\n",
    "    eGFR_data['patient_id'] = eGFR_data['patient_id'].astype(int)\n",
    "    eGFR_data.set_index('patient_id', inplace=True)\n",
    "\n",
    "    # Dictionary to store lists of images per patient\n",
    "    patient_image_map = defaultdict(lambda: {\"b_mode\": [], \"h_scan\": []})\n",
    "\n",
    "    # Collect all files in both folders\n",
    "    b_mode_files = set(os.listdir(b_mode_folder))\n",
    "    h_scan_files = set(os.listdir(h_scan_folder))\n",
    "\n",
    "    # Match only filenames that exist in both folders\n",
    "    common_files = sorted(b_mode_files.intersection(h_scan_files))\n",
    "    missing_in_hscan = b_mode_files - h_scan_files\n",
    "    missing_in_bmode = h_scan_files - b_mode_files\n",
    "\n",
    "    # Log missing files\n",
    "    for f in missing_in_hscan:\n",
    "        print(f\"Skipping {f}: exists in B-mode but not in H-scan folder\")\n",
    "    for f in missing_in_bmode:\n",
    "        print(f\"Skipping {f}: exists in H-scan but not in B-mode folder\")\n",
    "\n",
    "    # Map files to patients\n",
    "    for filename in common_files:\n",
    "        try:\n",
    "            patient_id = int(filename.split('_')[1])  # adjust parsing if needed\n",
    "            if patient_id in eGFR_data.index:\n",
    "                patient_image_map[patient_id][\"b_mode\"].append(os.path.join(b_mode_folder, filename))\n",
    "                patient_image_map[patient_id][\"h_scan\"].append(os.path.join(h_scan_folder, filename))\n",
    "            else:\n",
    "                print(f\"Patient ID {patient_id} not found in CSV, skipping...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "    # Build Patient objects\n",
    "    patient_objects = []\n",
    "    for patient_id, paths in patient_image_map.items():\n",
    "        egfr = eGFR_data.loc[patient_id, 'eGFR']\n",
    "        egfrLabel = 1 if egfr >= 60 else 0\n",
    "        patient_objects.append(Patient(\n",
    "            patient_id,\n",
    "            egfrLabel,\n",
    "            paths[\"b_mode\"],\n",
    "            paths[\"h_scan\"]\n",
    "        ))\n",
    "\n",
    "\n",
    "# Prepare image\n",
    "def preprocess_img(img_path):\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_png(image, channels=3)  # Gray-scale image\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # Convert TensorFlow tensor to NumPy array for compatibility with ImageDataGenerator\n",
    "    return tf.keras.utils.img_to_array(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1757878294681,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "ubivM9i1w_Mb"
   },
   "outputs": [],
   "source": [
    "def augment_images(original_images, labels, random_state):\n",
    "    \"\"\"Generate augmented images.\n",
    "       Returns array of images and labels containing the original and augmented images.\n",
    "    \"\"\"\n",
    "    augmented_images = original_images.copy()\n",
    "    augmented_labels = labels.copy()\n",
    "\n",
    "    deterministic_random_number_gen = np.random.default_rng(random_state)\n",
    "\n",
    "    for image, label in zip(original_images, labels):\n",
    "        # First add horizontally flipped image\n",
    "        augmented_images.append(np.fliplr(image))\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "        # Randomly rotate between -40 and 40 degrees (black as padding)\n",
    "        angle = deterministic_random_number_gen.integers(-40, 40)\n",
    "        rotated_image = scipy.ndimage.rotate(image, angle, reshape=False, mode='constant', cval=1)\n",
    "        augmented_images.append(rotated_image)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "        # Rotate again\n",
    "        angle = deterministic_random_number_gen.integers(-40, 40)\n",
    "        rotated_image = scipy.ndimage.rotate(image, angle, reshape=False, mode='constant', cval=1)\n",
    "        augmented_images.append(rotated_image)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "        # Crop image randomly\n",
    "        height, width = image.shape[:2]\n",
    "        random_array = deterministic_random_number_gen.random(size=4);\n",
    "        # Ensure crop is a square aspect ratio\n",
    "        w = int((width*0.90) * (1+random_array[0]*0.10))\n",
    "        h = w\n",
    "        x = int(random_array[2] * (width-w))\n",
    "        y = int(random_array[3] * (height-h))\n",
    "\n",
    "        image_crop = image[y:h+y, x:w+x, 0:3]\n",
    "        image_crop = resize(image_crop, image.shape, anti_aliasing=True)\n",
    "        augmented_images.append(image_crop)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "    return np.array(augmented_images), np.array(augmented_labels)\n",
    "\n",
    "def create_dataset_wip(file_paths, labels, augment=False, batch_size=4):\n",
    "    # Not using generator in this implementation\n",
    "    all_images = [preprocess_img(file_path) for file_path in file_paths]\n",
    "    all_labels = labels\n",
    "\n",
    "    if augment:\n",
    "        all_images, all_labels = augment_images_wip(all_images, all_labels, SEED)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((all_images, all_labels))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "def create_dataset_from_patients(patients, augment=False, batch_size=4):\n",
    "    # Load all images + labels into arrays\n",
    "    all_images = [preprocess_img(file_path) for patient in patients for file_path in patient.image_paths]\n",
    "    all_labels = [patient.egfr for patient in patients for image in patient.image_paths]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((all_images, all_labels))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    if augment:\n",
    "        # Apply augmentation on-the-fly\n",
    "        # data_augmentation = tf.keras.Sequential([\n",
    "        #     tf.keras.layers.RandomFlip(\"horizontal\"),  # horizontal flip\n",
    "        #     tf.keras.layers.RandomTranslation(\n",
    "        #         height_factor=0.3,  # vertical shift up to 30%\n",
    "        #         width_factor=0.3    # horizontal shift up to 30%\n",
    "        #     ),\n",
    "        # ])\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.25),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        ])\n",
    "\n",
    "        # data_augmentation = tf.keras.Sequential([\n",
    "        #     # Random reflections\n",
    "        #     tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "\n",
    "        #     # Random rotation (0–360 degrees → full circle)\n",
    "        #     tf.keras.layers.RandomRotation(1.0),   # 1.0 = full range [-180°, +180°]\n",
    "\n",
    "        #     # Random scaling (0.8–1.2)\n",
    "        #     tf.keras.layers.RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2))\n",
    "        # ])\n",
    "\n",
    "        dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6544,
     "status": "ok",
     "timestamp": 1757878302925,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "54xI33ASF4DZ",
    "outputId": "a2fcc49d-5124-4a9a-8718-e0c3f0c92754"
   },
   "outputs": [],
   "source": [
    "# 1. Get file paths and labels\n",
    "# file_paths, eGFR_labels, absolute_file_paths_indices = label_img(IMAGE_FOLDER, CSV_FILE)\n",
    "patients = label_img_classification_by_patient(IMAGE_FOLDER, CSV_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1757878846533,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "PJ4tg4PXtoLS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def conv_block(x, filters, stride=1):\n",
    "    \"\"\"A residual block with projection shortcut when stride > 1\"\"\"\n",
    "    shortcut = x\n",
    "\n",
    "    # First conv\n",
    "    x = layers.Conv2D(filters, kernel_size=3, strides=stride, padding=\"same\",\n",
    "                      use_bias=False, kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Second conv\n",
    "    x = layers.Conv2D(filters, kernel_size=3, strides=1, padding=\"same\",\n",
    "                      use_bias=False, kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Projection for shortcut if shape mismatch\n",
    "    if stride != 1 or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=1, strides=stride,\n",
    "                                 use_bias=False, kernel_initializer=\"he_normal\")(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_ResNet18(input_shape=(224, 224, 3)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Initial conv + maxpool\n",
    "    x = layers.Conv2D(64, 7, strides=2, padding=\"same\", use_bias=False,\n",
    "                      kernel_initializer=\"he_normal\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    # Residual blocks\n",
    "    x = conv_block(x, 64, stride=1)\n",
    "    x = conv_block(x, 64, stride=1)\n",
    "\n",
    "    x = conv_block(x, 128, stride=2)\n",
    "    x = conv_block(x, 128, stride=1)\n",
    "\n",
    "    x = conv_block(x, 256, stride=2)\n",
    "    x = conv_block(x, 256, stride=1)\n",
    "\n",
    "    x = conv_block(x, 512, stride=2)\n",
    "    x = conv_block(x, 512, stride=1)\n",
    "\n",
    "    # Global avg pool + classifier\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = layers.Dense(4096, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(2048, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs, name=\"ResNet18\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzD2bL_4YhKH"
   },
   "source": [
    "# Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 16042,
     "status": "ok",
     "timestamp": 1757875292835,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "qyvRbMXOYnts",
    "outputId": "45ce6193-3b1e-4d85-d9dc-530793ee9d51"
   },
   "outputs": [],
   "source": [
    "def build_resnet_model(with_transfer_learning):\n",
    "    if with_transfer_learning:\n",
    "      base_model = models.load_model(MODEL_WEIGHTS_PATH, compile=False)\n",
    "    else:\n",
    "      base_model = tf.keras.applications.ResNet50(weights=None, include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    base_model.trainable = True\n",
    "\n",
    "\n",
    "\n",
    "    model = models.Sequential([\n",
    "        # layers.Input(shape=(224, 224, 1)), # Define an Input layer with the desired shape\n",
    "        # layers.Conv2D(3, (3, 3), padding='same', activation='relu'), # Convert grayscale to 3 channels\n",
    "        base_model,\n",
    "        # layers.Flatten(input_shape=(7, 7, 512)),  # Equivalent to PyTorch's view(output.size(0), -1)\n",
    "        layers.GlobalAveragePooling2D(),  # Replace Flatten\n",
    "        layers.Dense(4096, activation='relu', ),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(2048, activation='relu', ),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1024, activation='relu',),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid'),\n",
    "\n",
    "        # kernel_regularizer=regularizers.l2(0.01)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile the model outside the function\n",
    "model = build_resnet_model(True)\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_resnet_two_input_model():\n",
    "    # Define two inputs\n",
    "    input1 = layers.Input(shape=(224, 224, 1), name='input1')\n",
    "    input2 = layers.Input(shape=(224, 224, 1), name='input2')\n",
    "    \n",
    "    # Concatenate along channel axis → (224, 224, 2)\n",
    "    x = layers.Concatenate(axis=-1)([input1, input2])\n",
    "    \n",
    "    # Load ResNet50 without the top layers\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        weights=None, \n",
    "        include_top=False, \n",
    "        input_shape=(224, 224, 2)\n",
    "    )\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    x = base_model(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(4096, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(2048, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1024, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Final binary classification\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = models.Model(inputs=[input1, input2], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_resnet_two_input_model()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd1NaH0Tm5Kb"
   },
   "source": [
    "# Training With K-Fold (Patient Exclusivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K_JlxDZj4iS"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1757878389980,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "vifyDqG2j61w"
   },
   "outputs": [],
   "source": [
    "def plotHistogramOfDataset(training_labels, validation_labels, n_bins):\n",
    "  plt.hist(training_labels, bins=n_bins, alpha=0.5, label='Training Data', color='blue')\n",
    "  plt.hist(validation_labels, bins=n_bins, alpha=0.5, label='Validation Data', color='red')\n",
    "  plt.xlabel('eGFR')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.title(f'Histogram with {n_bins} bins')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def plotTrainingHistory(history):\n",
    "  # Plot training and validation loss\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.plot(history.history['loss'], label='Training Loss')\n",
    "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Binary Crossentropy Loss')\n",
    "  plt.title('Training and Validation Loss')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  # Plot Accuracy\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.title('Training and Validation Accuracy')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "def plotAndReturnValidationTesting(val_patients, model):\n",
    "    patient_to_true_labels = {}\n",
    "    patient_to_predicted_probs_list = {}\n",
    "\n",
    "    all_true_labels_individual = []\n",
    "    all_predicted_probs_individual = []\n",
    "\n",
    "    # ================= Collect predictions =================\n",
    "    for patient in val_patients:\n",
    "        patient_dataset = create_dataset_from_patients([patient], augment=False, batch_size=BATCH_SIZE)\n",
    "        for images, labels in patient_dataset:\n",
    "            true_label = labels[0].numpy()  # Single label per patient\n",
    "            predictions = model.predict(images).flatten()\n",
    "\n",
    "            patient_to_true_labels[patient.patient_id] = true_label\n",
    "            patient_to_predicted_probs_list[patient.patient_id] = predictions.tolist()\n",
    "\n",
    "            # Collect image-level predictions for metrics\n",
    "            all_true_labels_individual.extend(labels.numpy().flatten())\n",
    "            all_predicted_probs_individual.extend(predictions)\n",
    "\n",
    "    # ================= Image-level ROC =================\n",
    "    all_predicted_labels_individual = [1 if prob >= 0.5 else 0 for prob in all_predicted_probs_individual]\n",
    "    fpr_img, tpr_img, _ = roc_curve(all_true_labels_individual, all_predicted_probs_individual)\n",
    "    auc_img = roc_auc_score(all_true_labels_individual, all_predicted_probs_individual)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_img, tpr_img, label=f\"Image-level ROC (AUC = {auc_img:.4f})\", color='blue')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label=\"Random Guess\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Image-level ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # ================= Patient-level ROC =================\n",
    "    patient_true = []\n",
    "    patient_probs = []\n",
    "\n",
    "    for pid in patient_to_predicted_probs_list:\n",
    "        patient_true.append(patient_to_true_labels[pid])\n",
    "        patient_probs.append(np.mean(patient_to_predicted_probs_list[pid]))  # average across images\n",
    "\n",
    "    fpr_pat, tpr_pat, _ = roc_curve(patient_true, patient_probs)\n",
    "    auc_pat = roc_auc_score(patient_true, patient_probs)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_pat, tpr_pat, label=f\"Patient-level ROC (AUC = {auc_pat:.4f})\", color='green')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label=\"Random Guess\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Patient-level ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return patient_to_true_labels, patient_to_predicted_probs_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFvqXLnSdMUO"
   },
   "source": [
    "# Hyper-parmeter tuning configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1757878854851,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "EFAX7f-_dJzD"
   },
   "outputs": [],
   "source": [
    "\n",
    "def resnet_no_tranfer_learning():\n",
    "  model = build_resnet_model(False)\n",
    "  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "  return model\n",
    "\n",
    "def resnet_with_tranfer_learning():\n",
    "  model = build_resnet_model(True)\n",
    "  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "  return model\n",
    "\n",
    "def resnet_with_two_inputs():\n",
    "\n",
    "  model = build_resnet_two_input_model()\n",
    "  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "  return model\n",
    "\n",
    "\n",
    "hyper_parameter_tuning_configs = {\n",
    "    \n",
    "    #'resnet_no_tranfer_learning': resnet_no_tranfer_learning,\n",
    "    #'resnet_with_tranfer_learning': resnet_with_tranfer_learning,\n",
    "    'resnet_with_two_inputs': resnet_with_two_inputs\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUxxFe5ZiaAP"
   },
   "source": [
    "# Training with K-Fold (New Method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0D8ZpcfNn5K"
   },
   "source": [
    "Latest k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 544124,
     "status": "error",
     "timestamp": 1757811017692,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "tK-s75w_Nl7g",
    "outputId": "ca254c41-b914-4092-cbd3-a42844ce3ade"
   },
   "outputs": [],
   "source": [
    "#stratified k-fold cross validation with early stopping and best weights storage, no change in test set across each hyper parameter (only the validatoin, training set changes)\n",
    "#used for hyper parameter tuning \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 1: Load patients and split into train/val vs test\n",
    "# ---------------------------------------------------\n",
    "all_patients = label_img_classification_by_patient(IMAGE_FOLDER, CSV_FILE)\n",
    "\n",
    "train_and_val_patients, test_patients = train_test_split(\n",
    "    all_patients, test_size=0.1, random_state=SEED\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 2: Hyperparameter tuning with stratified k-fold CV\n",
    "# ---------------------------------------------------\n",
    "for config_name, model_creator in hyper_parameter_tuning_configs.items():\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Training with Config: {config_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    all_patient_egfr = [patient.egfr for patient in train_and_val_patients]\n",
    "\n",
    "    val_fold_aucs = []\n",
    "    test_fold_aucs = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_and_val_patients, all_patient_egfr)):\n",
    "        print(f\"\\n----- Fold {fold + 1} -----\")\n",
    "\n",
    "        train_patients = [train_and_val_patients[i] for i in train_idx]\n",
    "        val_patients = [train_and_val_patients[i] for i in val_idx]\n",
    "\n",
    "        # Plot distribution\n",
    "        plotHistogramOfDataset([p.egfr for p in train_patients], [p.egfr for p in val_patients], 2)\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset_fold = create_dataset_from_patients(train_patients, augment=True, batch_size=BATCH_SIZE)\n",
    "        val_dataset = create_dataset_from_patients(val_patients, augment=False, batch_size=BATCH_SIZE)\n",
    "        test_dataset = create_dataset_from_patients(test_patients, augment=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Compute class weights\n",
    "        weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(np.array([p.egfr for p in train_patients]).astype(int)),\n",
    "            y=np.array([p.egfr for p in train_patients]).astype(int)\n",
    "        )\n",
    "        class_weights_dict = dict(enumerate(weights))\n",
    "\n",
    "        # Build model\n",
    "        model = model_creator()\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_auc', mode='max', patience=50, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            train_dataset_fold,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[early_stopping],\n",
    "            class_weight=class_weights_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        plotTrainingHistory(history)\n",
    "\n",
    "        # -------------------------\n",
    "        # Validation performance\n",
    "        # -------------------------\n",
    "        patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting(val_patients, model)\n",
    "\n",
    "        true_labels, predicted_probs = [], []\n",
    "        for patient_id in patient_to_predicted_probs:\n",
    "            for prob in patient_to_predicted_probs[patient_id]:\n",
    "                true_labels.append(patient_to_true_labels[patient_id])\n",
    "                predicted_probs.append(prob)\n",
    "\n",
    "        predicted_labels = [1 if p >= 0.5 else 0 for p in predicted_probs]\n",
    "        val_auc = roc_auc_score(true_labels, predicted_probs)\n",
    "        print(f\"Validation AUC (Fold {fold + 1}): {val_auc:.4f}\")\n",
    "        print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "        val_fold_aucs.append(val_auc)\n",
    "\n",
    "        # Save best model for fold\n",
    "        model.save_weights(f\"best_model_fold_{fold+1}_{config_name}.weights.h5\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Test performance (same fold model)\n",
    "        # -------------------------\n",
    "        patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting(test_patients, model)\n",
    "\n",
    "        true_labels, predicted_probs = [], []\n",
    "        for patient_id in patient_to_predicted_probs:\n",
    "            for prob in patient_to_predicted_probs[patient_id]:\n",
    "                true_labels.append(patient_to_true_labels[patient_id])\n",
    "                predicted_probs.append(prob)\n",
    "\n",
    "        predicted_labels = [1 if p >= 0.5 else 0 for p in predicted_probs]\n",
    "        test_auc = roc_auc_score(true_labels, predicted_probs)\n",
    "        print(f\"Test AUC (Fold {fold + 1}): {test_auc:.4f}\")\n",
    "        print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "        test_fold_aucs.append(test_auc)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Final summary for this config\n",
    "    # ---------------------------------------------------\n",
    "    print(f\"\\n==== Final Results for Config: {config_name} ====\")\n",
    "    print(f\"Mean Validation AUC: {np.mean(val_fold_aucs):.4f} ± {np.std(val_fold_aucs):.4f}\")\n",
    "    print(f\"Mean Test AUC:       {np.mean(test_fold_aucs):.4f} ± {np.std(test_fold_aucs):.4f}\")\n",
    "    print(\"=================================================\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_classification_results_patient_image(patient_to_true_labels, \n",
    "                                               patient_to_predicted_probs, \n",
    "                                               patients,\n",
    "                                               dataset_name=\"Dataset\",\n",
    "                                               jitter=0.05):\n",
    "    \"\"\"\n",
    "    Plot classification results at both patient-level and image-level.\n",
    "    Adds jitter to x-axis so class 0 and 1 points are closer together.\n",
    "    \n",
    "    Parameters:\n",
    "    - patient_to_true_labels: dict {patient_id: true_label}\n",
    "    - patient_to_predicted_probs: dict {patient_id: list of predicted probabilities per image}\n",
    "    - patients: list of patient objects (must have .patient_id and .egfr_val)\n",
    "    - dataset_name: string for plot titles\n",
    "    - jitter: float, max horizontal offset for points\n",
    "    \"\"\"\n",
    "    patient_map = {p.patient_id: p for p in patients}\n",
    "\n",
    "    # ================= Patient-level =================\n",
    "    patient_true_labels, patient_pred_labels, patient_egfr = [], [], []\n",
    "\n",
    "    for patient_id in patient_to_true_labels:\n",
    "        true_label = patient_to_true_labels[patient_id]\n",
    "        probs = patient_to_predicted_probs[patient_id]\n",
    "        avg_prob = np.mean(probs)\n",
    "        pred_label = 1 if avg_prob >= 0.5 else 0\n",
    "\n",
    "        patient_true_labels.append(true_label)\n",
    "        patient_pred_labels.append(pred_label)\n",
    "        patient_egfr.append(patient_map[patient_id].egfr_val)\n",
    "\n",
    "    patient_true_labels = np.array(patient_true_labels)\n",
    "    patient_pred_labels = np.array(patient_pred_labels)\n",
    "    patient_egfr = np.array(patient_egfr)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Add jitter\n",
    "    x_jitter = patient_true_labels + np.random.uniform(-jitter, jitter, size=len(patient_true_labels))\n",
    "\n",
    "    correct_idx = np.where(patient_true_labels == patient_pred_labels)[0]\n",
    "    wrong_idx = np.where(patient_true_labels != patient_pred_labels)[0]\n",
    "\n",
    "    plt.scatter(x_jitter[correct_idx], patient_egfr[correct_idx], \n",
    "                color=\"blue\", alpha=0.7, label=\"Correct\")\n",
    "    plt.scatter(x_jitter[wrong_idx], patient_egfr[wrong_idx], \n",
    "                color=\"red\", alpha=0.7, label=\"Misclassified\")\n",
    "\n",
    "    plt.xticks([0, 1], [\"Class 0 (<60)\", \"Class 1 (≥60)\"])\n",
    "    plt.xlabel(\"True Class\")\n",
    "    plt.ylabel(\"eGFR (continuous)\")\n",
    "    plt.title(f\"Patient-level Predictions: {dataset_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    # ================= Image-level =================\n",
    "    image_true_labels, image_pred_labels, image_egfr = [], [], []\n",
    "\n",
    "    for patient_id in patient_to_true_labels:\n",
    "        true_label = patient_to_true_labels[patient_id]\n",
    "        probs = patient_to_predicted_probs[patient_id]\n",
    "        egfr_val = patient_map[patient_id].egfr_val\n",
    "\n",
    "        image_true_labels.extend([true_label]*len(probs))\n",
    "        image_pred_labels.extend([1 if p >= 0.5 else 0 for p in probs])\n",
    "        image_egfr.extend([egfr_val]*len(probs))\n",
    "\n",
    "    image_true_labels = np.array(image_true_labels)\n",
    "    image_pred_labels = np.array(image_pred_labels)\n",
    "    image_egfr = np.array(image_egfr)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Add jitter\n",
    "    x_jitter = image_true_labels + np.random.uniform(-jitter, jitter, size=len(image_true_labels))\n",
    "\n",
    "    correct_idx = np.where(image_true_labels == image_pred_labels)[0]\n",
    "    wrong_idx = np.where(image_true_labels != image_pred_labels)[0]\n",
    "\n",
    "    plt.scatter(x_jitter[correct_idx], image_egfr[correct_idx], \n",
    "                color=\"blue\", alpha=0.3, label=\"Correct\")\n",
    "    plt.scatter(x_jitter[wrong_idx], image_egfr[wrong_idx], \n",
    "                color=\"red\", alpha=0.3, label=\"Misclassified\")\n",
    "\n",
    "    plt.xticks([0, 1], [\"Class 0 (<60)\", \"Class 1 (≥60)\"])\n",
    "    plt.xlabel(\"True Class\")\n",
    "    plt.ylabel(\"eGFR (continuous)\")\n",
    "    plt.title(f\"Image-level Predictions: {dataset_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "etb80JBbucZX",
    "outputId": "e9030893-8ae8-4c7b-b72b-6a4bc0b42b4c"
   },
   "outputs": [],
   "source": [
    "#running stratified hold-out split with early stopping and best weights storage \n",
    "#used for final model training and testing \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "EPOCHS = 100\n",
    "SEED         = 42\n",
    "EPOCHS       = 100\n",
    "N_RUNS       = 5\n",
    "\n",
    "# ===================================================\n",
    "# Load patients\n",
    "# ===================================================\n",
    "all_patients = label_img_classification_by_patient(IMAGE_FOLDER, CSV_FILE)\n",
    "all_labels   = np.array([p.egfr for p in all_patients])  # patient-level labels\n",
    "\n",
    "# ===================================================\n",
    "# Run 5 stratified hold-out splits\n",
    "# ===================================================\n",
    "\n",
    "for config_name, model_creator in hyper_parameter_tuning_configs.items():\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Training with Config: {config_name}\")\n",
    "    print(f\"==============================\")\n",
    "    val_aucs = []\n",
    "\n",
    "    test_aucs = []\n",
    "    for run in range(N_RUNS):\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\" Hold-out Run {run+1}/{N_RUNS} \")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        # Outer stratified split (90% train+val, 10% test)\n",
    "        trainval_patients, test_patients, trainval_labels, test_labels = train_test_split(\n",
    "            all_patients,\n",
    "            all_labels,\n",
    "            test_size=0.1,\n",
    "            random_state=SEED + run,\n",
    "            stratify=all_labels\n",
    "        )\n",
    "\n",
    "        # Inner stratified split (for early stopping)\n",
    "        train_patients, val_patients, train_labels, val_labels = train_test_split(\n",
    "            trainval_patients,\n",
    "            trainval_labels,\n",
    "            test_size=0.2,\n",
    "            random_state=SEED + run,\n",
    "            stratify=trainval_labels\n",
    "        )\n",
    "\n",
    "        # Build datasets\n",
    "        train_dataset = create_dataset_from_patients(train_patients, augment=True,  batch_size=BATCH_SIZE)\n",
    "        val_dataset   = create_dataset_from_patients(val_patients,   augment=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Build and compile model\n",
    "        model = model_creator()\n",
    "\n",
    "        \n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_auc\", mode=\"max\", patience=50, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        def step_decay(epoch, lr):\n",
    "            drop_rate = 0.5        # factor to reduce LR by\n",
    "            drop_every = 20        # reduce every 20 epochs\n",
    "            if epoch > 0 and epoch % drop_every == 0:\n",
    "                return lr * drop_rate\n",
    "            return lr\n",
    "\n",
    "        lr_scheduler = tf.keras.callbacks.LearningRateScheduler(step_decay, verbose=1)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[early_stopping, lr_scheduler],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        plotTrainingHistory(history)\n",
    "\n",
    "        # ===================================================\n",
    "        # Evaluate TRAIN set (patient-level)\n",
    "        # ===================================================\n",
    "        train_true, train_probs = plotAndReturnValidationTesting(train_patients, model)\n",
    "        plot_classification_results_patient_image(train_true, train_probs, train_patients, dataset_name=\"Training Set\")\n",
    "\n",
    "\n",
    "        # -------------------------\n",
    "        # Validation performance\n",
    "        # -------------------------\n",
    "        patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting(val_patients, model)\n",
    "\n",
    "        true_labels, predicted_probs = [], []\n",
    "        for patient_id in patient_to_predicted_probs:\n",
    "            for prob in patient_to_predicted_probs[patient_id]:\n",
    "                true_labels.append(patient_to_true_labels[patient_id])\n",
    "                predicted_probs.append(prob)\n",
    "\n",
    "        predicted_labels = [1 if p >= 0.5 else 0 for p in predicted_probs]\n",
    "        val_auc = roc_auc_score(true_labels, predicted_probs)\n",
    "        val_aucs.append(val_auc)\n",
    "        print(f\"Validation AUC (90/10): {val_auc:.4f}\")\n",
    "        print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "        plot_classification_results_patient_image(patient_to_true_labels, patient_to_predicted_probs, val_patients, dataset_name=\"Validation Set\")\n",
    "\n",
    "        # ===================================================\n",
    "        # Evaluate on test set (PATIENT LEVEL)\n",
    "        # ===================================================\n",
    "\n",
    "        patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting(test_patients, model)\n",
    "\n",
    "\n",
    "        true_labels, predicted_probs = [], []\n",
    "        for patient_id in patient_to_predicted_probs:\n",
    "            for prob in patient_to_predicted_probs[patient_id]:\n",
    "                true_labels.append(patient_to_true_labels[patient_id])\n",
    "                predicted_probs.append(prob)\n",
    "\n",
    "        predicted_labels = [1 if p >= 0.5 else 0 for p in predicted_probs]\n",
    "        test_auc = roc_auc_score(true_labels, predicted_probs)\n",
    "        test_aucs.append(test_auc)\n",
    "        print(f\"Test AUC (Run {run}): {test_auc:.4f}\")\n",
    "        print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "        plot_classification_results_patient_image(patient_to_true_labels, patient_to_predicted_probs, test_patients, dataset_name=\"Test Set\")\n",
    "\n",
    "# ===================================================\n",
    "# Final summary across runs\n",
    "# ===================================================\n",
    "print(\"\\n==============================\")\n",
    "print(\"\\n==============================\")\n",
    "print(\" Validation Test Results \")\n",
    "print(\"==============================\")\n",
    "print(f\"Mean Validation AUC: {np.mean(val_aucs):.4f} ± {np.std(val_aucs):.4f}\")\n",
    "\n",
    "print(\" Final Hold-out Test Results \")\n",
    "print(\"==============================\")\n",
    "print(f\"Mean Test AUC: {np.mean(test_aucs):.4f} ± {np.std(test_aucs):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for hold out testing without validation set \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 100\n",
    "SEED   = 42\n",
    "N_RUNS = 5\n",
    "\n",
    "# ===================================================\n",
    "# Load patients\n",
    "# ===================================================\n",
    "all_patients = label_img_classification_by_patient(IMAGE_FOLDER, CSV_FILE)\n",
    "all_labels   = np.array([p.egfr for p in all_patients])  # patient-level labels\n",
    "\n",
    "# ===================================================\n",
    "# Run stratified hold-out splits (no validation)\n",
    "# ===================================================\n",
    "for config_name, model_creator in hyper_parameter_tuning_configs.items():\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Training with Config: {config_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    test_aucs = []\n",
    "\n",
    "    for run in range(N_RUNS):\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\" Hold-out Run {run+1}/{N_RUNS} \")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        # Outer stratified split (90% train, 10% test)\n",
    "        train_patients, test_patients, train_labels, test_labels = train_test_split(\n",
    "            all_patients,\n",
    "            all_labels,\n",
    "            test_size=0.1,\n",
    "            random_state=SEED + run,\n",
    "            stratify=all_labels\n",
    "        )\n",
    "\n",
    "        # Build datasets\n",
    "        train_dataset = create_dataset_from_patients(train_patients, augment=True,  batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Build and compile model\n",
    "        model = model_creator()\n",
    "\n",
    "        # Learning rate scheduler only (no early stopping since no val set)\n",
    "        def step_decay(epoch, lr):\n",
    "            drop_rate = 0.5\n",
    "            drop_every = 10\n",
    "            if epoch > 0 and epoch % drop_every == 0:\n",
    "                return lr * drop_rate\n",
    "            return lr\n",
    "\n",
    "        lr_scheduler = tf.keras.callbacks.LearningRateScheduler(step_decay, verbose=1)\n",
    "\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[lr_scheduler],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        plotTrainingHistory(history)\n",
    "\n",
    "        # ===================================================\n",
    "        # Evaluate TRAIN set (patient-level)\n",
    "        # ===================================================\n",
    "        train_true, train_probs = plotAndReturnValidationTesting(train_patients, model)\n",
    "        plot_classification_results_patient_image(train_true, train_probs, train_patients, dataset_name=\"Training Set\")\n",
    "\n",
    "        # ===================================================\n",
    "        # Evaluate TEST set (patient-level)\n",
    "        # ===================================================\n",
    "        patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting(test_patients, model)\n",
    "\n",
    "        true_labels, predicted_probs = [], []\n",
    "        for patient_id in patient_to_predicted_probs:\n",
    "            for prob in patient_to_predicted_probs[patient_id]:\n",
    "                true_labels.append(patient_to_true_labels[patient_id])\n",
    "                predicted_probs.append(prob)\n",
    "\n",
    "        predicted_labels = [1 if p >= 0.5 else 0 for p in predicted_probs]\n",
    "        test_auc = roc_auc_score(true_labels, predicted_probs)\n",
    "        test_aucs.append(test_auc)\n",
    "\n",
    "        print(f\"Test AUC (Run {run+1}): {test_auc:.4f}\")\n",
    "        print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "        plot_classification_results_patient_image(patient_to_true_labels, patient_to_predicted_probs, test_patients, dataset_name=\"Test Set\")\n",
    "\n",
    "    # ===================================================\n",
    "    # Final summary across runs\n",
    "    # ===================================================\n",
    "    print(\"\\n==============================\")\n",
    "    print(\" Final Hold-out Test Results \")\n",
    "    print(\"==============================\")\n",
    "    print(f\"Mean Test AUC: {np.mean(test_aucs):.4f} ± {np.std(test_aucs):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score, classification_report\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------------\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Step 1: Load patients and split into train/val vs test\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------------\u001b[39;00m\n\u001b[32m     10\u001b[39m all_patients = label_img_classification_by_patient(IMAGE_FOLDER, CSV_FILE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TUPIL/Code/TUPIL_Kidney/venv/lib/python3.13/site-packages/tensorflow/__init__.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TUPIL/Code/TUPIL_Kidney/venv/lib/python3.13/site-packages/tensorflow/python/pywrap_tensorflow.py:37\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m self_check\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# TODO(mdan): Cleanup antipattern: import for side effects.\u001b[39;00m\n\u001b[32m     35\u001b[39m \n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mself_check\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreload_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     42\u001b[39m   \u001b[38;5;66;03m# This import is expected to fail if there is an explicit shared object\u001b[39;00m\n\u001b[32m     43\u001b[39m   \u001b[38;5;66;03m# dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TUPIL/Code/TUPIL_Kidney/venv/lib/python3.13/site-packages/tensorflow/python/platform/self_check.py:63\u001b[39m, in \u001b[36mpreload_check\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     50\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     51\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mCould not find the DLL(s) \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m. TensorFlow requires that these DLLs \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mbe installed in a directory that is named in your \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[33mPATH\u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mhttps://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m           % \u001b[33m\"\u001b[39m\u001b[33m or \u001b[39m\u001b[33m\"\u001b[39m.join(missing))\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     59\u001b[39m   \u001b[38;5;66;03m# Load a library that performs CPU feature guard checking.  Doing this here\u001b[39;00m\n\u001b[32m     60\u001b[39m   \u001b[38;5;66;03m# as a preload check makes it more likely that we detect any CPU feature\u001b[39;00m\n\u001b[32m     61\u001b[39m   \u001b[38;5;66;03m# incompatibilities before we trigger them (which would typically result in\u001b[39;00m\n\u001b[32m     62\u001b[39m   \u001b[38;5;66;03m# SIGILL).\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pywrap_cpu_feature_guard\n\u001b[32m     64\u001b[39m   _pywrap_cpu_feature_guard.InfoAboutUnusedCPUFeatures()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 1: Load patients and split into train/val vs test\n",
    "# ---------------------------------------------------\n",
    "all_patients = label_img_classification_by_patient(B_MODE_IMAGE_FOLDER, H_SCAN_IMAGE_FOLDER, CSV_FILE)\n",
    "\n",
    "train_and_val_patients, test_patients = train_test_split(\n",
    "    all_patients, test_size=0.1, random_state=SEED\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 2: Hyperparameter tuning with stratified k-fold CV\n",
    "# ---------------------------------------------------\n",
    "for config_name, model_creator in hyper_parameter_tuning_configs.items():\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Training with Config: {config_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    all_patient_egfr = [patient.egfr for patient in train_and_val_patients]\n",
    "\n",
    "    val_fold_aucs = []\n",
    "    test_fold_aucs = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_and_val_patients, all_patient_egfr)):\n",
    "        print(f\"\\n----- Fold {fold + 1} -----\")\n",
    "\n",
    "        train_patients = [train_and_val_patients[i] for i in train_idx]\n",
    "        val_patients = [train_and_val_patients[i] for i in val_idx]\n",
    "\n",
    "        # Plot distribution\n",
    "        plotHistogramOfDataset([p.egfr for p in train_patients], [p.egfr for p in val_patients], 2)\n",
    "\n",
    "        # -------------------------\n",
    "        # Create datasets for two-input models\n",
    "        # Assume `create_two_input_dataset_from_patients` returns\n",
    "        # a tf.data.Dataset of ([input1_batch, input2_batch], labels_batch)\n",
    "        # -------------------------\n",
    "        train_dataset_fold = create_dataset_from_patients(train_patients, augment=True, batch_size=BATCH_SIZE)\n",
    "        val_dataset = create_dataset_from_patients(val_patients, augment=False, batch_size=BATCH_SIZE)\n",
    "        test_dataset = create_dataset_from_patients(test_patients, augment=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Compute class weights\n",
    "        weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(np.array([p.egfr for p in train_patients]).astype(int)),\n",
    "            y=np.array([p.egfr for p in train_patients]).astype(int)\n",
    "        )\n",
    "        class_weights_dict = dict(enumerate(weights))\n",
    "\n",
    "        # Build model\n",
    "        model = model_creator()  # model_creator should now return a two-input model\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_auc', mode='max', patience=50, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            train_dataset_fold,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[early_stopping],\n",
    "            class_weight=class_weights_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        plotTrainingHistory(history)\n",
    "\n",
    "        # -------------------------\n",
    "        # Validation performance\n",
    "        # -------------------------\n",
    "        patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting(val_patients, model)\n",
    "\n",
    "        true_labels, predicted_probs = [], []\n",
    "        for patient_id in patient_to_predicted_probs:\n",
    "            for prob in patient_to_predicted_probs[patient_id]:\n",
    "                true_labels.append(patient_to_true_labels[patient_id])\n",
    "                predicted_probs.append(prob)\n",
    "\n",
    "        predicted_labels = [1 if p >= 0.5 else 0 for p in predicted_probs]\n",
    "        val_auc = roc_auc_score(true_labels, predicted_probs)\n",
    "        print(f\"Validation AUC (Fold {fold + 1}): {val_auc:.4f}\")\n",
    "        print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "        val_fold_aucs.append(val_auc)\n",
    "\n",
    "        # Save best model for fold\n",
    "        model.save_weights(f\"best_model_fold_{fold+1}_{config_name}.weights.h5\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Test performance (same fold model)\n",
    "        # -------------------------\n",
    "        patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting(test_patients, model)\n",
    "\n",
    "        true_labels, predicted_probs = [], []\n",
    "        for patient_id in patient_to_predicted_probs:\n",
    "            for prob in patient_to_predicted_probs[patient_id]:\n",
    "                true_labels.append(patient_to_true_labels[patient_id])\n",
    "                predicted_probs.append(prob)\n",
    "\n",
    "        predicted_labels = [1 if p >= 0.5 else 0 for p in predicted_probs]\n",
    "        test_auc = roc_auc_score(true_labels, predicted_probs)\n",
    "        print(f\"Test AUC (Fold {fold + 1}): {test_auc:.4f}\")\n",
    "        print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "        test_fold_aucs.append(test_auc)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Final summary for this config\n",
    "    # ---------------------------------------------------\n",
    "    print(f\"\\n==== Final Results for Config: {config_name} ====\")\n",
    "    print(f\"Mean Validation AUC: {np.mean(val_fold_aucs):.4f} ± {np.std(val_fold_aucs):.4f}\")\n",
    "    print(f\"Mean Test AUC:       {np.mean(test_fold_aucs):.4f} ± {np.std(test_fold_aucs):.4f}\")\n",
    "    print(\"=================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o96sOcJQa0Br"
   },
   "source": [
    "# Save Model to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4cREufHu-7h"
   },
   "outputs": [],
   "source": [
    "model_name = 'resnet_radimagenet_no_masking_lr0001__1346596590793527366'\n",
    "# Please do not set to true accidently, to ensure models to not get erased\n",
    "overwrite = False\n",
    "\n",
    "# Set model save path based on environment\n",
    "if is_google_colab() and is_google_drive_mounted():\n",
    "    model_save_path = '/content/drive/MyDrive/uttiya_test_lab/model_weights/' + model_name + '.keras'\n",
    "elif is_google_colab():\n",
    "    model_save_path = '/content/model_weights/' + model_name + '.keras'\n",
    "else:\n",
    "    model_save_path = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/model_weights/' + model_name + '.keras'\n",
    "\n",
    "model.save(model_save_path, overwrite=overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVpDXe3TBr8o"
   },
   "source": [
    "# Load model and show performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "error",
     "timestamp": 1756673513149,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "vbI2k82PSHiR",
    "outputId": "e71944c8-4d3a-4344-e5e4-a8eac71d3890"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "# Set model load path based on environment\n",
    "if is_google_colab() and is_google_drive_mounted():\n",
    "    model_load_path = '/content/drive/MyDrive/uttiya_test_lab/model_weights/' + model_name + '.keras'\n",
    "elif is_google_colab():\n",
    "    model_load_path = '/content/model_weights/' + model_name + '.keras'\n",
    "else:\n",
    "    model_load_path = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/model_weights/' + model_name + '.keras'\n",
    "\n",
    "model = tf.keras.models.load_model(model_load_path)\n",
    "\n",
    "# Predict on the test set\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    preds = model.predict(images)\n",
    "    true_labels.extend(labels.numpy().flatten())\n",
    "    predicted_labels.extend(preds.flatten())\n",
    "\n",
    "\n",
    "\n",
    "# Plot True vs Predicted eGFR\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(true_labels, predicted_labels, c='blue', alpha=0.5)\n",
    "# Draw line of best fit for the prediction (to show trend)\n",
    "plt.plot(np.unique(true_labels), np.poly1d(np.polyfit(true_labels, predicted_labels, 1))(np.unique(true_labels)))\n",
    "\n",
    "plt.plot([min(true_labels), max(true_labels)], [min(true_labels), max(true_labels)], 'r--')  # Perfect prediction line\n",
    "plt.xlabel('True eGFR')\n",
    "# Set scale to be 20-100\n",
    "plt.xlim(15, 115)\n",
    "plt.ylim(15, 110)\n",
    "plt.ylabel('Predicted eGFR')\n",
    "plt.title('True vs Predicted eGFR')\n",
    "plt.show()\n",
    "\n",
    "correlation = np.corrcoef(true_labels, predicted_labels)[0, 1]\n",
    "print(f\"Correlation between True and Predicted eGFR: {correlation:.2f}\")\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "results = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {results[0]}, Test MAE: {results[1]}\")\n",
    "\n",
    "# Compute metrics\n",
    "mae = mean_absolute_error(true_labels, predicted_labels)\n",
    "rmse = np.sqrt(mean_squared_error(true_labels, predicted_labels))\n",
    "r2 = r2_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUhbM0_IDFlJ"
   },
   "source": [
    "# Display Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "error",
     "timestamp": 1741125163253,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 300
    },
    "id": "WrLeVEnPDHVL",
    "outputId": "e14e6e6d-6548-4646-c7dc-4dd48321d9a7"
   },
   "outputs": [],
   "source": [
    "# Show feature maps of conv layers\n",
    "\n",
    "for layer in model.layers[1].layers:\n",
    "  # print(layer.name)\n",
    "\n",
    "  if 'conv' not in layer.name:\n",
    "      continue\n",
    "  filters , bias = layer.get_weights()\n",
    "  print(layer.name , filters.shape)\n",
    "\n",
    "filters, bias = model.layers[1].layers[1].get_weights()\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "n_filters =6\n",
    "ix=1\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "for i in range(n_filters):\n",
    "    # get the filters\n",
    "    f = filters[:,:,:,i]\n",
    "    for j in range(3):\n",
    "        # subplot for 6 filters and 3 channels\n",
    "        plt.subplot(n_filters,3,ix)\n",
    "        plt.imshow(f[:,:,j] ,cmap='gray')\n",
    "        ix+=1\n",
    "#plot the filters\n",
    "plt.show()\n",
    "\n",
    "# Visualization of first block\n",
    "first_block_model = Model(inputs=model.layers[1].inputs, outputs=model.layers[1].layers[1].output)\n",
    "\n",
    "image = load_img(IMAGE_FOLDER + '/Patient_8_Resized_Image_1.png' , target_size=(224,224))\n",
    "\n",
    "# convert the image to an array\n",
    "image = img_to_array(image)\n",
    "# expand dimensions so that it represents a single 'sample'\n",
    "image = expand_dims(image, axis=0)\n",
    "\n",
    "image = preprocess_input(image)\n",
    "\n",
    "#calculating features_map\n",
    "features = first_block_model.predict(image)\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "for i in range(1,features.shape[3]+1):\n",
    "    fig.suptitle(\"BLOCK_1\", fontsize=20)\n",
    "    plt.subplot(8,8,i)\n",
    "    plt.imshow(features[0,:,:,i-1] , cmap='gray')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Visualization of other blocks\n",
    "blocks = [ 2, 5 , 9 , 13 , 17]\n",
    "outputs = [model.layers[1].layers[i].output for i in blocks]\n",
    "\n",
    "model2 = Model(inputs= model.layers[1].inputs, outputs = outputs)\n",
    "\n",
    "feature_map = model2.predict(image)\n",
    "\n",
    "for i,fmap in zip(blocks,feature_map):\n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "    #https://stackoverflow.com/a/12444777\n",
    "    fig.suptitle(\"BLOCK_{}\".format(i) , fontsize=20)\n",
    "    for i in range(1,features.shape[3]+1):\n",
    "\n",
    "        plt.subplot(8,8,i)\n",
    "        plt.imshow(fmap[0,:,:,i-1] , cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM9pDb4gqcW+6GHF8ydM7uE",
   "collapsed_sections": [
    "7vLhJCCFcjb1",
    "c2MVI9TGcnTc",
    "zd1NaH0Tm5Kb"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "1f2Z66JDnwNNAjyugDY8yz4izwuL-tiqc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
