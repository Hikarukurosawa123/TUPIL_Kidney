{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "executionInfo": {
     "elapsed": 25638,
     "status": "error",
     "timestamp": 1756486779113,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "E-mCkvSCGVH9",
    "outputId": "28de828b-0a42-4102-d563-461917826d2c"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/Hikarukurosawa123/TUPIL_Kidney.git\n",
    "#!git clone https://github.com/Hikarukurosawa123/TUPIL_Kidney.git drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney\n",
    "#%cd drive/MyDrive/Hikaru_Colab_Workspace/TUPIL_Kidney\n",
    "#!git stash\n",
    "\n",
    "!git pull origin main     # or \"master\", depending on the branch name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4331,
     "status": "ok",
     "timestamp": 1757878289738,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "KbXHUIK5oT2y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "IMAGE_FOLDER: /Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/lanczos_shape_corrected_only_nc_resized_images\n",
      "CSV_FILE: /Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/csv/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv\n",
      "MODEL_WEIGHTS_PATH: /Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/model_weights/RadImageNet-ResNet50_notop.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    "from skimage.transform import resize\n",
    "from collections import defaultdict\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from numpy import expand_dims\n",
    "import scipy.ndimage\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Environment detection\n",
    "def is_google_colab():\n",
    "    \"\"\"Check if running in Google Colab environment\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def is_google_drive_mounted():\n",
    "    \"\"\"Check if Google Drive is mounted in Colab\"\"\"\n",
    "    return os.path.exists('/content/drive/MyDrive')\n",
    "\n",
    "# Set paths based on environment\n",
    "if is_google_colab() and is_google_drive_mounted():\n",
    "    # Google Colab with Google Drive mounted\n",
    "    IMAGE_FOLDER = '/content/drive/MyDrive/Hikaru_Colab_Workspace/lanczos_shape_corrected_only_nc_resized_images'\n",
    "    CSV_FILE = '/content/drive/MyDrive/Hikaru_Colab_Workspace/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
    "    MODEL_WEIGHTS_PATH = '/content/drive/MyDrive/Hikaru_Colab_Workspace/model_weights/RadImageNet-ResNet50_notop.h5'\n",
    "    print(\"Running on Google Colab with Google Drive mounted\")\n",
    "elif is_google_colab():\n",
    "    # Google Colab without Google Drive mounted\n",
    "    IMAGE_FOLDER = '/content/lanczos_shape_corrected_only_nc_resized_images'\n",
    "    CSV_FILE = '/content/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
    "    MODEL_WEIGHTS_PATH = '/content/model_weights/RadImageNet-ResNet50_notop.h5'\n",
    "    print(\"Running on Google Colab without Google Drive mounted\")\n",
    "else:\n",
    "    # Local environment\n",
    "    IMAGE_FOLDER = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/lanczos_shape_corrected_only_nc_resized_images'\n",
    "    CSV_FILE = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/csv/patient_eGFR_at_pocus_2025_Jul_polynomial_estimation.csv'\n",
    "    MODEL_WEIGHTS_PATH = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/model_weights/RadImageNet-ResNet50_notop.h5'\n",
    "    print(\"Running locally\")\n",
    "\n",
    "print(f\"IMAGE_FOLDER: {IMAGE_FOLDER}\")\n",
    "print(f\"CSV_FILE: {CSV_FILE}\")\n",
    "print(f\"MODEL_WEIGHTS_PATH: {MODEL_WEIGHTS_PATH}\")\n",
    "\n",
    "BATCH_SIZE = 16 # batch = 8 -> 10% batch = 16 --> doesnt work\n",
    "EPOCHS = 160 # epoch = 40 --> 10% epoch = 50 --> 9%\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Detection Test ===\n",
      "is_google_colab(): False\n",
      "is_google_drive_mounted(): False\n",
      "Current working directory: /Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney\n",
      "IMAGE_FOLDER exists: True\n",
      "CSV_FILE exists: True\n",
      "MODEL_WEIGHTS_PATH exists: True\n",
      "=== End Test ===\n"
     ]
    }
   ],
   "source": [
    "# Test environment detection and path setup\n",
    "print(\"=== Environment Detection Test ===\") \n",
    "print(f\"is_google_colab(): {is_google_colab()}\")\n",
    "print(f\"is_google_drive_mounted(): {is_google_drive_mounted()}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"IMAGE_FOLDER exists: {os.path.exists(IMAGE_FOLDER)}\")\n",
    "print(f\"CSV_FILE exists: {os.path.exists(CSV_FILE)}\")\n",
    "print(f\"MODEL_WEIGHTS_PATH exists: {os.path.exists(MODEL_WEIGHTS_PATH)}\")\n",
    "print(\"=== End Test ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1757878290965,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "vHGZMxH_U5p0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROFXH-uNi8OT"
   },
   "source": [
    "## 1. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1757878292553,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "SNQ-3gY75oY2"
   },
   "outputs": [],
   "source": [
    "# Patient class used to hold all information related for training\n",
    "class Patient:\n",
    "    def __init__(self, patient_id, egfr, image_paths):\n",
    "        self.patient_id = patient_id\n",
    "        self.egfr = egfr\n",
    "        self.image_paths = image_paths\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Patient({self.patient_id}, eGFR={self.egfr}, # of images={len(self.image_paths)})\"\n",
    "\n",
    "# Creates patients objects based on images and eGFR csv raw dataset\n",
    "def label_img_classification_by_patient(image_folder, csv_file):\n",
    "    eGFR_data = pd.read_csv(csv_file)\n",
    "    eGFR_data.rename(columns={'Patient ID': 'patient_id', 'eGFR (abs/closest)': 'eGFR'}, inplace=True)\n",
    "    eGFR_data['patient_id'] = eGFR_data['patient_id'].astype(int)\n",
    "    eGFR_data.set_index('patient_id', inplace=True)\n",
    "\n",
    "    patient_image_map = defaultdict(list)\n",
    "\n",
    "    for filename in sorted(os.listdir(image_folder)):\n",
    "        try:\n",
    "            patient_id = int(filename.split('_')[1])  # adjust if your filename pattern changes\n",
    "            if patient_id in eGFR_data.index:\n",
    "                img_path_full = os.path.join(image_folder, filename)\n",
    "                patient_image_map[patient_id].append(img_path_full)\n",
    "            else:\n",
    "                print(f\"Patient ID {patient_id} not found in CSV, skipping...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "    # Build Patient objects\n",
    "    patient_objects = []\n",
    "    for patient_id, image_paths in patient_image_map.items():\n",
    "        egfr = eGFR_data.loc[patient_id, 'eGFR']\n",
    "\n",
    "        egfrLabel = 1 if egfr >= 60 else 0\n",
    "\n",
    "        patient_objects.append(Patient(patient_id, egfrLabel, image_paths))\n",
    "\n",
    "    return patient_objects\n",
    "\n",
    "# Prepare image\n",
    "def preprocess_img(img_path):\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_png(image, channels=3)  # Gray-scale image\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # Convert TensorFlow tensor to NumPy array for compatibility with ImageDataGenerator\n",
    "    return tf.keras.utils.img_to_array(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1757875267776,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "2TzDCCDAtd09"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSvJbc1WwiPC"
   },
   "source": [
    "# WIP: NEW DATA AUGMENT SYSTEM\n",
    "I really don't like the ImageDataGenerator\n",
    "- Its deprecated\n",
    "- It created random results\n",
    "- It only performs 1 operation (ex. horizontal flip or vertical flip or crop)\n",
    "- This WIP system should just take each image, and create multiple images from it, using every operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1757878294681,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "ubivM9i1w_Mb"
   },
   "outputs": [],
   "source": [
    "def augment_images(original_images, labels, random_state):\n",
    "    \"\"\"Generate augmented images.\n",
    "       Returns array of images and labels containing the original and augmented images.\n",
    "    \"\"\"\n",
    "    augmented_images = original_images.copy()\n",
    "    augmented_labels = labels.copy()\n",
    "\n",
    "    deterministic_random_number_gen = np.random.default_rng(random_state)\n",
    "\n",
    "    for image, label in zip(original_images, labels):\n",
    "        # First add horizontally flipped image\n",
    "        augmented_images.append(np.fliplr(image))\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "        # Randomly rotate between -40 and 40 degrees (black as padding)\n",
    "        angle = deterministic_random_number_gen.integers(-40, 40)\n",
    "        rotated_image = scipy.ndimage.rotate(image, angle, reshape=False, mode='constant', cval=1)\n",
    "        augmented_images.append(rotated_image)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "        # Rotate again\n",
    "        angle = deterministic_random_number_gen.integers(-40, 40)\n",
    "        rotated_image = scipy.ndimage.rotate(image, angle, reshape=False, mode='constant', cval=1)\n",
    "        augmented_images.append(rotated_image)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "        # Crop image randomly\n",
    "        height, width = image.shape[:2]\n",
    "        random_array = deterministic_random_number_gen.random(size=4);\n",
    "        # Ensure crop is a square aspect ratio\n",
    "        w = int((width*0.90) * (1+random_array[0]*0.10))\n",
    "        h = w\n",
    "        x = int(random_array[2] * (width-w))\n",
    "        y = int(random_array[3] * (height-h))\n",
    "\n",
    "        image_crop = image[y:h+y, x:w+x, 0:3]\n",
    "        image_crop = resize(image_crop, image.shape, anti_aliasing=True)\n",
    "        augmented_images.append(image_crop)\n",
    "        augmented_labels.append(label)\n",
    "\n",
    "    return np.array(augmented_images), np.array(augmented_labels)\n",
    "\n",
    "def create_dataset_wip(file_paths, labels, augment=False, batch_size=4):\n",
    "    # Not using generator in this implementation\n",
    "    all_images = [preprocess_img(file_path) for file_path in file_paths]\n",
    "    all_labels = labels\n",
    "\n",
    "    if augment:\n",
    "        all_images, all_labels = augment_images_wip(all_images, all_labels, SEED)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((all_images, all_labels))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "def create_dataset_from_patients(patients, augment=False, batch_size=4):\n",
    "    # Load all images + labels into arrays\n",
    "    all_images = [preprocess_img(file_path) for patient in patients for file_path in patient.image_paths]\n",
    "    all_labels = [patient.egfr for patient in patients for image in patient.image_paths]\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((all_images, all_labels))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    if augment:\n",
    "        # Apply augmentation on-the-fly\n",
    "        # data_augmentation = tf.keras.Sequential([\n",
    "        #     tf.keras.layers.RandomFlip(\"horizontal\"),  # horizontal flip\n",
    "        #     tf.keras.layers.RandomTranslation(\n",
    "        #         height_factor=0.3,  # vertical shift up to 30%\n",
    "        #         width_factor=0.3    # horizontal shift up to 30%\n",
    "        #     ),\n",
    "        # ])\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.25),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        ])\n",
    "\n",
    "        # data_augmentation = tf.keras.Sequential([\n",
    "        #     # Random reflections\n",
    "        #     tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "\n",
    "        #     # Random rotation (0–360 degrees → full circle)\n",
    "        #     tf.keras.layers.RandomRotation(1.0),   # 1.0 = full range [-180°, +180°]\n",
    "\n",
    "        #     # Random scaling (0.8–1.2)\n",
    "        #     tf.keras.layers.RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2))\n",
    "        # ])\n",
    "\n",
    "        dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6544,
     "status": "ok",
     "timestamp": 1757878302925,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "54xI33ASF4DZ",
    "outputId": "a2fcc49d-5124-4a9a-8718-e0c3f0c92754"
   },
   "outputs": [],
   "source": [
    "# 1. Get file paths and labels\n",
    "# file_paths, eGFR_labels, absolute_file_paths_indices = label_img(IMAGE_FOLDER, CSV_FILE)\n",
    "patients = label_img_classification_by_patient(IMAGE_FOLDER, CSV_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1757878846533,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "PJ4tg4PXtoLS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def conv_block(x, filters, stride=1):\n",
    "    \"\"\"A residual block with projection shortcut when stride > 1\"\"\"\n",
    "    shortcut = x\n",
    "\n",
    "    # First conv\n",
    "    x = layers.Conv2D(filters, kernel_size=3, strides=stride, padding=\"same\",\n",
    "                      use_bias=False, kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # Second conv\n",
    "    x = layers.Conv2D(filters, kernel_size=3, strides=1, padding=\"same\",\n",
    "                      use_bias=False, kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Projection for shortcut if shape mismatch\n",
    "    if stride != 1 or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=1, strides=stride,\n",
    "                                 use_bias=False, kernel_initializer=\"he_normal\")(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_ResNet18(input_shape=(224, 224, 3)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Initial conv + maxpool\n",
    "    x = layers.Conv2D(64, 7, strides=2, padding=\"same\", use_bias=False,\n",
    "                      kernel_initializer=\"he_normal\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    # Residual blocks\n",
    "    x = conv_block(x, 64, stride=1)\n",
    "    x = conv_block(x, 64, stride=1)\n",
    "\n",
    "    x = conv_block(x, 128, stride=2)\n",
    "    x = conv_block(x, 128, stride=1)\n",
    "\n",
    "    x = conv_block(x, 256, stride=2)\n",
    "    x = conv_block(x, 256, stride=1)\n",
    "\n",
    "    x = conv_block(x, 512, stride=2)\n",
    "    x = conv_block(x, 512, stride=1)\n",
    "\n",
    "    # Global avg pool + classifier\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = layers.Dense(4096, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(2048, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs, name=\"ResNet18\")\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzD2bL_4YhKH"
   },
   "source": [
    "# Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 16042,
     "status": "ok",
     "timestamp": 1757875292835,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "qyvRbMXOYnts",
    "outputId": "45ce6193-3b1e-4d85-d9dc-530793ee9d51"
   },
   "outputs": [],
   "source": [
    "def build_resnet_model(with_transfer_learning):\n",
    "    if with_transfer_learning:\n",
    "      base_model = models.load_model(MODEL_WEIGHTS_PATH, compile=False)\n",
    "    else:\n",
    "      base_model = tf.keras.applications.ResNet50(weights=None, include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    base_model.trainable = True\n",
    "\n",
    "\n",
    "\n",
    "    model = models.Sequential([\n",
    "        # layers.Input(shape=(224, 224, 1)), # Define an Input layer with the desired shape\n",
    "        # layers.Conv2D(3, (3, 3), padding='same', activation='relu'), # Convert grayscale to 3 channels\n",
    "        base_model,\n",
    "        # layers.Flatten(input_shape=(7, 7, 512)),  # Equivalent to PyTorch's view(output.size(0), -1)\n",
    "        layers.GlobalAveragePooling2D(),  # Replace Flatten\n",
    "        layers.Dense(4096, activation='relu', ),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(2048, activation='relu', ),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1024, activation='relu',),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid'),\n",
    "\n",
    "        # kernel_regularizer=regularizers.l2(0.01)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile the model outside the function\n",
    "model = build_resnet_model(True)\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd1NaH0Tm5Kb"
   },
   "source": [
    "# Training With K-Fold (Patient Exclusivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K_JlxDZj4iS"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1757878389980,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "vifyDqG2j61w"
   },
   "outputs": [],
   "source": [
    "def plotHistogramOfDataset(training_labels, validation_labels, n_bins):\n",
    "  plt.hist(training_labels, bins=n_bins, alpha=0.5, label='Training Data', color='blue')\n",
    "  plt.hist(validation_labels, bins=n_bins, alpha=0.5, label='Validation Data', color='red')\n",
    "  plt.xlabel('eGFR')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.title(f'Histogram with {n_bins} bins')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def plotTrainingHistory(history):\n",
    "  # Plot training and validation loss\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.plot(history.history['loss'], label='Training Loss')\n",
    "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Binary Crossentropy Loss')\n",
    "  plt.title('Training and Validation Loss')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  # Plot Accuracy\n",
    "  plt.figure(figsize=(10, 6))\n",
    "  plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.title('Training and Validation Accuracy')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "# def plotAndReturnValidationTesting(val_patients, model):\n",
    "#   # Collect true vs prediction per patient\n",
    "#   patient_to_true_labels = {}\n",
    "#   patient_to_predicted_labels = {}\n",
    "\n",
    "\n",
    "#   for patient in val_patients:\n",
    "#     patient_dataset = create_dataset_from_patients([patient], augment=False, batch_size=BATCH_SIZE)\n",
    "#     for images, labels in patient_dataset:\n",
    "#       patient_to_true_labels[patient.patient_id] = labels[0]\n",
    "#       patient_to_predicted_labels[patient.patient_id] = model.predict(images).flatten()\n",
    "\n",
    "#   # Plot True vs Predicted eGFR\n",
    "#   fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#   for i, patient_id in enumerate(patient_to_true_labels):\n",
    "#     true_label = patient_to_true_labels[patient_id]\n",
    "#     predictions = patient_to_predicted_labels[patient_id]\n",
    "\n",
    "#     # Plot all individual predictions at the x-position of true_label\n",
    "#     x_vals = np.full_like(predictions, true_label, dtype=np.float32)\n",
    "#     ax.scatter(x_vals, predictions, color='blue', alpha=0.6, label='Predictions' if i == 0 else \"\")\n",
    "\n",
    "#     # Add faint vertical line (error bar)\n",
    "#     ax.vlines(x=true_label,\n",
    "#               ymin=np.min(predictions),\n",
    "#               ymax=np.max(predictions),\n",
    "#               color='gray',\n",
    "#               alpha=0.3,\n",
    "#               linewidth=2)\n",
    "\n",
    "#   # Line of best fit\n",
    "#   true_labels = list(patient_to_true_labels.values())\n",
    "#   predicted_labels = list(patient_to_predicted_labels.values())\n",
    "#   x_vals = np.unique(true_labels)\n",
    "#   fit_fn = np.poly1d(np.polyfit(true_labels, predicted_labels, 1))\n",
    "#   plt.plot(x_vals, fit_fn(x_vals), color='blue', linewidth=2, label=\"Fit Line\")\n",
    "\n",
    "#   # Identity line\n",
    "#   plt.plot([min(true_labels), max(true_labels)],\n",
    "#           [min(true_labels), max(true_labels)],\n",
    "#           'r--', label=\"Perfect Prediction\")\n",
    "\n",
    "#   ax.set_xlabel(\"True Label\")\n",
    "#   ax.set_ylabel(\"Predicted Values\")\n",
    "#   ax.set_title(\"True vs Predicted eGFR\")\n",
    "#   ax.grid(True)\n",
    "#   ax.legend()\n",
    "#   plt.show()\n",
    "\n",
    "#   return patient_to_true_labels, patient_to_predicted_labels\n",
    "def plotAndReturnValidationTesting(val_patients, model):\n",
    "    patient_to_true_labels = {}\n",
    "    patient_to_predicted_probs_list = {}\n",
    "\n",
    "    all_true_labels_individual = []\n",
    "    all_predicted_probs_individual = []\n",
    "\n",
    "    for patient in val_patients:\n",
    "        patient_dataset = create_dataset_from_patients([patient], augment=False, batch_size=BATCH_SIZE)\n",
    "        for images, labels in patient_dataset:\n",
    "            true_label = labels[0].numpy()  # Single label per patient\n",
    "            predictions = model.predict(images).flatten()\n",
    "\n",
    "            patient_to_true_labels[patient.patient_id] = true_label\n",
    "            patient_to_predicted_probs_list[patient.patient_id] = predictions.tolist()\n",
    "\n",
    "            # Collect individual predictions for metrics\n",
    "            all_true_labels_individual.extend(labels.numpy().flatten())\n",
    "            all_predicted_probs_individual.extend(predictions)\n",
    "\n",
    "    # Binarize predictions (e.g., threshold = 0.5)\n",
    "    all_predicted_labels_individual = [1 if prob >= 0.5 else 0 for prob in all_predicted_probs_individual]\n",
    "\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    accuracy = accuracy_score(all_true_labels_individual, all_predicted_labels_individual)\n",
    "    fpr, tpr, _ = roc_curve(all_true_labels_individual, all_predicted_probs_individual)\n",
    "    auc_score = roc_auc_score(all_true_labels_individual, all_predicted_probs_individual)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc_score:.4f})\", color='blue')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label=\"Random Guess\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return patient_to_true_labels, patient_to_predicted_probs_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFvqXLnSdMUO"
   },
   "source": [
    "# Hyper-parmeter tuning configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1757878854851,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "EFAX7f-_dJzD"
   },
   "outputs": [],
   "source": [
    "\n",
    "def resnet_no_tranfer_learning():\n",
    "  model = build_resnet_model(False)\n",
    "  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "  return model\n",
    "\n",
    "def resnet_with_tranfer_learning():\n",
    "  model = build_resnet_model(True)\n",
    "  model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "  return model\n",
    "\n",
    "hyper_parameter_tuning_configs = {\n",
    "    \n",
    "    'resnet_no_tranfer_learning': resnet_no_tranfer_learning,\n",
    "    'resnet_with_tranfer_learning': resnet_with_tranfer_learning,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUxxFe5ZiaAP"
   },
   "source": [
    "# Training with K-Fold (New Method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0D8ZpcfNn5K"
   },
   "source": [
    "Latest k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 544124,
     "status": "error",
     "timestamp": 1757811017692,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "tK-s75w_Nl7g",
    "outputId": "ca254c41-b914-4092-cbd3-a42844ce3ade"
   },
   "outputs": [],
   "source": [
    "#stratified k-fold cross validation with early stopping and best weights storage, no change in test set across each hyper parameter (only the validatoin, training set changes)\n",
    "#used for hyper parameter tuning \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 1: Load patients and split into train/val vs test\n",
    "# ---------------------------------------------------\n",
    "all_patients = label_img_classification_by_patient(IMAGE_FOLDER, CSV_FILE)\n",
    "\n",
    "train_and_val_patients, test_patients = train_test_split(\n",
    "    all_patients, test_size=0.1, random_state=SEED\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Step 2: Hyperparameter tuning with stratified k-fold CV\n",
    "# ---------------------------------------------------\n",
    "for config_name, model_creator in hyper_parameter_tuning_configs.items():\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Training with Config: {config_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    all_patient_egfr = [patient.egfr for patient in train_and_val_patients]\n",
    "\n",
    "    val_fold_aucs = []\n",
    "    test_fold_aucs = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_and_val_patients, all_patient_egfr)):\n",
    "        print(f\"\\n----- Fold {fold + 1} -----\")\n",
    "\n",
    "        train_patients = [train_and_val_patients[i] for i in train_idx]\n",
    "        val_patients = [train_and_val_patients[i] for i in val_idx]\n",
    "\n",
    "        # Plot distribution\n",
    "        plotHistogramOfDataset([p.egfr for p in train_patients], [p.egfr for p in val_patients], 2)\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset_fold = create_dataset_from_patients(train_patients, augment=True, batch_size=BATCH_SIZE)\n",
    "        val_dataset = create_dataset_from_patients(val_patients, augment=False, batch_size=BATCH_SIZE)\n",
    "        test_dataset = create_dataset_from_patients(test_patients, augment=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Compute class weights\n",
    "        weights = class_weight.compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(np.array([p.egfr for p in train_patients]).astype(int)),\n",
    "            y=np.array([p.egfr for p in train_patients]).astype(int)\n",
    "        )\n",
    "        class_weights_dict = dict(enumerate(weights))\n",
    "\n",
    "        # Build model\n",
    "        model = model_creator()\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_auc', mode='max', patience=50, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            train_dataset_fold,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[early_stopping],\n",
    "            class_weight=class_weights_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        plotTrainingHistory(history)\n",
    "\n",
    "        # -------------------------\n",
    "        # Validation performance\n",
    "        # -------------------------\n",
    "        patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting(val_patients, model)\n",
    "\n",
    "        true_labels, predicted_probs = [], []\n",
    "        for patient_id in patient_to_predicted_probs:\n",
    "            for prob in patient_to_predicted_probs[patient_id]:\n",
    "                true_labels.append(patient_to_true_labels[patient_id])\n",
    "                predicted_probs.append(prob)\n",
    "\n",
    "        predicted_labels = [1 if p >= 0.5 else 0 for p in predicted_probs]\n",
    "        val_auc = roc_auc_score(true_labels, predicted_probs)\n",
    "        print(f\"Validation AUC (Fold {fold + 1}): {val_auc:.4f}\")\n",
    "        print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "        val_fold_aucs.append(val_auc)\n",
    "\n",
    "        # Save best model for fold\n",
    "        model.save_weights(f\"best_model_fold_{fold+1}_{config_name}.weights.h5\")\n",
    "\n",
    "        # -------------------------\n",
    "        # Test performance (same fold model)\n",
    "        # -------------------------\n",
    "        patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting(test_patients, model)\n",
    "\n",
    "        true_labels, predicted_probs = [], []\n",
    "        for patient_id in patient_to_predicted_probs:\n",
    "            for prob in patient_to_predicted_probs[patient_id]:\n",
    "                true_labels.append(patient_to_true_labels[patient_id])\n",
    "                predicted_probs.append(prob)\n",
    "\n",
    "        predicted_labels = [1 if p >= 0.5 else 0 for p in predicted_probs]\n",
    "        test_auc = roc_auc_score(true_labels, predicted_probs)\n",
    "        print(f\"Test AUC (Fold {fold + 1}): {test_auc:.4f}\")\n",
    "        print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "        test_fold_aucs.append(test_auc)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Final summary for this config\n",
    "    # ---------------------------------------------------\n",
    "    print(f\"\\n==== Final Results for Config: {config_name} ====\")\n",
    "    print(f\"Mean Validation AUC: {np.mean(val_fold_aucs):.4f} ± {np.std(val_fold_aucs):.4f}\")\n",
    "    print(f\"Mean Test AUC:       {np.mean(test_fold_aucs):.4f} ± {np.std(test_fold_aucs):.4f}\")\n",
    "    print(\"=================================================\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# Plotting utility\n",
    "# ===================================================\n",
    "def plot_classification_results(patient_to_true_labels, \n",
    "                                patient_to_predicted_probs, \n",
    "                                dataset_name=\"Dataset\"):\n",
    "    \"\"\"Plot true class (x-axis) vs eGFR (y-axis), mark misclassifications in red.\"\"\"\n",
    "    true_labels, predicted_labels, egfr_values = [], [], []\n",
    "\n",
    "    for patient in patient_to_true_labels:\n",
    "        true_label = patient_to_true_labels[patient]\n",
    "        probs = patient_to_predicted_probs[patient]\n",
    "        avg_prob = np.mean(probs)  # average if multiple images per patient\n",
    "        pred_label = 1 if avg_prob >= 0.5 else 0\n",
    "\n",
    "        true_labels.append(true_label)\n",
    "        predicted_labels.append(pred_label)\n",
    "        egfr_values.append(patient.egfr)  # assumes patient object has .egfr attribute\n",
    "\n",
    "    true_labels = np.array(true_labels)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    egfr_values = np.array(egfr_values)\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for cls in [0, 1]:\n",
    "        idx = np.where(true_labels == cls)[0]\n",
    "        correct_idx = idx[true_labels[idx] == predicted_labels[idx]]\n",
    "        wrong_idx   = idx[true_labels[idx] != predicted_labels[idx]]\n",
    "\n",
    "        plt.scatter(np.full_like(correct_idx, cls), \n",
    "                    egfr_values[correct_idx], \n",
    "                    color=\"black\", alpha=0.7, \n",
    "                    label=f\"Class {cls} Correct\" if cls==0 else None)\n",
    "\n",
    "        plt.scatter(np.full_like(wrong_idx, cls), \n",
    "                    egfr_values[wrong_idx], \n",
    "                    color=\"red\", alpha=0.7, \n",
    "                    label=f\"Class {cls} Misclassified\" if cls==0 else None)\n",
    "\n",
    "    plt.xticks([0, 1], [\"Class 0\", \"Class 1\"])\n",
    "    plt.xlabel(\"True Class\")\n",
    "    plt.ylabel(\"eGFR\")\n",
    "    plt.title(f\"Classification Results: {dataset_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "etb80JBbucZX",
    "outputId": "e9030893-8ae8-4c7b-b72b-6a4bc0b42b4c"
   },
   "outputs": [],
   "source": [
    "#running stratified hold-out split with early stopping and best weights storage \n",
    "#used for final model training and testing \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "EPOCHS = 100\n",
    "SEED         = 42\n",
    "EPOCHS       = 100\n",
    "N_RUNS       = 5\n",
    "\n",
    "# ===================================================\n",
    "# Load patients\n",
    "# ===================================================\n",
    "all_patients = label_img_classification_by_patient(IMAGE_FOLDER, CSV_FILE)\n",
    "all_labels   = np.array([p.egfr for p in all_patients])  # patient-level labels\n",
    "\n",
    "# ===================================================\n",
    "# Run 5 stratified hold-out splits\n",
    "# ===================================================\n",
    "\n",
    "for config_name, model_creator in hyper_parameter_tuning_configs.items():\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"Training with Config: {config_name}\")\n",
    "    print(f\"==============================\")\n",
    "    val_aucs = []\n",
    "\n",
    "    test_aucs = []\n",
    "    for run in range(N_RUNS):\n",
    "        print(f\"\\n==============================\")\n",
    "        print(f\" Hold-out Run {run+1}/{N_RUNS} \")\n",
    "        print(f\"==============================\")\n",
    "\n",
    "        # Outer stratified split (90% train+val, 10% test)\n",
    "        trainval_patients, test_patients, trainval_labels, test_labels = train_test_split(\n",
    "            all_patients,\n",
    "            all_labels,\n",
    "            test_size=0.1,\n",
    "            random_state=SEED + run,\n",
    "            stratify=all_labels\n",
    "        )\n",
    "\n",
    "        # Inner stratified split (for early stopping)\n",
    "        train_patients, val_patients, train_labels, val_labels = train_test_split(\n",
    "            trainval_patients,\n",
    "            trainval_labels,\n",
    "            test_size=0.2,\n",
    "            random_state=SEED + run,\n",
    "            stratify=trainval_labels\n",
    "        )\n",
    "\n",
    "        # Build datasets\n",
    "        train_dataset = create_dataset_from_patients(train_patients, augment=True,  batch_size=BATCH_SIZE)\n",
    "        val_dataset   = create_dataset_from_patients(val_patients,   augment=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Build and compile model\n",
    "        model = model_creator()\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_auc\", mode=\"max\", patience=50, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        plotTrainingHistory(history)\n",
    "\n",
    "        # ===================================================\n",
    "        # Evaluate TRAIN set (patient-level)\n",
    "        # ===================================================\n",
    "        train_true, train_probs = plotAndReturnValidationTesting(train_patients, model)\n",
    "        plot_classification_results(train_true, train_probs, dataset_name=\"Training Set\")\n",
    "\n",
    "\n",
    "        # -------------------------\n",
    "        # Validation performance\n",
    "        # -------------------------\n",
    "        patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting(val_patients, model)\n",
    "\n",
    "        true_labels, predicted_probs = [], []\n",
    "        for patient_id in patient_to_predicted_probs:\n",
    "            for prob in patient_to_predicted_probs[patient_id]:\n",
    "                true_labels.append(patient_to_true_labels[patient_id])\n",
    "                predicted_probs.append(prob)\n",
    "\n",
    "        predicted_labels = [1 if p >= 0.5 else 0 for p in predicted_probs]\n",
    "        val_auc = roc_auc_score(true_labels, predicted_probs)\n",
    "        val_aucs.append(val_auc)\n",
    "        print(f\"Validation AUC (90/10): {val_auc:.4f}\")\n",
    "        print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "        plot_classification_results(patient_to_true_labels, patient_to_predicted_probs, dataset_name=\"Validation Set\")\n",
    "\n",
    "        # ===================================================\n",
    "        # Evaluate on test set (PATIENT LEVEL)\n",
    "        # ===================================================\n",
    "\n",
    "        patient_to_true_labels, patient_to_predicted_probs = plotAndReturnValidationTesting(test_patients, model)\n",
    "\n",
    "\n",
    "        true_labels, predicted_probs = [], []\n",
    "        for patient_id in patient_to_predicted_probs:\n",
    "            for prob in patient_to_predicted_probs[patient_id]:\n",
    "                true_labels.append(patient_to_true_labels[patient_id])\n",
    "                predicted_probs.append(prob)\n",
    "\n",
    "        predicted_labels = [1 if p >= 0.5 else 0 for p in predicted_probs]\n",
    "        test_auc = roc_auc_score(true_labels, predicted_probs)\n",
    "        test_aucs.append(test_auc)\n",
    "        print(f\"Test AUC (Run {run}): {test_auc:.4f}\")\n",
    "        print(classification_report(true_labels, predicted_labels, digits=4))\n",
    "        plot_classification_results(patient_to_true_labels, patient_to_predicted_probs, dataset_name=\"Test Set\")\n",
    "\n",
    "# ===================================================\n",
    "# Final summary across runs\n",
    "# ===================================================\n",
    "print(\"\\n==============================\")\n",
    "print(\"\\n==============================\")\n",
    "print(\" Validation Test Results \")\n",
    "print(\"==============================\")\n",
    "print(f\"Mean Validation AUC: {np.mean(val_aucs):.4f} ± {np.std(val_aucs):.4f}\")\n",
    "\n",
    "print(\" Final Hold-out Test Results \")\n",
    "print(\"==============================\")\n",
    "print(f\"Mean Test AUC: {np.mean(test_aucs):.4f} ± {np.std(test_aucs):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o96sOcJQa0Br"
   },
   "source": [
    "# Save Model to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4cREufHu-7h"
   },
   "outputs": [],
   "source": [
    "model_name = 'resnet_radimagenet_no_masking_lr0001__1346596590793527366'\n",
    "# Please do not set to true accidently, to ensure models to not get erased\n",
    "overwrite = False\n",
    "\n",
    "# Set model save path based on environment\n",
    "if is_google_colab() and is_google_drive_mounted():\n",
    "    model_save_path = '/content/drive/MyDrive/uttiya_test_lab/model_weights/' + model_name + '.keras'\n",
    "elif is_google_colab():\n",
    "    model_save_path = '/content/model_weights/' + model_name + '.keras'\n",
    "else:\n",
    "    model_save_path = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/model_weights/' + model_name + '.keras'\n",
    "\n",
    "model.save(model_save_path, overwrite=overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVpDXe3TBr8o"
   },
   "source": [
    "# Load model and show performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "error",
     "timestamp": 1756673513149,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 240
    },
    "id": "vbI2k82PSHiR",
    "outputId": "e71944c8-4d3a-4344-e5e4-a8eac71d3890"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "# Set model load path based on environment\n",
    "if is_google_colab() and is_google_drive_mounted():\n",
    "    model_load_path = '/content/drive/MyDrive/uttiya_test_lab/model_weights/' + model_name + '.keras'\n",
    "elif is_google_colab():\n",
    "    model_load_path = '/content/model_weights/' + model_name + '.keras'\n",
    "else:\n",
    "    model_load_path = '/Users/hikaru/Desktop/TUPIL/Code/TUPIL_Kidney/data/model_weights/' + model_name + '.keras'\n",
    "\n",
    "model = tf.keras.models.load_model(model_load_path)\n",
    "\n",
    "# Predict on the test set\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    preds = model.predict(images)\n",
    "    true_labels.extend(labels.numpy().flatten())\n",
    "    predicted_labels.extend(preds.flatten())\n",
    "\n",
    "\n",
    "\n",
    "# Plot True vs Predicted eGFR\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(true_labels, predicted_labels, c='blue', alpha=0.5)\n",
    "# Draw line of best fit for the prediction (to show trend)\n",
    "plt.plot(np.unique(true_labels), np.poly1d(np.polyfit(true_labels, predicted_labels, 1))(np.unique(true_labels)))\n",
    "\n",
    "plt.plot([min(true_labels), max(true_labels)], [min(true_labels), max(true_labels)], 'r--')  # Perfect prediction line\n",
    "plt.xlabel('True eGFR')\n",
    "# Set scale to be 20-100\n",
    "plt.xlim(15, 115)\n",
    "plt.ylim(15, 110)\n",
    "plt.ylabel('Predicted eGFR')\n",
    "plt.title('True vs Predicted eGFR')\n",
    "plt.show()\n",
    "\n",
    "correlation = np.corrcoef(true_labels, predicted_labels)[0, 1]\n",
    "print(f\"Correlation between True and Predicted eGFR: {correlation:.2f}\")\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "results = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {results[0]}, Test MAE: {results[1]}\")\n",
    "\n",
    "# Compute metrics\n",
    "mae = mean_absolute_error(true_labels, predicted_labels)\n",
    "rmse = np.sqrt(mean_squared_error(true_labels, predicted_labels))\n",
    "r2 = r2_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUhbM0_IDFlJ"
   },
   "source": [
    "# Display Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "error",
     "timestamp": 1741125163253,
     "user": {
      "displayName": "TUPIL ML",
      "userId": "06238654818563083219"
     },
     "user_tz": 300
    },
    "id": "WrLeVEnPDHVL",
    "outputId": "e14e6e6d-6548-4646-c7dc-4dd48321d9a7"
   },
   "outputs": [],
   "source": [
    "# Show feature maps of conv layers\n",
    "\n",
    "for layer in model.layers[1].layers:\n",
    "  # print(layer.name)\n",
    "\n",
    "  if 'conv' not in layer.name:\n",
    "      continue\n",
    "  filters , bias = layer.get_weights()\n",
    "  print(layer.name , filters.shape)\n",
    "\n",
    "filters, bias = model.layers[1].layers[1].get_weights()\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "n_filters =6\n",
    "ix=1\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "for i in range(n_filters):\n",
    "    # get the filters\n",
    "    f = filters[:,:,:,i]\n",
    "    for j in range(3):\n",
    "        # subplot for 6 filters and 3 channels\n",
    "        plt.subplot(n_filters,3,ix)\n",
    "        plt.imshow(f[:,:,j] ,cmap='gray')\n",
    "        ix+=1\n",
    "#plot the filters\n",
    "plt.show()\n",
    "\n",
    "# Visualization of first block\n",
    "first_block_model = Model(inputs=model.layers[1].inputs, outputs=model.layers[1].layers[1].output)\n",
    "\n",
    "image = load_img(IMAGE_FOLDER + '/Patient_8_Resized_Image_1.png' , target_size=(224,224))\n",
    "\n",
    "# convert the image to an array\n",
    "image = img_to_array(image)\n",
    "# expand dimensions so that it represents a single 'sample'\n",
    "image = expand_dims(image, axis=0)\n",
    "\n",
    "image = preprocess_input(image)\n",
    "\n",
    "#calculating features_map\n",
    "features = first_block_model.predict(image)\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "for i in range(1,features.shape[3]+1):\n",
    "    fig.suptitle(\"BLOCK_1\", fontsize=20)\n",
    "    plt.subplot(8,8,i)\n",
    "    plt.imshow(features[0,:,:,i-1] , cmap='gray')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Visualization of other blocks\n",
    "blocks = [ 2, 5 , 9 , 13 , 17]\n",
    "outputs = [model.layers[1].layers[i].output for i in blocks]\n",
    "\n",
    "model2 = Model(inputs= model.layers[1].inputs, outputs = outputs)\n",
    "\n",
    "feature_map = model2.predict(image)\n",
    "\n",
    "for i,fmap in zip(blocks,feature_map):\n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "    #https://stackoverflow.com/a/12444777\n",
    "    fig.suptitle(\"BLOCK_{}\".format(i) , fontsize=20)\n",
    "    for i in range(1,features.shape[3]+1):\n",
    "\n",
    "        plt.subplot(8,8,i)\n",
    "        plt.imshow(fmap[0,:,:,i-1] , cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM9pDb4gqcW+6GHF8ydM7uE",
   "collapsed_sections": [
    "7vLhJCCFcjb1",
    "c2MVI9TGcnTc",
    "zd1NaH0Tm5Kb"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "1f2Z66JDnwNNAjyugDY8yz4izwuL-tiqc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
